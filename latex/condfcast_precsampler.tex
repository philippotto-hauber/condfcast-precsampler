\documentclass[notitlepage,a4paper,12pt]{article}
\usepackage[authoryear]{natbib}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.25cm,bottom=2.25cm,nohead}
\usepackage{mathtools}
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--} % dash instead of "to" in range
%\usepackage{amsfonts} 
%\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
%\usepackage[bitstream-charter]{mathdesign}
\usepackage[charter, cal = cmcal]{mathdesign}
\usepackage[T1]{fontenc}
\mathchardef\shortminus="2D
%\newcommand{\shortminus}{-}
% % font used in Korobilis and Schumacher (2014)
%\usepackage[sc]{mathpazo}
%\linespread{1.05}         % Palladio needs more leading (space between lines)
%\usepackage[T1]{fontenc}
% % alternative I found on the web
%\usepackage{tgpagella}
%\usepackage[T1]{fontenc}
\usepackage{xfrac}

\newcommand{\transpose}[1]{{#1}^{\sf{T}}}

\linespread{1.2}
\setlength\parindent{0pt}

\begin{document}
\title{Precision-based algorithms for conditional forecasting and scenario analysis in large state space models}

\author{
  Philipp Hauber\\
  Kiel Insitute for the World Economy
}
\date{\today}

\maketitle

\section{Introduction}

Conditional forecasting is important \citep{bgl_2015ijf}

Precision-based sampling algorithms have been proposed in the literature as an alternative to Kalman-filter based methods \citep{chanjeliazkov_2009}.

This paper proposes precision-based sampling algorithms for conditional forecasting and scenario analysis as an alternative to Kalman-filter based approaches. In an empirical application involving large Bayesian dynamic factor models and vector autoregressions I demonstrate its usefulness. 

\section{Precision-based sampling algorithms for conditional forecasting}\label{sec:precsampler}

In this section, I lay out the precision-based sampling algorithms to obtain conditional forecasts. To fix notation and ideas, I show how 
\subsection{Precision-based sampling}\label{subsec:precsampler}

To fix notation and ideas I outline the precision-based algorithms to sample unobserved components before turning to forecasting. The analysis is done through the lens of a dynamic factor model but can be generalized to other state space models with unobserved components.

Factor models feature prominently in the macroeconomic literature. For an overview of the methodological developments of factor models and their applications, see \citet{stockwatson2016_hbmacro}. Precision-based sampling applications include \citet{chanjeliazkov_2009,mccausland_factor2015,kaufmannschumacher_jae2017,kaufmannschumacher_jectrcs2019}\\

A \textit{dynamic factor model} is defined as

\begin{subequations}
    \label{eqn:factormodel}
    \begin{align}
        \mathbf{y}_t &= \sum_{k=0}^K \lambda_k \, \mathbf{f}_{t-k} + \mathbf{e}_t \label{eqn:facmod_obs}\\ 
        \mathbf{f}_t &= \sum_{p=1}^P \phi_p \, \mathbf{f}_{t-p} + \boldsymbol{\upsilon}_t \label{eqn:facmod_factors}\\
        \mathbf{e}_t &= \sum_{j=0}^J \psi_j \, \mathbf{e}_{t-j} + \boldsymbol{\epsilon}_t \label{eqn:facmod_idios}
    \end{align}
\end{subequations}

where $\mathbf{f}_t$ denotes an $R \times 1$ vector of unobserved factors which summarizes the co-movement of the observables $\mathbf{y}_t$. The dynamics of the factors are modelled as a vector autoregression of order $P$. The idiosyncratic components, $\mathbf{e}_t$ are modeled as independent autoregressive processes of order $J$, i.e. $\psi_1, \dots, \psi_J$ are diagonal matrices. The $N \times R$ loadings matrices $\mathbf{\lambda}_k$ capture the dynamic relationships between observables and factors. Setting $K=0$, so that the variables only load contemporaneously on the factors, yields a \textit{static factor model}. The innovations $\epsilon_t$, $\upsilon_t$ are Normal, uncorrelated at all leads and lags and their covariance matrices given by $\Omega$ and $\Sigma$, respectively. \\

Stacking the observables, factors and idiosyncratic components over $t$ yields 

\begin{equation}
    \mathbf{y} = \mathbf{M} \, \mathbf{f} + \mathbf{e}
\end{equation} 

\noindent where 

$$
\mathbf{M}
=
\begin{bmatrix}
    \boldsymbol{\lambda_0} &   \\
    \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}   \\
    \boldsymbol{\lambda_2} & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0} \\
     & \ddots & \ddots & \ddots \\
     & \boldsymbol{\lambda_K} &\dots & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}
     &  \\
     & & \ddots & \ddots & \ddots & \ddots \\
    & & & \boldsymbol{\lambda_K} &\dots & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}
\end{bmatrix}.
$$

The vector of idiosyncratic components can be written as $\mathbf{H}_e\, \mathbf{e} = \boldsymbol{\varepsilon}$ with 

$$
\mathbf{H}_e
=
\begin{bmatrix}
    I_N &  \\
    \shortminus \psi_1 & I_N &  \\
    \shortminus \psi_2 & \shortminus \psi_1 & I_N &  \\
     &  &  &  &  & \\
    \shortminus \psi_J & \dots & \dots & \shortminus \psi_1 & I_N \\
     &  &  &  &  & \\
    & & \shortminus \psi_J & \dots & \dots & \shortminus \psi_1 & I_N
\end{bmatrix}
$$

\noindent and $\boldsymbol{\varepsilon} \sim \mathcal{N}(0_{NT}, I_T \otimes \boldsymbol{\Omega})$. It follows that $\mathbf{e}$ is Normal, with mean $0$ and covariance matrix $\bm{V}_{e} = (\transpose{\mathbf{H}}_{e} (I_T \otimes \boldsymbol{\Omega}^{-1}) \mathbf{H}_e)^{-1}$. \\

In a similar fashion, the vector of factors is distributed as 

$$
\mathbf{f} \sim \mathcal{N}\left(0, (\transpose{\mathbf{H}}_{\sf{f}} (I_T \otimes \boldsymbol{\Sigma^{-1}}) \mathbf{H}_{\sf{f}})^{-1}\right)
$$

with 

$$
\mathbf{H}_{\sf{f}}
=
\begin{bmatrix}
    I_N &  \\
    \shortminus \phi_1 & I_N &  \\
    \shortminus \phi_2 & \shortminus \phi_1 & I_N &  \\
     &  &  &  &  & \\
    \shortminus \phi_P & \dots & \dots & \shortminus \phi_1 & I_N \\
     &  &  &  &  & \\
    & & \shortminus \phi_P & \dots & \dots & \shortminus \phi_1 & I_N
\end{bmatrix}.
$$

Define 

\begin{equation}
    \mathbf{z} 
    \coloneqq 
    \begin{bmatrix}
        \mathbf{f} \\
        \mathbf{y}
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        I_{TR} & 0 \\
        \mathbf{M} & I_{TN} 
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{f}\\
        \mathbf{e}\\
    \end{bmatrix}.
\end{equation}

It follows that $\mathbf{z}$ is Normal with mean 0 and covariance matrix $\mathbf{V}_{\!z}$. The corresponding precision matrix is the inverse of the covariance matrix: 

\begin{align*}
    \mathbf{Q}_z 
    &= 
    \mathbf{V}^{-1}_{\!z} \\
    &= 
    \text{\sf{Var}}\left(
    \begin{bmatrix}
        \mathbf{f} \\
        \mathbf{y}
    \end{bmatrix}
    \right)^{-1} \\
    &=  
    \left(
    \begin{bmatrix}
        I & 0 \\
        \mathbf{M} & I
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{H}_{\sf{f}}^{-1} \mathbf{V}_{\!\upsilon} \transpose{\mathbf{H}_{\sf{f}}^{-1}} & 0 \\
        0 & \mathbf{V}_{\!e}
    \end{bmatrix} 
    \begin{bmatrix}
        I & \transpose{\mathbf{M}} \\
        0 & I
    \end{bmatrix}
    \right)^{-1}\\
    &=
    \begin{bmatrix}
        I & \shortminus\transpose{\mathbf{M}} \\
        0 & I
    \end{bmatrix}
    \begin{bmatrix}
        \transpose{\mathbf{H}_{\sf{f}}} \mathbf{V}_{\!\upsilon} \mathbf{H}_{\sf{f}} & 0 \\
        0 & \mathbf{V}_{\!e}
    \end{bmatrix} 
    \begin{bmatrix}
        I & 0 \\
        \shortminus\mathbf{M} & I 
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
         \transpose{\mathbf{H}_{\sf{f}}} \mathbf{V}_{\!\upsilon} \mathbf{H}_{\sf{f}} + \transpose{\mathbf{M}}\bm{V}_{\varepsilon}\mathbf{M}  & \shortminus \transpose{\mathbf{M}} \mathbf{V}_{\!e} \\
         \shortminus \mathbf{V}_{\!e} \mathbf{M} & \mathbf{V}_{\!e}
    \end{bmatrix} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\sf{f}} & \mathbf{Q}_{\sf{f} y} \\
        \transpose{\mathbf{Q}}_{\sf{f} y} & \mathbf{Q}_{y}  
    \end{bmatrix}
\end{align*}

Bayesian estimation of \Crefrange{eqn:facmod_obs}{eqn:facmod_idios} requires a draw from the distribution of the factors conditional on the observations and parameters $\Theta$, $p(\mathbf{f}_1, \dots, \mathbf{f}_T | \mathbf{y}_1, \dots, \mathbf{y}_T, \Theta)$. "Simulation smoothing" methods based on the Kalman filter/smoother have been proposed by Carter and Kohn (1994) and Durbin and Koopman (2001) and are widely used in the literature. Precision-based sampling algorithms \citep{chanjeliazkov_2009} provide an alternative to these methods.\\

Using standard results for multivariate Normal distributions \citep[e.g.][pp. 86-87]{bishop_prml_2006}, we can derive the desired conditional distribution:

\begin{align*}
    p
    \left(
    \mathbf{f} \mid \mathbf{y}, \Theta
     \right) 
    &\sim
    \mathcal{N}
    \left(
    \mathbf{Q}^{-1}_{\alpha} \left( \mathbf{Q}_{\alpha}\boldsymbol{\mu}_{\alpha} - \mathbf{Q}_{\alpha y} \left(\mathbf{y} - \boldsymbol{\mu}_{\text{y}}\right)\right), \mathbf{Q}_{\alpha}^{-1}
    \right) \\
    &\coloneqq \mathcal{N}
        \left(
            \boldsymbol{\mu}_{\alpha\mid y}, \mathbf{Q}_{\alpha\mid y}^{-1}
        \right) \\
\end{align*}

with \citep[see also][eqn. 6-8]{chanjeliazkov_2009}:
\begin{align*}
    \mathbf{Q}_{\sf{f}\mid y} &= \mathbf{Q}_{\sf{f}} \\
    &= 
    \transpose{\mathbf{H}} \mathbf{V}^{\,-1}_{\!\upsilon} \mathbf{H} + {\transpose{\mathbf{M}} \mathbf{V}^{\,-1}_{\!\epsilon}\mathbf{M}}\\
    \boldsymbol\mu_{\sf{f}\mid y} &= \mathbf{Q}_{\sf{f}}^{-1} \mathbf{Q}_{\sf{f}} \tilde{\mathbf{c}}+ \transpose{\mathbf{M}} \mathbf{V}_{\!\epsilon}^{\,-1} \mathbf{y}
\end{align*}

Draws from this distribution can be obtained efficiently since the $T\cdot S \times R$ matrix $\mathbf{Q}_{\sf{f}}$ is banded and its inversion not required. Specifically, calculate the conditional mean following \citet[][Algorithm 2.1]{rueheld_2005}. This requires the computation of the lower Cholesky factor $\mathbf{L}$ of $\mathbf{Q}_{\sf{f}}$. Then draw $\mathbf{v} \sim \mathcal{N}(0,I_{TR})$, solve $\transpose{\mathbf{L}} \mathbf{x} = \mathbf{v}$ for $\mathbf{x}$ and set $\boldsymbol{\mu}_{\sf{f}\mid y} +\mathbf{x}$, yielding a draw of $\mathbf{f}$ conditional on $\mathbf{y}$ \citep[][Algorithm 2.4]{rueheld_2005}. \\

\subsection{Forecasting}

In addition to a draw of the factors conditional on the data, macroeconomic applications may require forecasts - both unconditional and conditional on future paths of a subset of the observables - from the model in \Crefrange{eqn:facmod_obs}{eqn:facmod_idios}. Examples include projections of inflation which take assumptions regarding the macroeconomic environment into account \citep{giannone_etal_2014_ijf} or external nowcasts as "jump-off" points for longer-term forecasts in reduced form or structural models \citep{faustwright2009_jbes,wolters2015_jae}; \citet{knotekzaman2019_ijof} show how macroeconomic forecasts can be improved by conditioning on nowcasts of financial variables.\\

Let $\mathbf{y}^{\sf{o}} = \transpose{[\mathbf{y}^{\sf{T}}_{1}, \dots, \mathbf{y}^{\sf{T}}_{T}]}$ denote the available observations through $T$. For the forecast periods $h = 1:H$, $\mathbf{y}_{c,t}\: \forall t = T+1:T+H$ are the observations which are being conditioned on. The entire conditioning set is given by 
$$
\mathbf{y}^{\sf{c}} = \transpose{[\mathbf{y}^{\sf{T}}_{c,T+1}, \dots, \mathbf{y}^{\sf{T}}_{c,T+H}]}
$$ 
with $N^c = \text{dim}(\mathbf{y}^{\sf{c}})$. Conversely, $\mathbf{y}_{-c,t} = \mathbf{y}_t\notin\mathbf{y}_{c,t}$ denotes the observations for which no conditioning information is available at time $t$ and which are being forecast: $\mathbf{y}^{\sf{f}} = \transpose{[\mathbf{y}^{\sf{T}}_{-c,T+1}, \dots, \mathbf{y}^{\sf{T}}_{-c,T+H}]}$.\\

In what follows, I will not be concerned with the estimation of model parameters and assume that random draws from $p(\Theta|\mathbf{y}^{\sf{o}})$ are readily available. Also, for the sake of exposition, I assume that between periods $t=1$ and $t=T$ all observations are available. Situations where this is not the case, e.g. because some series start at a later date in the sample or because outlying observations have been removed by setting them to missing, can naturally be accommodated.\\

From a Bayesian perspective, forecasting boils down to sampling from the predictive density 

\begin{equation}\label{eqn:preddens}    
    p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}},\mathbf{y^{\sf{c}}}, \Theta) p(\Theta|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})\, \text{d}\Theta.
\end{equation}

Note that this density takes into account that the conditioning set $y^c$ contains information about which $\Theta$ are more likely a posteriori. Draws from this density can be obtained by applying the insights in Hauber and Schumacher (2021) who generalize the precision sampler outlined above to applications with missing data. Since future values of the variables can simply be considered as missing, draws from the joint distribution of states and forecasts conditional on the data (and possibly the future paths of a subset of the variables) can be sampled efficiently.\\

To evaluate Equation \ref{eqn:preddens}, derive the joint distribution of states and observations from $t=1:T+H$: 

\begin{align*}
    \mathbf{z} &= \transpose{[\transpose{\mathbf{f}}, \transpose{\mathbf{y}}]} \\
     &= \transpose{[\mathbf{f}_1, \dots, \mathbf{f}_{T+H}, \mathbf{y}_1, \dots, \mathbf{y}_{T+H}]} 
\end{align*}

It follows that the distribution of $\mathbf{z}$ is Normal with mean $\boldsymbol{\mu}_{z} = 0$ and precision matrix $\mathbf{Q}_{z}$ as in Section \ref{subsec:precsampler}. To sample from the conditional distribution $p\left(\mathbf{y}^{\sf{f}}, \mathbf{f} \mid \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}, \Theta \right)$  requires a permutation of $\mathbf{z}$. To this end, define the permutation matrix $\mathcal{P}$ that reorders $\mathbf{z}$ such that the states and elements of $\mathbf{y^f}$ - ordered by time period $t$ - are placed first. That is to say,

$$
\mathbf{z}_{\mathcal{P}} \coloneqq \mathcal{P} \mathbf{z} =
 \transpose{[
     \mathbf{f}^{\sf{T}}_1, 
     \dots, 
     \mathbf{f}^{\sf{T}}_T, 
     \mathbf{f}^{\sf{T}}_{T+1}, 
     \mathbf{y}^{\sf{T}}_{\shortminus c,T+1}, 
     \dots, 
     \mathbf{f}^{\sf{T}}_{T+H}, 
     \mathbf{y}^{\sf{T}}_{\shortminus c,T+H},
     \mathbf{y}^{\sf{T}}_1,
     \dots,
     \mathbf{y}^{\sf{T}}_{T+H},
     \mathbf{y}^{\sf{T}}_{c, T+1},
     \dots, 
     \mathbf{y}^{\sf{T}}_{c, T+H}]}.
$$

Note that the conditioning arguments $\mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}$ are now ordered last. The permuted vector of states and observations is also Normal with (permuted) moments given by 

\begin{equation*}
    \mathbf{z_{\mathcal{P}}} \sim \mathcal{N}\left(\mathcal{P}\boldsymbol{\mu}_z, \left(\mathcal{P} \mathbf{Q}_z \transpose{\mathcal{P}}\right)^{-1}\right)
\end{equation*}

where we have made use of the fact that the inverse of a permutation matrix is equal to its transpose.  The precision matrix of $\mathbf{z}_{\mathcal{P}}$ can be partitioned as follows: 

\begin{align*}
    \mathbf{Q}_{z_{\mathcal{P}}} &\coloneqq  \mathcal{P} \mathbf{Q}_z \transpose{\mathcal{P}} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\sf{f}\! y^{\sf{f}}} & \mathbf{Q}_{\sf{f}\! y^{\sf{f}}, y^{\sf{o}}} \\ 
        \mathbf{Q}^{\sf{T}}_{\sf{f}\! y^{\sf{f}}, y^{\sf{o}}} & \mathbf{Q}_{y^{\sf{o}}}
    \end{bmatrix}
\end{align*}

where $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}, y^{\sf{o}}}$ is a 
$T\!\cdot\!R + H\!\cdot\!N \times T\!\cdot\!N$ matrix and $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}}$ and $\mathbf{Q}_{y^{\sf{o}}}$ are square with dimensions $T\!\cdot\!R + H\!\cdot\!N$ and $T\!\cdot\!N $, respectively.\\


Similarly, the mean of $\mathbf{z}_{\mathcal{P}}$ can be partitioned as 
$$
\boldsymbol{\mu}_{z_{\mathcal{P}}} = 
\begin{bmatrix}
    \boldsymbol{\mu}_{\sf{f} y^{\sf{f}}} \\
    \boldsymbol{\mu}_{y^{\sf{o}}}
\end{bmatrix}
$$

Denote by $\mathbf{z}^*$ a draw from the conditional distribution (surpressing the dependence on the parmeters $\Theta$ for the sake of readability)

\begin{equation*}
    \mathbf{z}_{\sf{f} y^{\sf{f}}} | \mathbf{y}^{o}, \mathbf{y}^{c} \sim 
    \mathcal{N}\left(
        \mathbf{Q}^{-1}_{\sf{f} y^{\sf{f}}}\left( \mathbf{Q}_{\sf{f} y^{\sf{f}}}\boldsymbol{\mu}_{\sf{f} y^{\sf{f}}} - \mathbf{Q}_{\sf{f} y^{\sf{f}}, y^{\sf{o}}}\left(\mathbf{y}^o-\boldsymbol{\mu}_{y^{\sf{o}}}\right)\right)
    ,
    \mathbf{Q}_{\sf{f} y^{\sf{f}}}
    \right).
\end{equation*}

Then by reversing the permutation, i.e 

$$
\begin{bmatrix}
    \mathbf{f} \\
    \mathbf{y}_1 \\
    \vdots \\
    \mathbf{y}_{T+H} 
\end{bmatrix}
= 
\mathcal{P}^{-1} 
\begin{bmatrix}
    \mathbf{z}^*\\
    \mathbf{y^{o}} \\
    \mathbf{y^{c}}
\end{bmatrix}
$$

we can back out the draws of $\mathbf{f}$ and $\mathbf{y}^{\sf{f}} = [\mathbf{y}_{\shortminus c, T+1}, \dots, \mathbf{y}_{\shortminus c, T+H}]$.\\

Note that by having placed those components of $\mathbf{z}$ which are conditionally dependent close to each other, the bandwidth of $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}}$ is kept small. Figure \ref{figQDFM} shows the precision matrix of $z_{\mathcal{P}}$ for a factor model with $T=100$, $N=100$ and $R=10$\footnote{For the sake of illustration we set $P=1$, $J=0$, $S=0$.}, a forecast horizon of $H=20$ and conditioning on the first 10 variables, i.e.

$$
y^c = [y_{1,T+1}, \dots, y_{10,T+1}, \dots, y_{1,T+20}, \dots, y_{10,T+20}].
$$

The precision matrix $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}}$ is highlighted by the square. 

\begin{figure}[htbp] \centering
    \caption{Precision precision matrix in the case of a dynamic factor model \label{figQDFM}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_Qperm_DFM.pdf}} \\
    {
    \footnotesize \textbf{Note:} The figure shows the precision matrix of the permuted vector of observations and states $\mathbf{z}_{\mathcal{P}}$ for a factor model with $T=100$, $N=100$ and $R=10$. Highlighted is the $(T+H)\cdot R + N^c \times (T+H) \cdot R + N^c$ partition corresponding to $z_{\sf{f} y^{\sf{f}}}$.  
    }
    \end{tabular}
    \newline
    \normalsize
\end{figure}

Draws from the predictive density $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})$ can then be obtainted by the following algorithm \citep[see also][Algorithm 1]{waggonerzha1999_res}:\\

\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
    \noindent\textbf{Algorithm 1: Draws from the predictive density $p(\mathbf{y}^f | \mathbf{y}^o, \mathbf{y}^c)$}\\

    Initialize $\Theta^{0}$ from $p(\Theta | y^o)$. For $g=1:(G_0+G)$
    
    \begin{enumerate}    
        \item Construct the system matrices $\mathbf{H}, \mathbf{M}, \mathbf{V}_{\!\varepsilon}, \mathbf{V}_{\!\upsilon}, \tilde{\mathbf{c}}, \mathbf{d}$ given the parameters $\Theta^{(g-1)}$
        \item Compute the mean $\boldsymbol{\mu}_{z}$ and precision matrix $\mathbf{Q}_{z}$ of the joint distribution of $\mathbf{z} = [\boldsymbol{\alpha}, \mathbf{y}]$  
        \item Permute $\mathbf{z}$, yielding $\mathbf{z}_{\mathcal{P}'} \sim \mathcal{N}(\mu_{z_\mathcal{P}'}, Q_{z_\mathcal{P}'}^{-1})$ with partitions $\transpose{[\mathbf{z}^{\sf{T}}_{\alpha\! y^{\sf{f}}}, \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}]}$
        \item Sample from the conditional distribution $\mathbf{z}_{\alpha\! y^{\sf{f}}} | \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}, \Theta^{(g-1)}$ using \citep[][Algorithm 2.1 and 2.4]{rueheld_2005} and reverse the permutation to back out the draws of $\mathbf{y}^{\sf{f}}$ and $\boldsymbol{\alpha}$
        \item Draw $\Theta^{(g)}$ from $p(\Theta|\mathbf{y}^o, \mathbf{y}^{\sf{c}}, {\mathbf{y}^{\sf{f}}}^{(g)}, \alpha^{(g)})$ 
     \end{enumerate}
    
    Discard the first $G_0$ draws as burn-in.\\


    \xdef\tpd{\the\prevdepth}
\end{minipage}

If the sample information and prior dominate the information in $y^{\sf{c}}$, the impact of the conditioning arguments on the posterior distribution of the parameters is likely to be negligible and $p(\Theta | y^o, y^c) \approx p(\Theta | y^o)$ \citep{delnegro_schorfheide_2013_hb,bgl_2015ijf}. In such applications, draws from Equation \ref{eqn:preddens_cond} can then be obtained by skipping step 5 at each iteration in \textbf{Algorithm 1}.\\

Similarly, when $y^c$ is empty, Step 5 can obviously be skipped as well and \textbf{Algorithm 1} returns unconditional forecasts, i.e. draws from the density $p(\mathbf{y}^f | \mathbf{y}^o)$.\\

While the discussion above has focussed on dynamic factor models, it can easily be extended to general state space models of the form:

\begin{subequations}
    \label{eqn:statespacesys}
    \begin{align}
    y_t &= Z \alpha_t + \varepsilon_t, \varepsilon_t \sim \mathcal{N}(0, H) \\
    \alpha_{t+1} &= T \alpha_t + \eta_t, \eta_t \sim \mathcal{N}(0, Q) 
    \end{align}
\end{subequations}

by setting $P=1$, $J=S=0$.\\

Lastly, note that Hauber and Schumacher (2021) also propose a sequential sampler that first draws the missing observations conditional on the states and then the states given a complete data set. This approach performs somewhat better in terms of computing time and could in principle be used in Algorithms 2, which in any case requires a Gibbs sampling step to update the parameters given the conditioning set. However, the gains in performance from faster sampling of states and forecasts may not outweigh the longer burn-in required due to the additional block in the Gibbs Sampler.\\

\subsection{Extension to vector autoregressions}

Vector autoregressions (VAR) are another class of popular macroeconometric models and can be described by the following equation: 

\begin{equation}\label{eqn:var}
    \mathbf{y}_t = \mathbf{b} + \sum_{p=1}^P \mathbf{B}_p \mathbf{y}_{t-p} + \boldsymbol{\upsilon}_t
\end{equation}

where $\mathbf{b}$ is an $N \times 1$ vector of intercepts, $\mathbf{B}_1, \dots, \mathbf{B}_P$ are the $N \times N$ coefficient matrices and $\boldsymbol{\upsilon}_t \sim \mathcal{N}(0, \Sigma)$. With slight modifications that account for the fact that in the case of a VAR the states are observed without meausrement error, the algorithm outlined above can also be employed to generate conditional forecasts.\footnote{Unconditional forecasts given the parameters can simply be obtained by drawing $\boldsymbol{\upsilon}_{T+1}, \dots, \boldsymbol{\upsilon}_{T+1}$ and using Equation \ref{eqn:var} to recursively construct $\mathbf{y}_{T+1}, \dots, \mathbf{y}_{T+H}$.}

For conditional forecasts, the predictive density is given by 

\begin{equation}\label{eqn:preddens}    
    p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathbf{y}_T, \dots, \mathbf{y}_{T-P+1}, \mathbf{y^{\sf{c}}}, \Theta) p(\Theta|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})\, \text{d}\Theta.
\end{equation}

where in the absence of unobserved states the forecasts given the parameters only depend on the last $P$ observations of the observables. 

That is to say, given the parameters the model can rewritten in stacked form as


\begin{equation*}
    \begin{bmatrix}
        I_N \\
        \shortminus\mathbf{B}_1 & I_N \\
        & \ddots & \ddots & \\
        \shortminus\mathbf{B}_{P} & \dots & \shortminus\mathbf{B}_1 & I_N & & \\
        & \ddots &  & \ddots & \ddots & \\
        & & \shortminus\mathbf{B}_{P} & \dots & \shortminus\mathbf{B}_1 & I_N       
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{y}_{T-P+1} \\
        \vdots \\
        \mathbf{y}_{T} \\
        \mathbf{y}_{T+1} \\
        \vdots \\
        \mathbf{y}_{T+H} 
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mathbf{b} \\
        \vdots \\
        \mathbf{b} \\
        \mathbf{b} \\
        \vdots \\
        \mathbf{b} 
    \end{bmatrix}
    +
    \begin{bmatrix}
        \boldsymbol{\upsilon}_{T-P+1} \\
        \vdots \\
        \boldsymbol{\upsilon}_{T} \\
        \boldsymbol{\upsilon}_{T+1} \\
        \vdots \\
        \boldsymbol{\upsilon}_{T+H} 
    \end{bmatrix}
\end{equation*}

or more compactly:
\begin{equation*}
    \mathbf{H} \mathbf{y} = \mathbf{c} + \boldsymbol{\upsilon}.
\end{equation*}

Define $\mathbf{z}={y} = \transpose{[\transpose{\mathbf{y}_{T-P+1}}, \dots, \transpose{\mathbf{y}_T}, \dots, \transpose{\mathbf{y}_{T+H}}]}$. Then the joint distribution of available observations and forecasts is Normal:

$$
\mathbf{z} \sim \mathcal{N}(\mathbf{H}^{-1} \mathbf{c}, (\transpose{\mathbf{H}} \mathbf{V}^{-1}_{\!\upsilon}\mathbf{H})^{-1}).
$$

Draws from the predictive density $p(y^f| y^o, y^c)$ can be obtained by permuting $\mathcal{P} \mathbf{z} \coloneqq \mathbf{z}_{\mathcal{P}} = \transpose{[\mathbf{y}^{\sf{f}}, \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}]}$, where $y^o = \transpose{[\transpose{\mathbf{y}_{T-P+1}}, \dots, \transpose{\mathbf{y}_T}]}$.\\

The corresponding mean and precision matrix can be partitioned as follows:

\begin{align*}
    \boldsymbol{\mu}_{z_{\mathcal{P}}} &= 
    \coloneqq
    \begin{bmatrix}
        \boldsymbol{\mu}_{y^{\sf{f}}} \\
        \boldsymbol{\mu}_{y^{\sf{o}}}
    \end{bmatrix}
    ,\\
    \mathbf{Q}_{z_{\mathcal{P}}} &= 
    \coloneqq
    \begin{bmatrix}
        \mathbf{Q}_{y^f} & \mathbf{Q}_{y^f\!y^o, y^c} \\
        \mathbf{Q}^{\sf{T}}_{y^f\!y^o, y^c} & \mathbf{Q}_{y^o, y^c}
    \end{bmatrix}
    .
\end{align*}

The distribution of forecasts given observed data and conditioning set $y^c$ is then given by 

\begin{align*}
    \boldsymbol{\mu}_{y^f|y^o, y^c} &= 
    \boldsymbol{\mu}_{y^f} - \mathbf{Q}_{y^f}^{-1} \mathbf{Q}_{y^{\sf{f}}\!y^o, y^c} (\mathbf{y}_o - \boldsymbol{\mu}_{y^o, y^c})\\
    \mathbf{Q}_{y^f|y^o, y^c} &= \mathbf{Q}_{y^f}.
\end{align*}

Draws from $p(y^f|y^o, y^c)$ can be obtained efficiently even when $H$ is large as the precision matrix $\mathbf{Q}_{y^f}$ is banded. As above, we illustrate this point by way of an example: Figure \ref{figQVAR} shows the precision matrix $Q_{z_{\mathcal{P}}}$ for a VAR with $N=100$, $P=4$, a forecast horizon of $H=20$ and forecasts conditional on the first 10 variables, i.e. 

$$y^c = [y_{1,T+1}, \dots, y_{10,T+1}, \dots, y_{1,T+20}, \dots, y_{10,T+20}].$$

The highlighted area corresponds to $\mathbf{Q}_{y^f}$.

\begin{figure}[htbp] \centering
    \caption{Precision matrix $Q_{\mathcal{z}_{\mathcal{P}}}$ in the case of a VAR \label{figQVAR}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_Qperm_VAR.pdf}} \\
    {
    \footnotesize \textbf{Note:} The figure shows the precision matrix of the permuted vector $\mathbf{z}_{\mathcal{P}}$ for a vector autoregression with  $N=100$, $P=4$, a forecast horizon of $H=20$ and forecasts conditional on the first 10 variables. Highlighted by the square is the partition corresponding to the forecasts $\mathbf{y}^{\sf{f}} = [y_{-c, T+1}, \dots, y_{-c, T+H}]$.
    }
    \end{tabular}
    \newline
    \normalsize
\end{figure}

\subsection{Soft conditions and repeated samples from $p(y^f|y^o, \Theta^{(g)})$}

So far we have considered conditional forecasts that fix the future values of some of the endogeneous variables at single points. \citet{waggonerzha1999_res} labels these "hard" conditions to distinguish them from soft conditions where each element of $y^c$ is merely restricted to lie within a pre-specified range, i.e. $y^{\sf{f}_l}_l \leq y^{\sf{f}_l} \leq y^{\sf{f}_l}_u$. Draws from the unconditional predictive density $p(y^f|y^o, \Theta^{(g)})$ can be obtained until $y^{\sf{f}_l} \leq y^{\sf{f}_l} \leq y^{\sf{f}_l}_u$.\footnote{Similarly, hard and soft conditions can be combined, e.g. if inflation forecasts are produced conditional on an oil price path and subject to the restriction that the Federal Funds rate is positive. In such a scenario, draws from $p(y^f|y^o, y^c, \Theta^{(g)})$ are obtained until the soft restrictions are satisfied.}\\

The precision-sampling algorithms outlined above are particularly suited to this task since the bottleneck in the calculations is the Cholesky factorization of the band matrix $Q_{\sf{f} y^{\sf{f}}}$ which requires $((T+H)R+NH)\cdot b_w^2$ floating point operations where $b_w$ is the lower bandwidth of $Q_{\sf{f}, y^{\sf{f}}}$ \citep[][4.3.5]{GolubvanLoan2013}. However, it only needs to be computed once, irrespective of the number of desired draws \citep{rue2001_jrss}. Repeated samples thus come at a significantly reduced computational burden, consisting only of $(T+H)S+NH$ independent draws from the standard Normal distribution, band backward substitution - which requires $2 ((T+H)S+NH) \cdot b_w$ floating point operations \citep[][4.3.2]{GolubvanLoan2013}- and a vector addition. 

\subsection{Runtime comparison}

\subsubsection{Motivation and related literature}

In simulations I compare the computational efficiency of the precision-sampling algorithms outlined above to simulation smoothers such as \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} that rely on the Kalman-filter and are commonly used in the literature. The focus is \textbf{not} on finding the overall best-performing algorithms. In particular, I do not consider low-level implementations in C or Fortran. Also I do not explore the extent to which parallelized execution could provide. Instead, the aim is to compare the algorithms .Previous work in this area has analyzed the computational advantages of such simulation smoothers compared to the algorithms for conditional forecasting given in \citet{waggonerzha1999_res} in the context of a vector autoregressions \citep{bgl_2015ijf}; \citet{mmp2011_csda} analyzes the performance of precision-based samplers in the context of a time-varying parameter regression and dynamic factor models and compares the performance to the \citet{durbinkoopman2002_biomtr} simulation smoother.\\

I focus on both small ($N=20,\,R=2$) and large factor models ($N=100\,,R=10$) as well as a medium- and large-sized vector autoregression with $N=20$ and $N=100$, respectively; the lag length equals $P=4$ in both vector autoregressions. I evaluate the runtime of obtaining $100$ draws from the predictive density $p(y^f|y^o, y^c)$ given a sample size $T=100$ and forecast horizons $H = \{5, 20, 50\}$ conditional on a share $\kappa = 0.1$ of the observables (\textit{hard conditioning}).\footnote{The size of the conditioning set does not have a large impact on the relative runtime and in the interest of space I do not report results for $\kappa = \{0.5, 0.75\}$. They are available upon request.} Furthermore, I also consider the costs of producing repeated samples from the predictive density given the parameters as would be the case when on top of the conditioning set $$\mathbf{y}^{c} = [y_{1, T+1}, \dots, y_{\kappa \cdot N, T+1}, \dots, y_{1,T+H}, \dots, y_{\kappa \cdot N, T+H}]$$ restrictions are imposed on the remainining observables to lie within a prespecified range (\textit{soft conditioning}). 

\subsubsection{Data-generating process}

In the case of the dynamic factor model outlined in Section \ref{sec:dfm} the factor dynamics are given by a VAR(1) with parameters $\phi_1 = 0.7 \cdot I_R$ and $\Sigma = I_R$. The elements of the loadings on the contemporaneous factors $\lambda_0$ are drawn from independent Normal distributions with mean $0.5$ and variance $0.1$; all remaining loadings are set to 0, i.e. $K=0$. Setting $J=0$, the idiosyncratic components are i.i.d and $\omega_i$ is chosen such that the common component - $\boldsymbol{\lambda}_{0,i\cdot} \mathbf{f}_t$ - explains \sfrac{2}{3} of the total variation in the i-th variable. \\

To generate data from a vector autoregression as discussed in Section \ref{sec:var}, I closely follow the set-up outlined in \citet{CHP2020_ijf}. Let $b_{p, ij}$ denote the element of $B_p$ in row i and column j. The diagonal elements are set to

\begin{subequations}
    \begin{align}
        b_{1, ii} = \mathcal{N}(0, \sigma^2_o), \: \forall i = 1, \dots, N \\
b_{p, ii} = \frac{b_{1,ii}}{p}, \: \forall p = 2, \dots, P
    \end{align}    
\end{subequations}

\noindent while each off-diagonal element is drawn from $\mathcal{N}(0, \sigma^2_c)$ with probability $p_c$, otherwise it is set to $10^7$. The lag length $P$ equals 4 and for the medium-sized VAR with $N=20$ variables $\sigma^2_o = 0.2 $, $\sigma^2_c = 0.05$ and $p_c = 0.2$. To ensure a stationary VAR when the cross-section is large, i.e. $N=100$, the values are adjusted to $\sigma^2_o = 0.05 $, $\sigma^2_c = 0.001$ and $p_c = 0.1$.

Details on the implementation of the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers are provided in Appendix \ref{app:kalmansimsmoothers}.\\

\subsubsection{Results}

I simulate each model $10$ times and calculate the time it takes to draw from the predictive density using the Kalman-filter based simulation smoother and the precision sampling algorithms outlined above.\footnote{The simulations were run using Matlab2020a and a XX and XX. To time the execution of the code, I used the function \texttt{timeit()}.} Figure \ref{figsimulation} shows the relative runtime for hard (left column) and soft (right column) as boxplots. The precision-based algorithms perform as well or better than the Kalman-filter based simulation smoothers for all models and horizons. When repeated draws from the predictive density are required as in the case of soft conditioning, the gains in computational efficiency are generally quite large. 

\begin{figure}[htbp] \centering
    \caption{Computational efficiency analysis of different simulation smoothers \label{figsimulation}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_simulations_Ncond_10.pdf}} \\
    {
    \footnotesize \textbf{Note:} The figure shows the time it takes to generate a draw from the predictive density $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})$ for different forecast horizons $H$ using the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers relative to the precision sampler outlined above. The conditioning set $\mathbf{y}^{\sf{c}}$ consists of the first $0.1\cdot N$ variables (left column). For the small factor model, the number of variables is $N=20$ and the number of static factors is $R=2$. For the large factor model $N=100,\, R = 10$. For the medium- and large-sized vector autoregressions the number of lags is set to $P=4$ as well as $N=20$ and $N=100$, respectively. The runtime measures the entire procedure of generating a draw from the predictive density, including building the system matrices. For the precision-sampler this includes the costs of setting up the permutation matrices which is incurred only once.In the case of soft conditioning (right column), the runtime of producing 100 draws given the same parameters is reported. For details, see the main text.
    }
    \end{tabular}
    \newline
    \normalsize
\end{figure}

\section{Real-time evaluation of unconditional and conditional forecasts}

In this section, we detail the data used in the forecast evaluation and provide details on the Gibbs Sampler that is used to estimate the model with Bayesian methods. Furthermore, we discuss the set-up of the real-time evaluation. 

\subsection{Data}
\subsubsection{Real-time data}

To estimate the factor model, we construct a large quarterly real-time dataset covering different aspects of the macroeconomy. Beyond real gross domestic product, the main building block of the dataset are other series from the national accounts such as the entire expenditure side components, e.g. private and public consumption, gross fixed capital formation, export and imports. From the production side of GDP we can only include gross value added in the industrial and construction sector as well as trade, transport and hospitality as real-time vintages for other sectors are only available from 2011 onwards.\footnote{The three mentioned sectors comprise roughly 50 percent of total gross value added.} 

In addition to the chained volume indices we also include the corresponding deflators of the expenditure and production side components. Furthermore, it includes series on real activity such as industrial production, turnover or orders as well as construction, headline CPI and PPI indices as well as the corresponding core indices and a measure of prices in the construction sector as well as labor market indicators such as economy-wide employment and wages and hours worked in the industrial and construction sector.    

In addition, data are supplemented with financial indicators such as interest rates, exchange rates and stock market indices. Lastly, we also include a few survey-based indicators provided by the European Commission (ESI) and covering the industrial, construction and services sectors as well as a measure of firms' employment expectations (EEI). Note that for the latter two sources, there is no real-time problem, as financial indicators are not revised subsequent to their original publication. Similarly, for the European Commission's survey indicators, we rely on the non-seasonally adjusted indices which are also not revised. 

Altogether, our dataset contains XX series. All of them are transformed to stationarity prior to estimaton. In most cases, this involves taking the first difference of the logarithm of the original series, though in some cases, e.g. interest rates or survey indicators, we take simple differences. \footnote{The Deutsche Bundesbank's real-time database includes vintages of chained volume indices and nominal values for the national accounts. The implicit deflators are calculated by dividing the nominal series with the chained volume index and taking the first differenc of the logarithm of the resulting series.} Details of the time series, their sources as well as the transformation employed  can be found in Appendix XXX.

\subsubsection{Professional forecasts from the Reuters Poll}

As conditioning information, we rely on the Reuters Poll of forecasts for German GDP and CPI inflation. The survey is conducted once a quarter, typically during the first weeks of the  and compiles the views of around twenty different professional forecasters from the private sector and research institutes. 

For GDP, the forecasts are given in terms of quarter-on-quarter (q/q) growth rates and we can thus use the median forecast directly to obtain conditional forecasts from the model outlined above. The inflation forecasts, however, are reported as year-over-year (y/y) growth rates of the consumer price index. To bring the forecasts in line with the treatment of CPI inflation in the model, we have to transform the individual forecasts. Specifically, we interpret the y/y forecasts of inflation, $x_t^{y/y}$, as describing the four-quarter change in the quarterly consumper price index $X$, i.e. $ x_t^{y/y} = log(X_t/X_{t-4}$. Thus, given a sequence of y/y forecasts we can back out the implicit forecasts of the (log of the) consumer price index and derive the corresponding q/q forecasts, i.e. $ x_t^{q/q} = log(X_t/X_{t-1}$ for each participant in the survey. We then use the median of these transformed values in the conditional forecasting exercise.

\begin{figure}[htbp] \centering
    \caption{GDP growth and CPI inflation forecasts from the Reuters poll of professional forecasters \label{fig:reuterspoll}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_ReutersPoll.pdf}} \\
    {
    \footnotesize \textbf{Note:} 
    }
    \end{tabular}
    \newline
    \normalsize
\end{figure}

Figure \ref{fig:reuterspoll} plots the median forecast of q/q CPI inflation (top row) and GDP growth (bottom row). The lines correspond to different forecast horizons relative to the reference period and are given in quarters. That is to say, the $h=1$ forecast for the first quarter in 2010 was made in October 2009. For CPI inflation, there are no backcasts ($h=-1$) as a flash estimate for the previous quarter is already available at the time.  
 

\subsection{Estimation}

The factor model outlined above is estimated with Bayesian methods. Draws from the joint posterior distribution of factors and parameters are obtained via a Gibbs Sampler which alternately draws from the conditional posterior of the factors (parameters) given the parameters (factors). In more detail, the Gibbs Sampler cylces through the following blocks: 

\begin{itemize}
    \item p(f| )
    
\end{itemize}

The priors used in the estimation as well as the resulting conditional distributions are standard in the literature and are therefore relegated to Appendix.

\subsection{Evaluation set-up}

For CPI inflation in the first quarter of 2013 there are no forecasts available. In the following forecast evaluation, I therefore exclude. Note also that 

Missing vintages for hours industry

\section{Results}


\bibliographystyle{aernobold}
\bibliography{papers}

\appendix

\section{Appendix: Kalman-filter based simulation smoothers}\label{app:kalmansimsmoothers}

This appendix provides details on the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers that are used in the simulations in Section \ref{sec:models}



In this section we highlight how we implement the Kalman-filter based simulation smoothers and show how to obtain draws from the predictive densities. Although both the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers rely on the Kalman filter and smoother to produce a draw from the conditional distribution of the states, there are important conceptual differences between the two. Let $a_{t|s} = E[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ and $P_{t|s} = Var[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ denote the conditional mean and variance of the state at time $t$ conditional on information up to time $s$ which can be obtainted from the Kalman filter or smoother. In addition, let $a_t$ denote a draw from the conditional distribution $p(\alpha_t | \mathbf{y}_1, \dots, \mathbf{y}_{T+H})$. 

\subsubsection{\citet{carterkohn1994_biomtr} simulation smoother}

Given the parameters of the state space model $\Theta$ and  $a_{t|t}, P_{t|t} \: \forall t = 1, \dots, T+H$, the algorithm in \citet{carterkohn1994_biomtr} generates $a_{T+H}$ from 

$$
p(\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})
$$

\noindent with

\begin{align*}
    E[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = a_{T+H|T+H} \\
    Var[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = P_{T+H|T+H}.
\end{align*}

 For $t=T+H-1, \dots, 1$ $a_t$ is generated from $p(\alpha_{t}|\mathbf{y}_t, a_{t+1})$. For details on how to derive the moments of the conditional distributions given the output of the Kalman filter, see the original paper as well as the textbook treatment in \citet{KimNelson1999mit}.

 \subsubsection{\citet{durbinkoopman2002_biomtr} simulation smoother}

The \citet{durbinkoopman2002_biomtr} simulation smoother produces a draw from the conditional distribution of the states by first simulating the state space model in Equation \ref{eqn:statespacesys}, producing a draw from the \textit{joint} distribution of states and observables

$$p(\alpha_{1}, \dots, \alpha_{T+H}, \mathbf{y}_1, \dots, \mathbf{y}_{T+H}).$$

Denote this joint draw by $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Running the Kalman smoother recursions yields $a_{t|T+H}, \: a^+_{t|T+H}$ where $a^+_{t|T+H} = E[\alpha^+_t|y^+_1, \dots, y^+_{T+H}]$.  By setting $a_t = a_{t|T+H} - a^+_{t|T+H} + a^+$ adjusts the mean of $a^+_t$ to yield a draw from the desired conditional distribution. In practice, it is more efficient to first construct $y^* = y-y^+$ and run the Kalman smoother only once, yielding $a^*_{t|T+H} = E[a^*_t|y^*]$. A draw from $p(\alpha_{1}, \dots, \alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})$ is then obtainted by setting $a_t = a^*_{t|T+H} + a^+_t$. Note that since only an estimate of the smoothed mean of the states is required, more efficient smoothing recursions can be employed which do not require the storage of $a_{t|t}$ and $P_{t|t}$ \citep[see][ch. 4.4.2]{durbinkoopman2002_biomtr}.

\subsubsection{Drawing from the (conditional) predictive density $p(y^f|I, \Theta)$}
Under the common assumption of uncorrelated measurement errors, draws from the predictive density $p(y^{\sf{f}}_t\,|\,a_t, y^o, y^c, \Theta)$ can  be obtained by sampling the measurement errors independently from $\mathcal{N}(0, \omega_i) \: \forall i s.t. y_{i,t} \notin y_{c,t}$ and adding them to the draw of the state vector \citep[see][for the general case of a full covariance matrix $\Omega$]{bgl_2015ijf}. For the \citet{durbinkoopman2002_biomtr} simulation smoother the generated artifical observations $y^+$ can be re-used, since $F a^*_{t|T+H} + y^+_t$ is a draw of the observations conditional on the sampled states. In the case of a VAR, there is no 
measurement error so that $y^{\sf{f}}_t$ can be set equal to the corresponding elements of $a_t$. 

\subsubsection{Multiple draws from $p(y^f|I, \Theta)$}
Additional draws from the (conditional) predictive density given the same set of parameters $\Theta$ can be produced recursively from the \citet{carterkohn1994_biomtr} simulation smoother by independently sampling $a_t$ as many times as required and conditioning on the draw to generate $y^{\sf{f}}_t$. In the case of the \citet{durbinkoopman2002_biomtr} simulation smoother, each additional draw requires new simulated values of the states and observations, $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Given these, the entire filtering and smoothing recursions need to be re-run. However, computational savings still arise as the state covariances $P_{t|t}$ and functions thereof that are calculated during the recursions do not depend on the observations. As such they only need be calculated once and can be passed on to subsequent runs of the Kalman filter and smoother \citep[p. 606]{durbinkoopman2002_biomtr}.

\subsubsection{Initialisation}
In the case of a dynamic factor model, the Kalman filter is initialized by setting $a_{1|0}$ and $P_{1|0}$ equal to the unconditional mean and variance, respectively, of the factor VAR. Vector autoregression applications only require the simulation smoother to run over the sample with missing observations, i.e. $t\geq T+1$. The recursions are therefore initialized with $a_{T+1|T} = \mathbf{T} \, \transpose{[\transpose{\mathbf{y}}_T, \dots, \transpose{\mathbf{y}}_{T-P+1}]}$ and $P_{T+1|T} = \Sigma$, unless the more efficient one-step algorithm in \citet{durbinkoopman2002_biomtr} is employed, in which case $a_{T+1|T} = 0$ is required \citep[see][]{jarocinski2015csda}.



\end{document}
