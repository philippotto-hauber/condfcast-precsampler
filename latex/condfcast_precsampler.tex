\documentclass[notitlepage,a4paper,12pt]{article}
\usepackage[authoryear]{natbib}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.25cm,bottom=2.25cm,nohead}
\usepackage{mathtools}
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--} % dash instead of "to" in range
%\usepackage{amsfonts} 
%\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
%\usepackage[bitstream-charter]{mathdesign}
\usepackage[charter, cal = cmcal]{mathdesign}
\usepackage[T1]{fontenc}
\mathchardef\shortminus="2D
%\newcommand{\shortminus}{-}
% % font used in Korobilis and Schumacher (2014)
%\usepackage[sc]{mathpazo}
%\linespread{1.05}         % Palladio needs more leading (space between lines)
%\usepackage[T1]{fontenc}
% % alternative I found on the web
%\usepackage{tgpagella}
%\usepackage[T1]{fontenc}
\usepackage{xfrac}

\newcommand{\transpose}[1]{{#1}^{\sf{T}}}

\linespread{1.2}
\setlength\parindent{0pt}

\begin{document}
\title{Precision-based algorithms for conditional forecasting and scenario analysis in large state space models}

\author{
  Philipp Hauber\\
  Kiel Insitute for the World Economy
}
\date{\today}

\maketitle

\section{Introduction}

Conditional forecasting is important \citep{bgl_2015ijf}

Precision-based sampling algorithms have been proposed in the literature as an alternative to Kalman-filter based methods \citep{chanjeliazkov_2009}.

This paper proposes precision-based sampling algorithms for conditional forecasting and scenario analysis as an alternative to Kalman-filter based approaches. In an empirical application involving large Bayesian dynamic factor models and vector autoregressions I demonstrate its usefulness. 

\section{Precision-based sampling algorithms for conditional forecasting}\label{sec:precsampler}

\subsection{Precision-based sampling}\label{subsec:precsampler}

Let $\mathbf{y_t} = \transpose{[y_{1t}, \dots, y_{Nt}]}$ denote an $N \times 1$ vector of observations and $\mathbf{f_t}$ an $S \times 1$ vector of unobserved states. Consider the following state space model which $\forall \: t = 1:T$ 

\begin{subequations}
    \label{eqn:statespacesys}
    \begin{align}
        \mathbf{y}_t &= \mathbf{d}_t + \mathbf{F} \,\boldsymbol{\alpha}_t + \boldsymbol{\varepsilon}_t \label{eqn:statespacesys_meas}\\
        \boldsymbol{\alpha_t} &= \mathbf{c}_t + \mathbf{T} \, \boldsymbol{\alpha}_{t-1} + \boldsymbol{\upsilon}_t \label{eqn:statespacesys_trans}
    \end{align}
\end{subequations}

where 
$$
\begin{bmatrix}
    \boldsymbol{\varepsilon}_t \\
    \boldsymbol{\upsilon}_t
\end{bmatrix}
\sim \mathcal{N}(0,
\begin{bmatrix}
\Omega & \mathbf{0} \\
\mathbf{0} & \Sigma
\end{bmatrix}
)
$$
and $\Omega = \text{\sf{diag}}(\omega^2_{1}, \dots, \omega^2_{N})$. The recursions are initialized with $\boldsymbol{\alpha_1} \sim \mathcal{N}(\mathbf{c_1}, \Sigma_0)$.

Bayesian estimation of \Crefrange{eqn:statespacesys_meas}{eqn:statespacesys_trans} requires a draw from the distribution of the state vector conditional on the observations and parameters $\Theta$, $p(\boldsymbol{\alpha}_1, \dots, \boldsymbol{\alpha}_T | \mathbf{y}_1, \dots, \mathbf{y}_T, \Theta)$. "Simulation smoothing" methods based on the Kalman filter/smoother have been proposed by Carter and Kohn (1994) and Durbin and Koopman (2001) and are widely used in the literature.

Precision-based sampling algorithms \citep{chanjeliazkov_2009} provide an alternative to these methods. To derive the sampler, we stack observations and states over time - $\mathbf{y} = \transpose{[\transpose{\mathbf{y}}_1, \dots, \transpose{\mathbf{y}}_T]}$, $\boldsymbol{\alpha} = \transpose{[\transpose{\boldsymbol{\alpha}}_1, \dots, \transpose{\boldsymbol{\alpha}}_T]}$ - yielding the following measurement equation

\begin{equation}
\mathbf{y} = \mathbf{d} + \mathbf{M} \, \boldsymbol{\alpha} + \boldsymbol{\varepsilon}
\end{equation} 

where $ \mathbf{M} = I_T \otimes \mathbf{F}$ and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{V}_{\varepsilon})$ with $\mathbf{V}_{\varepsilon} = I_T \otimes \Omega$. The transition equation of the states can be written compactly as $\mathbf{H}\, \boldsymbol{\alpha} = \mathbf{c} + \upsilon$ using the differencing matrix 
$$
\mathbf{H}
=
\begin{bmatrix}
    I_R & 0_R & \dots \\
    \shortminus \mathbf{T} & I_R & \ddots \\
    0_R & \shortminus \mathbf{T} & I_R & \ddots \\
    \vdots & \ddots & \ddots & \ddots \\
    & & & \shortminus \mathbf{T} & I_R
\end{bmatrix}.
$$

The marginal distribution of the states, i.e. unconditional of the data, is $\mathcal{N}(\tilde{\mathbf{c}},\mathbf{H}^{-1} \bm{V}_{\upsilon} \transpose{\mathbf{H}^{-1}})$, where 

$$
\tilde{\mathbf{c}}
=
\mathbf{H}^{-1} \mathbf{c}
, \:
\mathbf{V}_{\upsilon} = 
\begin{bmatrix}
    \Sigma_{0} & 0_R & \dots \\
    0_R & \Sigma & \ddots \\
    \vdots & \ddots & \ddots \\
    & & & \Sigma 
\end{bmatrix}
$$

The joint distribution of states and observables is Normal with mean $\boldsymbol{\mu}_z$ and covariance matrix $\bm{V}_z$, where we have defined: 


\begin{equation}
    \mathbf{z} 
    \coloneqq 
    \begin{bmatrix}
        \boldsymbol{\alpha} \\
        \mathbf{y}
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        \tilde{\mathbf{c}} \\
        \mathbf{d} 
    \end{bmatrix}
    +
    \begin{bmatrix}
        I_{TR} & 0 \\
        \mathbf{M} & I_{TN} 
    \end{bmatrix}
    \begin{bmatrix}
        \boldsymbol{\alpha}\\
        \boldsymbol{\varepsilon}
    \end{bmatrix} 
\end{equation}

 The precision matrix of $\mathbf{z}$ is the inverse of the covariance matrix: 

\begin{align*}
    \mathbf{Q}_z 
    &= 
    \bm{V}^{-1}_z \\
    &= 
    \text{\sf{Var}}\left(
    \begin{bmatrix}
        \boldsymbol{\alpha} \\
        \mathbf{y}
    \end{bmatrix}
    \right)^{-1} \\
    &=  
    \left(
    \begin{bmatrix}
        I & 0 \\
        \mathbf{M} & I
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{H}^{-1} \bm{V}_{\upsilon} \transpose{\mathbf{H}^{-1}} & 0 \\
        0 & \bm{V}_{\varepsilon}
    \end{bmatrix} 
    \begin{bmatrix}
        I & \transpose{\mathbf{M}} \\
        0 & I
    \end{bmatrix}
    \right)^{-1}\\
    &=
    \begin{bmatrix}
        I & \shortminus\transpose{\mathbf{M}} \\
        0 & I
    \end{bmatrix}
    \begin{bmatrix}
        \transpose{\mathbf{H}} \bm{V}^{-1}_{\upsilon} \mathbf{H} & 0 \\
        0 & \bm{V}^{-1}_{\varepsilon}
    \end{bmatrix} 
    \begin{bmatrix}
        I & 0 \\
        \shortminus\mathbf{M} & I 
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
         \transpose{\mathbf{H}} \bm{V}^{-1}_{\upsilon} \mathbf{H} + \transpose{\mathbf{M}}\bm{V}_{\varepsilon}\mathbf{M}  & \shortminus \transpose{\mathbf{M}} \bm{V}^{-1}_{\varepsilon} \\
         \bm{V}^{-1}_{\epsilon} \mathbf{M} & \bm{V}^{-1}_{\varepsilon}
    \end{bmatrix} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\alpha} & \mathbf{Q}_{\alpha y} \\
        \transpose{\mathbf{Q}}_{\alpha y} & \mathbf{Q}_{y}  
    \end{bmatrix}
\end{align*}

Using standard results for multivariate Normal distributions \citep[e.g.][pp. 86-87]{bishop_prml_2006}, we can derive the desired conditional distribution:

\begin{align*}
    p
    \left(
        \boldsymbol{\alpha} \mid \mathbf{y}, \Theta
     \right) 
    &\sim
    \mathcal{N}
    \left(
    \mathbf{Q}^{-1}_{\alpha} \left( \mathbf{Q}_{\alpha}\boldsymbol{\mu}_{\alpha} - \mathbf{Q}_{\alpha y} \left(\mathbf{y} - \boldsymbol{\mu}_{\text{y}}\right)\right), \mathbf{Q}_{\alpha}^{-1}
    \right) \\
    &\coloneqq \mathcal{N}
        \left(
            \boldsymbol{\mu}_{\alpha\mid y}, \mathbf{Q}_{\alpha\mid y}^{-1}
        \right) \\
\end{align*}

with \citep[see also][eqn. 6-8]{chanjeliazkov_2009}:
\begin{align*}
    \mathbf{Q}_{\alpha\mid y} &= \mathbf{Q}_{\alpha} \\
    &= 
    \transpose{\mathbf{H}} \mathbf{V}^{\,-1}_{\!\upsilon} \mathbf{H} + {\transpose{\mathbf{M}} \mathbf{V}^{\,-1}_{\!\epsilon}\mathbf{M}}\\
    \boldsymbol\mu_{\alpha\mid y} &= \mathbf{Q}_{\alpha}^{-1} \left(\mathbf{Q}_{\alpha} \tilde{\mathbf{c}}+ \transpose{\mathbf{M}} \mathbf{V}_{\!\epsilon}^{\,-1} \left(\mathbf{y} - \mathbf{d}\right) \right)
\end{align*}

Draws from this distribution can be obtained efficiently since the $T\cdot S \times S$ matrix $\mathbf{Q}_{\alpha}$ is banded and its inversion not required. Specifically, calculate the conditional mean following \citet[][Algorithm 2.1]{rueheld_2005}. This requires the computation of the lower Cholesky factor $\mathbf{L}$ of $\mathbf{Q}_{\alpha}$. Then draw $\mathbf{v} \sim \mathcal{N}(0,I_{TS})$, solve $\transpose{\mathbf{L}} \mathbf{x} = \mathbf{v}$ for $\mathbf{x}$ and set $\boldsymbol{\mu}_{\alpha\mid y} +\mathbf{x}$, yielding a draw of $\boldsymbol{\alpha}$ conditional on $\mathbf{y}$ \citep[][Algorithm 2.4]{rueheld_2005}. \\

\subsection{Forecasting}

In addition to a draw of the state vector conditional on the data, macroeconomic applications may require forecasts - both unconditional and conditional on future paths of other variables - from the model in \Crefrange{eqn:statespacesys_meas}{eqn:statespacesys_trans}. 

From a Bayesian perspective, the aim is to sample from the predictive density 

$$
p(\mathbf{y^{\sf{f}}}|\mathcal{I}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathcal{I}, \boldsymbol{\alpha},\Theta) p(\boldsymbol{\alpha}|\mathcal{I}, \Theta) p(\Theta|\mathcal{I})\, \text{d}\Theta
$$

where $\mathcal{I}$ denotes the relevant conditioning information and $p(\Theta|\mathcal{I})$ the posterior distribution of the parameters.\\

Draws from this density can be obtained by applying the insights in Hauber and Schumacher (2021) who generalize the precision sampler outlined above to applications with missing data. Since future values of the variables can simply be considered as missing, draws from the joint distribution of states and forecasts conditional on the data (and possibly the future paths of a subset of the variables) can be sampled efficiently.\\

To illustrate the approach, I focus on unconditional forecasts first (Section \ref{sec:uncondfcast}). In Section \ref{sec:condfcast} I consider forecasts conditional on a subset of the observations in the forecast period. In this case, different permutations and an additional step is required when the impact of the conditioning values on the posterior distribution is taken into account of the parameters is taken into account. Moreover, I show how the algorithms can be used to produce conditional forecasts if the restrictions take the form of a pre-specified interval rather than a specific value (soft conditioning).\\

In what follows, I will not be concerned with the estimation of model parameters and assume that random draws from $p(\Theta|\mathcal{I})$ are readily available. Also, for the sake of exposition, I assume that between periods $t=1$ and $t=T$ all observations are available. Situations where this is not the case, e.g. because some series start at a later date in the sample or because outlying observations have been removed by setting them to missing, can naturally be accommodated. Moreover, I assume that the intercepts $\mathbf{c}_{T+1}, \dots, \mathbf{c}_{T+H}$ and $\mathbf{d}_{T+1}, \dots, \mathbf{d}_{T+H}$ are known at $t=T$. This assumption is reasonable to the extent that these reflect (time-invariant) constant terms, deterministic trends or seasonal and calendar components. 

\subsubsection{Unconditional forecasts}\label{sec:uncondfcast}

Let $h = 1:H$ denote the forecast horizons and $\mathbf{y}^{\sf{f}} = \transpose{[\mathbf{y}^{\sf{T}}_{T+1}, \dots, \mathbf{y}^{\sf{T}}_{T+H}]}$. In contrast, the observations through $T$ are denoted as $\mathbf{y}^{\sf{o}} = \transpose{[\mathbf{y}^{\sf{T}}_{1}, \dots, \mathbf{y}^{\sf{T}}_{T}]}$. The predictive density is given by

\begin{equation}\label{eqn:preddens_uncond}
p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \Theta) p(\Theta|\mathbf{y^{\sf{o}}})\, \text{d}\Theta
\end{equation}

To evaluate Equation \ref{eqn:preddens_uncond}, derive the joint distribution of states and (actually observed and missing) observations: 

\begin{align*}
    \mathbf{z} &= \transpose{[\transpose{\boldsymbol{\alpha}}, \transpose{\mathbf{y}}]} \\
    \mathbf{y} &= \transpose{[\transpose{\mathbf{y}^{\sf{o}}}, \transpose{\mathbf{y}^{\sf{f}}}]} \\
    \boldsymbol{\alpha} &= [\boldsymbol{\alpha}_1, \dots, \boldsymbol{\alpha}_{T+H}].
\end{align*}

It follows that the distribution of $\mathbf{z}$ is Normal with mean $\boldsymbol{\mu}_{z}$ and precision matrix $\mathbf{Q}_{z}$ as in Section \ref{subsec:precsampler}. To sample efficiently from the conditional distribution $p\left(\mathbf{y}^{\sf{f}}, \boldsymbol{\alpha} \mid \mathbf{y}^{\sf{o}}, \Theta \right)$  requires a reordering or permutation of $\mathbf{z}$. To this end, define the permutation matrix $\mathcal{P}$ that reorders $\mathbf{z}$ such that the states and elements of $\mathbf{y^f}$ - ordered by time period $t$ - are placed first. That is to say,

\begin{align*}
\mathbf{z_{\mathcal{P}}} \coloneqq \mathcal{P} \mathbf{z} 
&=
 \transpose{[
     \alpha^{\sf{T}}_1, 
     \dots, 
     \alpha^{\sf{T}}_T, 
     \alpha^{\sf{T}}_{T+1}, 
     \mathbf{y}^{\sf{T}}_{T+1}, 
     \dots, 
     \alpha^{\sf{T}}_{T+H}, 
     \mathbf{y}^{\sf{T}}_{T+H},
     \mathbf{y}^{\sf{T}}_1,
     \dots,
     \mathbf{y}^{\sf{T}}_T]}\\
&= \transpose{[\mathbf{z}^{\sf{T}}_{\alpha\! y^{\sf{f}}}, \mathbf{y}^{\sf{o}}]}
\end{align*}

Note that the conditioning argument $\mathbf{y}^{\sf{o}}$ is now ordered last. Moreover, by placing those components of $\mathbf{z}$ which are conditionally dependent close to each other, the bandwidth of $\mathbf{Q}_{z_{\mathcal{P}}}$ is kept small. The permuted vector of states and observations is also Normal with (permuted) moments given by 

\begin{equation*}
    \mathbf{z_{\mathcal{P}}} \sim \mathcal{N}\left(\mathcal{P}\boldsymbol{\mu}_z, \left(\mathcal{P} \mathbf{Q}_z \transpose{\mathcal{P}}\right)^{-1}\right)
\end{equation*}

where we have made use of the fact that the inverse of a permutation matrix is equal to its transpose.  The precision matrix of $\mathbf{z}_{\mathcal{P}}$ can be partitioned as follows: 

\begin{align*}
    \mathbf{Q}_{z_{\mathcal{P}}} &\coloneqq  \mathcal{P} \mathbf{Q}_z \transpose{\mathcal{P}} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\alpha\! y^{\sf{f}}} & \mathbf{Q}_{\alpha\! y^{\sf{f}}, y^{\sf{o}}} \\ 
        \mathbf{Q}^{\sf{T}}_{\alpha\! y^{\sf{f}}, y^{\sf{o}}} & \mathbf{Q}_{y^{\sf{o}}}
    \end{bmatrix}
\end{align*}

where $\mathbf{Q}_{\alpha\! y^{\sf{f}}, y^{\sf{o}}}$ is a 
$T\!\cdot\!R + H\!\cdot\!N \times T\!\cdot\!N$ matrix and $\mathbf{Q}_{\alpha\! y^{\sf{f}}}$ and $\mathbf{Q}_{y^{\sf{o}}}$ are square with dimensions $T\!\cdot\!R + H\!\cdot\!N$ and $T\!\cdot\!N $, respectively. Similarly, the mean of $\mathbf{z}_{\mathcal{P}}$ can be partitioned as 
$$
\boldsymbol{\mu}_{z_{\mathcal{P}}} = 
\begin{bmatrix}
    \boldsymbol{\mu}_{\alpha\! y^{\sf{f}}} \\
    \boldsymbol{\mu}_{y^{\sf{o}}}
\end{bmatrix}
$$

Denote by $\mathbf{z}^*$ a draw from the conditional distribution 

\begin{equation*}
    \mathbf{z}_{\alpha\! y^{\sf{f}}} | \mathbf{y}^{o} \sim 
    \mathcal{N}\left(
        \mathbf{Q}^{-1}_{\alpha\! y^{\sf{f}}}\left( \mathbf{Q}_{\alpha\! y^{\sf{f}}}\boldsymbol{\mu}_{\alpha\! y^{\sf{f}}} - \mathbf{Q}_{\alpha\! y^{\sf{f}}, y^{\sf{o}}}\left(\mathbf{y}^o-\boldsymbol{\mu}_{y^{\sf{o}}}\right)\right)
    ,
    \mathbf{Q}_{\alpha\! y^{\sf{f}}}
    \right).
\end{equation*}

Then by reversing the permutation, i.e 

$$
\begin{bmatrix}
    \boldsymbol{\alpha} \\
    \mathbf{y^{o}} \\
    \mathbf{y^{\sf{f}}}
\end{bmatrix}
= 
\mathcal{P}^{-1} 
\begin{bmatrix}
    \mathbf{z}^*\\
    \mathbf{y^{o}}
\end{bmatrix}
$$

we can back out the draws of $\boldsymbol{\alpha}$ and $\mathbf{y}^{\sf{f}}$ from the original conditional density - $p(\mathbf{y}^{\sf{f}},\boldsymbol{\alpha} | \mathbf{y}^{o},\Theta)$.\\

The following algorithm summarizes the required steps to generate $G$ independent draws from the predictive density in Equation \ref{eqn:preddens_uncond}:\\

 \noindent\textbf{Algorithm 1: Draws from the predictive density $p(\mathbf{y}^f | \mathbf{y}^o)$}

\begin{enumerate}
    \item Draw $\Theta^{(m)}$ from $p(\Theta | y^o)$
    \item Construct the system matrices $\mathbf{H}, \mathbf{M}, \mathbf{V}_{\!\varepsilon}, \mathbf{V}_{\!\upsilon}, \tilde{\mathbf{c}}, \mathbf{d}$
    \item Compute the mean $\boldsymbol{\mu}_{z}$ and precision matrix $\mathbf{Q}_{z}$ of the joint distribution of $\mathbf{z} = [\boldsymbol{\alpha}, \mathbf{y}]$  
    \item Permute $\mathbf{z}$, yielding $\mathbf{z}_{\mathcal{P}} \sim \mathcal{N}(\mu_{z_\mathcal{P}}, Q_{z_\mathcal{P}}^{-1})$ with partitions $\transpose{[\mathbf{z}^{\sf{T}}_{\alpha\! y^{\sf{f}}}, \mathbf{y}^{\sf{o}}]}$
    \item Sample from the conditional distribution $\mathbf{z}_{\alpha\! y^{\sf{f}}} | \mathbf{y}^{\sf{o}}$ using \citep[][Algorithm 2.1 and 2.4]{rueheld_2005}
    \item Reverse the permutation to back out the draw of $\mathbf{y}^{\sf{f}}$
    \item Repeat steps 1.-6. $G$ times
\end{enumerate}


\subsubsection{Conditional forecasting}\label{sec:condfcast}

In certain applications, forecasts conditional on a subset of the future values $y_{T+1}, \dots, y_{T+H}$ may be required. Examples include projections of inflation which take assumptions regarding the macroeconomic environment into account \citep{giannone_etal_2014_ijf} or external nowcasts as "jump-off" points for longer-term forecasts in reduced form or structural models \citep{faustwright2009_jbes,wolters2015_jae}; \citet{knotekzaman2019_ijof} show how macroeconomic forecasts can be improved by conditioning on nowcasts of financial variables.\\

Extending the notation, let $\mathbf{y}_{c,t}\: \forall t = T+1:T+H$ denote the observations which are being conditioned on. The entire conditioning set is given by 
$$
\mathbf{y}^{\sf{c}} = \transpose{[\mathbf{y}^{\sf{T}}_{c,T+1}, \dots, \mathbf{y}^{\sf{T}}_{c,T+H}]}
$$ 
with $N^c = \text{dim}(\mathbf{y}^{\sf{c}})$. Conversely, $\mathbf{y}_{-c,t} = \mathbf{y}_t\notin\mathbf{y}_{c,t}$ denotes the observations for which no conditioning information is available at time $t$ and $\mathbf{y}^{\sf{f}} = \transpose{[\mathbf{y}^{\sf{T}}_{-c,T+1}, \dots, \mathbf{y}^{\sf{T}}_{-c,T+H}]}$. 

In principle, conditional forecasts can be sampled in much the same way as outlined above. However, some conceptual and computational differences arise. Firstly note that in this case, the predictive density is given by

\begin{equation}\label{eqn:preddens_cond}    
    p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}},\mathbf{y^{\sf{c}}}, \Theta) p(\Theta|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})\, \text{d}\Theta
\end{equation}

which takes into account that the conditioning set $y^c$ contains information about which $\Theta$ are more likely a posteriori. \\

Secondly, to draw from $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}},\mathbf{y^{\sf{c}}}, \Theta)$ requires a different permutation of the joint vector of observables and states. With $\mathbf{z} = \transpose{[\transpose{\boldsymbol{\alpha}}, \transpose{\mathbf{y}}]}$ defined as in Section \ref{sec:uncondfcast}, let $\mathcal{P}'$ be a permutation matrix that reorders $z$ as follows:

$$
\mathbf{z}_{\mathcal{P}'} \coloneqq \mathcal{P}' \mathbf{z} =
 \transpose{[
     \alpha^{\sf{T}}_1, 
     \dots, 
     \alpha^{\sf{T}}_T, 
     \alpha^{\sf{T}}_{T+1}, 
     \mathbf{y}^{\sf{T}}_{c,T+1}, 
     \dots, 
     \alpha^{\sf{T}}_{T+H}, 
     \mathbf{y}^{\sf{T}}_{c,T+H},
     \mathbf{y}^{o\sf{T}},
     \mathbf{y}^{c\sf{T}}]}.
$$

The first and second moments are permuted accordingly, yielding 

\begin{align*}
\boldsymbol{\mu}_{z_{\mathcal{P}'}} &\coloneqq \mathcal{P}' \boldsymbol{\mu}_z = 
\begin{bmatrix}
    \boldsymbol{\mu}_{\alpha\! y^{\sf{f}}} \\
    \boldsymbol{\mu}_{y^{\sf{o}}\!y^{\sf{c}}}
\end{bmatrix}
\end{align*}

and 

\begin{align*}
    \mathbf{Q}_{z_{\mathcal{P}'}} &\coloneqq  \mathcal{P}' \mathbf{Q}_z \transpose{\mathcal{P}'} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\alpha\! y^{\sf{f}}} & \mathbf{Q}_{\alpha\! y^{\sf{f}}, y^{\sf{o}}\!y^{\sf{c}}} \\ 
        \mathbf{Q}^{\sf{T}}_{\alpha\! y^{\sf{f}}, y^{\sf{o}}\! y^{\sf{c}}} & \mathbf{Q}_{y^{\sf{o}}\!y^{\sf{c}}}
    \end{bmatrix}
\end{align*}

Then a draw of the $T\!\cdot\!N + H\!\cdot\!N - N^c$ vector of states and conditional forecasts given parameters can be obtained from the permuted moments of the joint distribution $\boldsymbol{\mu}_{z_{\mathcal{P}'}} $ and $\mathbf{Q}_{z_{\mathcal{P}'}} $ by conditioning on $\mathbf{y^{o}}$ and $\mathbf{y^c}$. As above, the desired draw of $y^f$ can be backed out by reversing the permutation.\\

Building on this, draws from the predictive density $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})$ can then be obtainted by the following algorithm \citep[see also][Algorithm 1]{waggonerzha1999_res}:\\

\noindent\textbf{Algorithm 2: Draws from the predictive density $p(\mathbf{y}^f | \mathbf{y}^o, \mathbf{y}^c)$}\\

Initialize $\Theta^{0}$ from $p(\Theta | y^o)$. For $g=1:(G_0+G)$

\begin{enumerate}    
    \item Construct the system matrices $\mathbf{H}, \mathbf{M}, \mathbf{V}_{\!\varepsilon}, \mathbf{V}_{\!\upsilon}, \tilde{\mathbf{c}}, \mathbf{d}$ given the parameters $\Theta^{(g-1)}$
    \item Compute the mean $\boldsymbol{\mu}_{z}$ and precision matrix $\mathbf{Q}_{z}$ of the joint distribution of $\mathbf{z} = [\boldsymbol{\alpha}, \mathbf{y}]$  
    \item Permute $\mathbf{z}$, yielding $\mathbf{z}_{\mathcal{P}'} \sim \mathcal{N}(\mu_{z_\mathcal{P}'}, Q_{z_\mathcal{P}'}^{-1})$ with partitions $\transpose{[\mathbf{z}^{\sf{T}}_{\alpha\! y^{\sf{f}}}, \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}]}$
    \item Sample from the conditional distribution $\mathbf{z}_{\alpha\! y^{\sf{f}}} | \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}, \Theta^{(g-1)}$ using \citep[][Algorithm 2.1 and 2.4]{rueheld_2005} and reverse the permutation to back out the draws of $\mathbf{y}^{\sf{f}}$ and $\boldsymbol{\alpha}$
    \item Draw $\Theta^{(g)}$ from $p(\Theta|\mathbf{y}^o, \mathbf{y}^{\sf{c}}, {\mathbf{y}^{\sf{f}}}^{(g)}, \alpha^{(g)})$ 
 \end{enumerate}

 Discard the first $G_0$ draws as burn-in.\\

Note that Hauber and Schumacher (2021) also propose a sequential sampler that first draws the missing observations conditional on the states and then the states given a complete data set. This approach performs somewhat better in terms of computing time and could in principle be used in Algorithms 2, which in any case requires a Gibbs sampling step to update the parameters given the conditioning set. However, the gains in performance from faster sampling of states and forecasts may not outweigh the longer burn-in required due to the additional block in the Gibbs Sampler.\\

If the sample information and prior dominate the information in $y^{\sf{c}}$, the impact of the conditioning arguments on the posterior distribution of the parameters is likely to be negligible and $p(\Theta | y^o, y^c) \approx p(\Theta | y^o)$ \citep{delnegro_schorfheide_2013_hb,bgl_2015ijf}. In such applications, draws from Equation \ref{eqn:preddens_cond} can then be obtained from Algorithm 1, using the permutation matrix $\mathcal{P}'$ instead of $\mathcal{P}$.

\subsubsection{Soft conditions and repeated samples from $p(y^f|y^o, \Theta^{(g)})$}

So far we have considered conditional forecasts that fix the future values of some of the endogeneous variables at single points. \citet{waggonerzha1999_res} labels these "hard" conditions to distinguish them from soft conditions where each element of $y^c$ is merely restricted to lie within a pre-specified range, i.e. $y^c_l \leq y^c \leq y^c_u$. If the probability that the soft conditions are satisified is not too small, then draws from the unconditional predictive density $p(y^f|y^o, \Theta^{(g)})$ can be obtained until $y^c_l \leq y^c \leq y^c_u$.\footnote{Similarly, hard and soft conditions can be combined, e.g. if inflation forecasts are produced conditional on an oil price path and subject to the restriction that the Federal Funds rate is positive. In such a scenario, draws from $p(y^f|y^o, y^c, \Theta^{(g)})$ are obtained until the soft restrictions are satisfied.}\\

The precision-sampling algorithms outlined above are particularly suited to this task since the bottleneck in the calculations is the Cholesky factorization of the band matrix $Q_{\alpha\!y^{\sf{f}}}$ which requires $((T+H)S+NH)\cdot b_w^2$ floating point operations where $b_w$ is the lower bandwidth of $Q_{\alpha, y^f}$ \citep[][4.3.5]{GolubvanLoan2013}. However, it only needs to be computed once, irrespective of the number of desired draws \citep{rue2001_jrss}. Repeated samples thus come at a significantly reduced computational burden, consisting only of $(T+H)S+NH$ independent draws from the standard Normal distribution, band backward substitution - which requires $2 ((T+H)S+NH) \cdot b_w$ floating point operations \citep[][4.3.2]{GolubvanLoan2013}- and a vector addition.  

\section{Models}\label{sec:models}

\subsection{Dynamic factor models}\label{sec:dfm}

Factor models feature prominently in the macroeconomic literature. By exploiting the cross-correlation in typical macroeconomic datasets, they can effectively handle large cross-sections without suffering from the "curse of dimensionality". Moreover, they are naturally represented as state space models, facilitating Frequentist and Bayesian estimation as well as the analysis of missing observations and mixed frequencies. For an overview of the methodological developments of factor models and their applications, see \citet{stockwatson2016_hbmacro}. Precision-based sampling applications include \citet{chanjeliazkov_2009,mccausland_factor2015,kaufmannschumacher_jae2017,kaufmannschumacher_jectrcs2019}\\

A \textit{dynamic factor model} is defined as

\begin{subequations}
    \label{eqn:factormodel}
    \begin{align}
        \mathbf{y}_t &= \sum_{k=0}^K \lambda_k \, \mathbf{f}_{t-k} + \mathbf{e}_t \\
        \mathbf{f}_t &= \sum_{p=1}^P \phi_p \, \mathbf{f}_{t-p} + \boldsymbol{\upsilon}_t \\
        \mathbf{e}_t &= \sum_{j=0}^J \psi_j \, \mathbf{e}_{t-j} + \boldsymbol{\epsilon}_t 
    \end{align}
\end{subequations}

where $\mathbf{f}_t$ denotes an $R \times 1$ vector of unobserved factors which summarizes the co-movement of the observables $\mathbf{y}_t$. The dynamics of the factors are modelled as a vector autoregression of order $P$. The idiosyncratic components, $\mathbf{e}_t$ are modeled as independent autoregressive processes of order $J$, i.e. $\psi_1, \dots, \psi_J$ are diagonal matrices. The $N \times R$ loadings matrices $\mathbf{\lambda}_k$ capture the dynamic relationships between observables and factors. Setting $K=0$, so that the variables only load contemporaneously on the factors, yields a \textit{static factor model}. The innovations $\epsilon_t$, $\upsilon_t$ are Normal, uncorrelated at all leads and lags and their covariance matrices given by $\Omega$ and $\Sigma$, respectively. \\

To apply the precision-based sampling algorithms outlined in Section \ref{sec:precsampler} for the general state space model, requires the dynamic loadings structure and the autocorrelation in the idiosyncratic components are taken into account when constructing the system matrices. As above, stacking the observables, factors and idiosyncratic components over $t$ yields 

\begin{equation}
    \mathbf{y} = \mathbf{M} \, \mathbf{f} + \mathbf{e}
\end{equation} 

\noindent where 

$$
\mathbf{M}
=
\begin{bmatrix}
    \boldsymbol{\lambda_0} &   \\
    \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}   \\
    \boldsymbol{\lambda_2} & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0} \\
     & \ddots & \ddots & \ddots \\
     & \boldsymbol{\lambda_K} &\dots & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}
     &  \\
     & & \ddots & \ddots & \ddots & \ddots \\
    & & & \boldsymbol{\lambda_K} &\dots & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}
\end{bmatrix}.
$$

The vector of idiosyncratic components can be written as $\mathbf{H}_e\, \mathbf{e} = \boldsymbol{\varepsilon}$ with 

$$
\mathbf{H}_e
=
\begin{bmatrix}
    I_N &  \\
    \shortminus \psi_1 & I_N &  \\
    \shortminus \psi_2 & \shortminus \psi_1 & I_N &  \\
     &  &  &  &  & \\
    \shortminus \psi_J & \dots & \dots & \shortminus \psi_1 & I_N \\
     &  &  &  &  & \\
    & & \shortminus \psi_J & \dots & \dots & \shortminus \psi_1 & I_N
\end{bmatrix}
$$

\noindent and $\boldsymbol{\varepsilon} \sim \mathcal{N}(0_{NT}, I_T \otimes \boldsymbol{\Omega})$. It follows that $\mathbf{e}$ is Normal, with mean $0$ and covariance matrix $\bm{V}_{e} = (\transpose{\mathbf{H}}_{e} (I_T \otimes \boldsymbol{\Omega}^{-1}) \mathbf{H}_e)^{-1}$. \\

In a similar fashion, the vector of factors is distributed as 

$$
\mathbf{f} \sim \mathcal{N}\left(0, (\transpose{\mathbf{H}}_{\sf{f}} (I_T \otimes \boldsymbol{\Sigma^{-1}}) \mathbf{H}_{\sf{f}})^{-1}\right)
$$

with 

$$
\mathbf{H}_{\sf{f}}
=
\begin{bmatrix}
    I_N &  \\
    \shortminus \phi_1 & I_N &  \\
    \shortminus \phi_2 & \shortminus \phi_1 & I_N &  \\
     &  &  &  &  & \\
    \shortminus \phi_P & \dots & \dots & \shortminus \phi_1 & I_N \\
     &  &  &  &  & \\
    & & \shortminus \phi_P & \dots & \dots & \shortminus \phi_1 & I_N
\end{bmatrix}.
$$

As above, the precision matrix of the joint vector $\mathbf{z} = \transpose{[\transpose{\mathbf{f}}, \transpose{\mathbf{y}}]}$ is then given by 

\begin{align*}
    \mathbf{Q}_z 
    &= 
    \begin{bmatrix}
         \transpose{\mathbf{H}_{\sf{f}}} \bm{V}^{-1}_{\upsilon} \mathbf{H}_{\sf{f}} + \transpose{\mathbf{M}}\bm{V}_{e}\mathbf{M}  & \shortminus \transpose{\mathbf{M}} \bm{V}^{-1}_{e} \\
         \shortminus \bm{V}^{-1}_{e} \mathbf{M} & \bm{V}^{-1}_{e}
    \end{bmatrix} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\sf{f}} & \mathbf{Q}_{\sf{f}\text{y}} \\
        \transpose{\mathbf{Q}}_{\sf{f} \text{y}} & \mathbf{Q}_{\text{y}}  
    \end{bmatrix}
\end{align*}

which allows to efficiently sample from $p(\mathbf{y}^{\sf{f}}\,|\, I, \Theta)$ using Algorithms 1 and 2. 

\subsection{Vector autoregressions}\label{sec:var}

Vector autoregressions (VAR) are another class of popular macroeconometric models. Given their flexibility, large VAR are heavily overparametrized and the estimation typically relies on Bayesian shrinkage (). 

\begin{equation}\label{eqn:var}
    \mathbf{y}_t = \mathbf{b} + \sum_{p=1}^P \mathbf{B}_p \mathbf{y}_{t-p} + \boldsymbol{\upsilon}_t
\end{equation}

where $\mathbf{b}$ is an $N \times 1$ vector of intercepts, $\mathbf{B}_1, \dots, \mathbf{B}_P$ are the $N \times N$ coefficient matrices and $\boldsymbol{\upsilon}_t \sim \mathcal{N}(0, \Sigma)$.

A VAR is easily cast into state space form, as the observables are the states and the measurement equation is simply an identity matrix without measurement error. In terms of the stacked model considered in Section \ref{sec:precsampler} this implies that $M = I_{TN}$ and $V_{\varepsilon} = 0$. Moreover, since there are no states it suffices to condition on $y_{T-P+1}, \dots, y_{T}$ when calculating the predictive density. That is to say, given the parameters the model can rewritten in stacked form as


\begin{equation*}
    \begin{bmatrix}
        I_N \\
        \shortminus\mathbf{B}_1 & I_N \\
        & \ddots & \ddots & \\
        \shortminus\mathbf{B}_{P} & \dots & \shortminus\mathbf{B}_1 & I_N & & \\
        & \ddots &  & \ddots & \ddots & \\
        & & \shortminus\mathbf{B}_{P} & \dots & \shortminus\mathbf{B}_1 & I_N       
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{y}_{T-P+1} \\
        \vdots \\
        \mathbf{y}_{T} \\
        \mathbf{y}_{T+1} \\
        \vdots \\
        \mathbf{y}_{T+H} 
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mathbf{b} \\
        \vdots \\
        \mathbf{b} \\
        \mathbf{b} \\
        \vdots \\
        \mathbf{b} 
    \end{bmatrix}
    +
    \begin{bmatrix}
        \boldsymbol{\upsilon}_{T-P+1} \\
        \vdots \\
        \boldsymbol{\upsilon}_{T} \\
        \boldsymbol{\upsilon}_{T+1} \\
        \vdots \\
        \boldsymbol{\upsilon}_{T+H} 
    \end{bmatrix}
\end{equation*}

or more compactly:
\begin{equation*}
    \mathbf{H} \mathbf{y} = \mathbf{c} + \boldsymbol{\upsilon}.
\end{equation*}

Sticking with the notation, define $\mathbf{z}={y} = \transpose{[\transpose{\mathbf{y}_1}, \dots, \transpose{\mathbf{y}_{T+H}}]}$. Then the joint distribution of available observations and forecasts is Normal:

$$
\mathbf{z} \sim \mathcal{N}(\mathbf{H}^{-1} \mathbf{c}, (\transpose{\mathbf{H}} \mathbf{V}^{-1}_{\!\upsilon}\mathbf{H})^{-1}).
$$

Draws from the predictive density can be obtained by applying Algorithms 1 and 2, treating the vector of states, $\boldsymbol{\alpha}$, as empty: in the case of unconditional forecasts, the partition of the permuted vector is thus $\mathbf{z}_{\mathcal{P}} = \transpose{[\mathbf{y}^{\sf{f}}, \mathbf{y}^{\sf{o}}]}$; for conditional forecasts $\mathbf{z}_{\mathcal{P}'} = \transpose{[\mathbf{y}^{\sf{f}}, \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}]}$.\\

Of course, unconditional forecasts given the parameters can simply be obtained by drawing $\boldsymbol{\upsilon}_{T+1}, \dots, \boldsymbol{\upsilon}_{T+1}$ and using Equation \ref{eqn:var} to recursively construct $\mathbf{y}_{T+1}, \dots, \mathbf{y}_{T+H}$. Nevertheless, it is instructive to see how such draws can also be obtained by permuting the joint distribution $\mathbf{z}$, as this forms the basis for generating conditional forecasts. Consider a VAR(1) and $H=1$. Clearly, in this case, $\mathbf{y}_{T+1}|\mathbf{y}_T, \Theta \sim \mathcal{N}(\mathbf{b} + \mathbf{B}_1 \mathbf{y}_T, \boldsymbol{\Sigma})$. To derive the result from the joint distribution $\mathbf{z} = [\mathbf{y}^o, \mathbf{y}^{\sf{f}}] = [\mathbf{y}_T, \mathbf{y}_{T+1}]$, start with the mean $\boldsymbol{\mu}_z$:

\begin{align*}
\boldsymbol{\mu}_z = \mathbf{H}^{-1} \mathbf{c} &= 
\begin{bmatrix}
    I_N \\
    \shortminus\mathbf{B}_1 & I_N 
\end{bmatrix}
^{-1}
\begin{bmatrix}
    \mathbf{b} \\
    \mathbf{b} 
\end{bmatrix}
\\
&= 
\begin{bmatrix}
    I_N \\
    \mathbf{B}_1 & I_N 
\end{bmatrix}
\begin{bmatrix}
    \mathbf{b} \\
    \mathbf{b} 
\end{bmatrix}
\\
&= 
\begin{bmatrix}
    \mathbf{b} \\
    \mathbf{B}_1\mathbf{b} + \mathbf{b} 
\end{bmatrix}
\end{align*}

Similarly, the precision matrix $Q_z$ is given by

\begin{align*}
    \mathbf{Q}_z = \transpose{\mathbf{H}} \mathbf{V}_{\!\upsilon}^{-1} \mathbf{H} &= 
    \transpose{
    \begin{bmatrix}
        I_N \\
        \shortminus\mathbf{B}_1 & I_N 
    \end{bmatrix}
    }
    \begin{bmatrix}
        \Sigma^{\shortminus 1} & \\
        &\Sigma^{\shortminus 1} 
    \end{bmatrix}
    \begin{bmatrix}
        I_N &  \shortminus\mathbf{B}_1 \\
        & I_N 
    \end{bmatrix}
    \\
    &= 
    \begin{bmatrix}
        \Sigma^{\shortminus 1} + \mathbf{B}^{\sf{T}}_1\Sigma^{\shortminus 1}\mathbf{B}_1 & \shortminus\mathbf{B}^{\sf{T}}_1\Sigma^{\shortminus 1}  \\
        \shortminus\Sigma^{\shortminus 1}\mathbf{B}_1 & \Sigma^{\shortminus 1} 
    \end{bmatrix}
\end{align*}

Permute $\mathbf{z}$ such that $\mathbf{y}_T$ is ordered last, i.e. $\mathbf{z}_{\mathcal{P}} = \mathcal{P} \mathbf{z}  = \transpose{[\transpose{\mathbf{y}^{\sf{f}}}, \transpose{\mathbf{y}^o}]} = \transpose{[\mathbf{y}^{\sf{T}}_{T+1}, \mathbf{y}^{\sf{T}}_T]}$. The corresponding moments with partitions $y^{\sf{f}}$ and $y^{\sf{o}}$ are then given by

\begin{align*}
\boldsymbol{\mu}_{z_{\mathcal{P}}} &= 
\begin{bmatrix}
    \mathbf{b} + \mathbf{B}_1\mathbf{b} \\
    \mathbf{b} 
\end{bmatrix}
\coloneqq
\begin{bmatrix}
    \boldsymbol{\mu}_{y^{\sf{f}}} \\
    \boldsymbol{\mu}_{y^{\sf{o}}}
\end{bmatrix}
,\\
\mathbf{Q}_{z_{\mathcal{P}}} &= 
\begin{bmatrix}
    \Sigma^{\shortminus 1}   & \shortminus\boldsymbol{\Sigma}^{\shortminus 1}\mathbf{B}_1 \\
    \shortminus\mathbf{B}^{\sf{T}}_1\boldsymbol{\Sigma}^{\shortminus 1} & \boldsymbol{\Sigma}^{\shortminus 1} + \mathbf{B}^{\sf{T}}_1\boldsymbol{\Sigma}^{\shortminus 1}\mathbf{B}_1
\end{bmatrix}
\coloneqq
\begin{bmatrix}
    \mathbf{Q}_{y^f} & \mathbf{Q}_{y^f\!y^o} \\
    \mathbf{Q}^{\sf{T}}_{y^f\!y^o} & \mathbf{Q}_{y^o}
\end{bmatrix}
.
\end{align*}

The precision matrix of the conditional distribution $\mathbf{z}_{y^f}|\mathbf{z}_{y^o}$ is simply given by

\begin{equation*}
    \mathbf{Q}_{y^f|y^o} = 
    \mathbf{Q}_{y^f} = 
    \boldsymbol{\Sigma}^{-1}
\end{equation*}

and the corresponding conditional mean is:

\begin{align*}
    \boldsymbol{\mu}_{y^f|y^o} &= 
    \boldsymbol{\mu}_{y^f} - \mathbf{Q}_{y^f}^{-1} \mathbf{Q}_{y^{\sf{f}}\!y^o} (\mathbf{y}_o - \boldsymbol{\mu}_{y^o})\\
    &= \mathbf{B}_1\mathbf{b} + \mathbf{b} - \boldsymbol{\Sigma} (\shortminus\boldsymbol{\Sigma}^{-1} \mathbf{B}_1) (\mathbf{y}_T - \mathbf{b}) \\
    &= \mathbf{B}_1\mathbf{b} + \mathbf{b} + \mathbf{B}_1 (\mathbf{y}_T - \mathbf{b}) \\
    &= \mathbf{b} + \mathbf{B}_1 \mathbf{y}_T.
\end{align*}


\subsection{Simulations}

\subsubsection{Motivation and related literature}

In simulations I compare the computational efficiency of the precision-sampling algorithms outlined above to  simulation smoothers such as \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} that rely on the Kalman-filter and are commonly used in the literature. Previous work in this area has analyzed the computational advantages of such simulation smoothers compared to the algorithms for conditional forecasting given in \citet{waggonerzha1999_res} in the context of a vector autoregression \citep{bgl_2015ijf}; \citet{mmp2011_csda} analyzes the performance of precision-based samplers in the context of a time-varying parameter regression and dynamic factor models and compares the performance to the \citet{durbinkoopman2002_biomtr} simulation smoother.\\

I focus on both small ($N=20,\,R=2$) and large factor models ($N=100\,,R=10$) as well as a medium- and large-sized vector autoregression with $N=20$ and $N=100$, respectively; the lag length equals $P=4$ in both vector autoregressions. I evaluate the runtime of obtaining $100$ draws from the predictive density $p(y^f|y^o, y^c)$ given a sample size $T=100$ and forecast horizons $H = \{5, 20, 50\}$ conditional on a share $\kappa = 0.1$ of the observables (\textit{hard conditioning}).\footnote{The size of the conditioning set does not have a large impact on the relative runtime and in the interest of space I do not report results for $\kappa = \{0.5, 0.75\}$. They are available upon request.} Furthermore, I also consider the costs of producing repeated samples from the predictive density given the parameters as would be the case when on top of the conditioning set $$\mathbf{y}^{c} = [y_{1, T+1}, \dots, y_{\kappa \cdot N, T+1}, \dots, y_{1,T+H}, \dots, y_{\kappa \cdot N, T+H}]$$ restrictions are imposed on the remainining observables to lie within a prespecified range (\textit{soft conditioning}). 

\subsubsection{Data-generating process}

In the case of the dynamic factor model outlined in Section \ref{sec:dfm} the factor dynamics are given by a VAR(1) with parameters $\phi_1 = 0.7 \cdot I_R$ and $\Sigma = I_R$. The elements of the loadings on the contemporaneous factors $\lambda_0$ are drawn from independent Normal distributions with mean $0.5$ and variance $0.1$; all remaining loadings are set to 0, i.e. $K=0$. Setting $J=0$, the idiosyncratic components are i.i.d and $\omega_i$ is chosen such that the common component - $\boldsymbol{\lambda}_{0,i\cdot} \mathbf{f}_t$ - explains \sfrac{2}{3} of the total variation in the i-th variable. \\

To generate data from a vector autoregression as discussed in Section \ref{sec:var}, I closely follow the set-up outlined in \citet{CHP2020_ijf}. Let $b_{p, ij}$ denote the element of $B_p$ in row i and column j. The diagonal elements are set to

\begin{subequations}
    \begin{align}
        b_{1, ii} = \mathcal{N}(0, \sigma^2_o), \: \forall i = 1, \dots, N \\
b_{p, ii} = \frac{b_{1,ii}}{p}, \: \forall p = 2, \dots, P
    \end{align}    
\end{subequations}

\noindent while each off-diagonal element is non-zero with probability $p_c$ and drawn from $\mathcal{N}(0, \sigma^2_c)$.The lag length $P$ equals 4 and for the medium-sized VAR with $N=20$ variables $\sigma^2_o = 0.2 $, $\sigma^2_c = 0.05$ and $p_c = 0.2$. To ensure a stationary VAR when the cross-section is large, i.e. $N=100$, the values are adjusted to $\sigma^2_o = 0.05 $, $\sigma^2_c = 0.001$ and $p_c = 0.1$.

Details on the implementation of the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers are provided in Appendix \ref{app:kalmansimsmoothers}.\\

\subsubsection{Results}

The results of the simulations can be seen in Figure \ref{figsimulation}. 

\begin{figure}[htbp] \centering
    \caption{Computational efficiency analysis of different simulation smoothers \label{figsimulation}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_simulations_Ncond_10.pdf}} \\
    {
    \footnotesize \textbf{Note:} The figure shows the time it takes to generate 1000 draws from the predictive density $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})$ using the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers relative to the precision sampler outlined above for different forecast horizons $H$. The conditioning set $\mathbf{y}^{\sf{c}}$ consists of the first $0.1\cdot N$ variables. For the small factor model, the number of variables is $N=20$ and the number of static factors is $R=2$. For the large factor model $N=100,\, R = 10$. For the medium- and large-sized vector autoregressions the number of lags is set to $P=4$ as well as $N=20$ and $N=100$, respectively.The runtime measures the entire procedure of generating a draw from the predictive density, including building the system matrices.For the precision-sampler this includes the costs of setting up the permutation matrices which is incurred only once.In the case of soft conditioning, the runtime of producing 1000 draws given the same parameters is reported. For details, see the main text.
    }
    \end{tabular}
    \newline
    \normalsize
    \end{figure}

\section{Empirical application}

\subsection{Data}

\subsection{Estimation}

\subsection{Results}

\section{Conclusion}

\bibliographystyle{aernobold}
\bibliography{papers}

\appendix

\section{Appendix: Kalman-filter based simulation smoothers}\label{app:kalmansimsmoothers}

This appendix provides details on the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers that are used in the simulations in Section \ref{sec:models}



In this section we highlight how we implement the Kalman-filter based simulation smoothers and show how to obtain draws from the predictive densities. Although both the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers rely on the Kalman filter and smoother to produce a draw from the conditional distribution of the states, there are important conceptual differences between the two. Let $a_{t|s} = E[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ and $P_{t|s} = Var[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ denote the conditional mean and variance of the state at time $t$ conditional on information up to time $s$ which can be obtainted from the Kalman filter or smoother. In addition, let $a_t$ denote a draw from the conditional distribution $p(\alpha_t | \mathbf{y}_1, \dots, \mathbf{y}_{T+H})$. 

\subsubsection{\citet{carterkohn1994_biomtr} simulation smoother}

Given the parameters of the state space model $\Theta$ and  $a_{t|t}, P_{t|t} \: \forall t = 1, \dots, T+H$, the algorithm in \citet{carterkohn1994_biomtr} generates $a_{T+H}$ from 

$$
p(\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})
$$

\noindent with

\begin{align*}
    E[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = a_{T+H|T+H} \\
    Var[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = P_{T+H|T+H}.
\end{align*}

 For $t=T+H-1, \dots, 1$ $a_t$ is generated from $p(\alpha_{t}|\mathbf{y}_t, a_{t+1})$. For details on how to derive the moments of the conditional distributions given the output of the Kalman filter, see the original paper as well as the textbook treatment in \citet{KimNelson1999mit}.

 \subsubsection{\citet{durbinkoopman2002_biomtr} simulation smoother}

The \citet{durbinkoopman2002_biomtr} simulation smoother produces a draw from the conditional distribution of the states by first simulating the state space model in Equation \ref{eqn:statespacesys}, producing a draw from the \textit{joint} distribution of states and observables

$$p(\alpha_{1}, \dots, \alpha_{T+H}, \mathbf{y}_1, \dots, \mathbf{y}_{T+H}).$$

Denote this joint draw by $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Running the Kalman smoother recursions yields $a_{t|T+H}, \: a^+_{t|T+H}$ where $a^+_{t|T+H} = E[\alpha^+_t|y^+_1, \dots, y^+_{T+H}]$.  By setting $a_t = a_{t|T+H} - a^+_{t|T+H} + a^+$ adjusts the mean of $a^+_t$ to yield a draw from the desired conditional distribution. In practice, it is more efficient to first construct $y^* = y-y^+$ and run the Kalman smoother only once, yielding $a^*_{t|T+H} = E[a^*_t|y^*]$. A draw from $p(\alpha_{1}, \dots, \alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})$ is then obtainted by setting $a_t = a^*_{t|T+H} + a^+_t$. Note that since only an estimate of the smoothed mean of the states is required, more efficient smoothing recursions can be employed which do not require the storage of $a_{t|t}$ and $P_{t|t}$ \citep[see][ch. 4.4.2]{durbinkoopman2002_biomtr}.

\subsubsection{Drawing from the (conditional) predictive density $p(y^f|I, \Theta)$}
Under the common assumption of uncorrelated measurement errors, draws from the predictive density $p(y^{\sf{f}}_t\,|\,a_t, y^o, y^c, \Theta)$ can  be obtained by sampling the measurement errors independently from $\mathcal{N}(0, \omega_i) \: \forall i s.t. y_{i,t} \notin y_{c,t}$ and adding them to the draw of the state vector \citep[see][for the general case of a full covariance matrix $\Omega$]{bgl_2015ijf}. For the \citet{durbinkoopman2002_biomtr} simulation smoother the generated artifical observations $y^+$ can be re-used, since $F a^*_{t|T+H} + y^+_t$ is a draw of the observations conditional on the sampled states. In the case of a VAR, there is no 
measurement error so that $y^{\sf{f}}_t$ can be set equal to the corresponding elements of $a_t$. 

\subsubsection{Multiple draws from $p(y^f|I, \Theta)$}
Additional draws from the (conditional) predictive density given the same set of parameters $\Theta$ can be produced recursively from the \citet{carterkohn1994_biomtr} simulation smoother by independently sampling $a_t$ as many times as required and conditioning on the draw to generate $y^{\sf{f}}_t$. In the case of the \citet{durbinkoopman2002_biomtr} simulation smoother, each additional draw requires new simulated values of the states and observations, $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Given these, the entire filtering and smoothing recursions need to be re-run. However, computational savings still arise as the state covariances $P_{t|t}$ and functions thereof that are calculated during the recursions do not depend on the observations. As such they only need be calculated once and can be passed on to subsequent runs of the Kalman filter and smoother \citep[p. 606]{durbinkoopman2002_biomtr}.

\subsubsection{Initialisation}
In the case of a dynamic factor model, the Kalman filter is initialized by setting $a_{1|0}$ and $P_{1|0}$ equal to the unconditional mean and variance, respectively, of the factor VAR. Vector autoregression applications only require the simulation smoother to run over the sample with missing observations, i.e. $t\geq T+1$. The recursions are therefore initialized with $a_{T+1|T} = \mathbf{T} \, \transpose{[\transpose{\mathbf{y}}_T, \dots, \transpose{\mathbf{y}}_{T-P+1}]}$ and $P_{T+1|T} = \Sigma$, unless the more efficient one-step algorithm in \citet{durbinkoopman2002_biomtr} is employed, in which case $a_{T+1|T} = 0$ is required \citep[see][]{jarocinski2015csda}.



\end{document}
