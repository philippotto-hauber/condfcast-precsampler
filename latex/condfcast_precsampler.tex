\documentclass[notitlepage,a4paper,12pt]{article}
\usepackage[authoryear]{natbib}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.25cm,bottom=2.25cm,nohead}
\usepackage{mathtools}
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--} % dash instead of "to" in range
%\usepackage{amsfonts} 
%\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
%\usepackage[bitstream-charter]{mathdesign}
\usepackage[charter, cal = cmcal]{mathdesign}
\usepackage[T1]{fontenc}
\mathchardef\shortminus="2D
%\newcommand{\shortminus}{-}
% % font used in Korobilis and Schumacher (2014)
%\usepackage[sc]{mathpazo}
%\linespread{1.05}         % Palladio needs more leading (space between lines)
%\usepackage[T1]{fontenc}
% % alternative I found on the web
%\usepackage{tgpagella}
%\usepackage[T1]{fontenc}
\usepackage{xfrac}
\usepackage{listings}
\usepackage{longtable}
\usepackage[table]{xcolor}  
\usepackage{lscape}
\usepackage{threeparttablex}
\usepackage{booktabs}
%\usepackage{hyperref}
\usepackage{url}

\newcommand{\transpose}[1]{{#1}^{\sf{T}}}
\newcommand\NumSeries{57}

\linespread{1.2}
\setlength\parindent{15pt}

\begin{document}
\title{Conditional forecasts in large factor models: how useful is external information from professional forecasters?}

\author{
  Philipp Hauber\\
  Kiel Insitute for the World Economy
}
\date{\today}

\maketitle

\begin{abstract}\noindent
    This paper evaluates forecasts from a factor model estimated with a large real-time dataset of the German economy. The evaluation focuses on a broad cross-section of variables such as activity series including components of the gross domestic product and gross value added, deflators and other price measures as well as several labor market indicators. In addition to unconditional forecasts for these variables, we also investigate to what extent the forecast accuracy improves when we condition on professional forecasters' view on GDP growth and CPI inflation. We find that over the period from 2006 to 2017 the model's unconditional  forecasts are broadly in line with autoregressive benchmarks for the majority of the 37 series that we focus on in the evaluation, in some cases performing somewhat better and in others somewhat worse. For a few variables capturing real activity and some price indicators, however, we find large gains in predictive accuracy that persist for forecast horizons of up to two quarters ahead. Conditioning on external information tends to improve the forecast accuracy in some instances but typically only for those series where the unconditional forecasts are already quite accurate. For around a third of the variables under consideration, the differences in forecast accuracy between conditional and unconditional forecasts are statistically significant for density forecasts; for point forecasts on the other hand we find no significant differences. From a methodological point of view, this paper proposes precision-based sampling algorithms to draw from the predictive density - unconditional or conditional on a subset of the system variables - in factor models and other models with unobserved components. Simulations show that these algorithms perform favorably compared to Kalman filter-based alternatives typically used in the literature.  
    
    \bigskip\noindent\textbf{Keywords:} factor models, conditional forecasting, precision-based sampling
    
    \bigskip\noindent \textbf{JEL classification:} C11, C53, C55, E37.
    \vfill
    \end{abstract}
    
    \pagebreak \pagestyle{plain} \pagenumbering{arabic}



\section{Introduction}

Factor models feature prominently in macroeconomic now- and forecasting applications. Typically, however, forecast evaluations in this literature focus only on a small subset of the large number of time series that are used to estimate these models. For example, \citet{stockwatson_2002} evaluate the forecast performance of industrial production, personal income, manufacturing and trade sales and (nonagricultural) employment in the United States. The factor-model based nowcasting literature has largely focused on predicting the real gross domestic product (GDP) \citep[see][]{grs_2008, bok_etal_2018, breitungschumacher_2008, kms_jae_2013}. While arguably the single most important measure of economic activity, analysts or policy-makers are typically also interested in forecasts for many other variables, e.g. concerning the labor market, price measures including deflators, components of GDP or activity measures like industrial production  or turnover. Moreover, forecasts of these variables given a path of future real GDP growth and CPI inflation may be used in scenario analysis or as consistency checks to other model-based forecasts. Such conditional forecasts have received little attention in the factor model literature.

This paper therefore extends the literature on factor models by broadening the cross-sectional dimension of the forecast evaluation to include series that are not commonly featured. For example, we consider a total of 37 time series comprising labor market indicators, price measures such as the CPI or PPI as well components of gross domestic product such as consumption, investment and exports as well gross value added of different sectors like construction or manufacturing. In addition to an evaluation of unconditional forecasts for such variables, we also investigate how the predictive accuracy changes external information in the form of professional forecasters' views on activity and inflation is incorporated. Such an evaluation captures both how well the model describes the joint dynamics of the data as well as the accuracy of the conditioning information and thus gives a complete assessment of the forecast accuracy for the variables under consideration in this study. 

Specifically, in the empirical application we rely on external information from the Reuters Poll of professional forecasters which collects the forecasts from private sector institutions and research institutes on a quarterly basis. We supplement these historical forecasts with a large real-time dataset consisting of roughly 80 time series that allows us to exactly replicate the information available at the time the forecast survey was conducted. We then estimate factor models with Bayesian methods to obtain forecasts both unconditional as well as conditional on the GDP growth and CPI inflation forecasts from the Reuters Poll. This allows us to assess how predictive accuracy improves both in terms of point and density forecast accuracy when external information is included.

As such, we also contribute to the literature on conditional forecasting which has mostly focussed on Bayesian vectorautoregressions - another class of popular macroeconmetric methods. \citet{bgl_2015ijf} consider conditional forecasts by conditioning on future values they shed light on the stability of dynamic relationships in the Euro area. Similarly, in their empirical application \citet{clarkmccracken_2017_jae} condition on realizations when testing for bias, efficiency and MSE accuracy of conditional versus unconditional forecasts. 

Closer to the questions guiding our research are \citet{kruegerclarkravazzolo2017_jbes}, \citet{tallmanzaman_2020} and \citet{ganicsodendahl_2021_ijf} who combine model-based density forecasts with external information in the form of forecast surveys. Using entropic tilting, these authors tend to find gains in predictive accuracy even for those variables that are not directly tilted to match the survey forecasts. However, common to both papers is the focus on only a few time series within a Bayesian vector autoregression context.\footnote{The latter two also consider medium-sized VAR with 10 and 14 variables, respectively as a robustness check and find similar results to their smaller baseline models.} Our analysis differs in that we work with factor models rather than vector autoregressions, condition on quarterly point forecasts for GDP growth and CPI inflation and consider the impact on a much broader and more disaggregated range of variables.

Besides the empirical contribution, this paper also proposes precision-based sampling algorithms for conditional forecasting and scenario analysis. Precision-based sampling algorithms \citep{chanjeliazkov_2009,mccausland2012_jecmtr} that build on the seminal work by \citet{rue2001_jrss} on Gaussian Markov random fields, are increasingly used in macroeconomic applications. \citet{HauberSchumacher2021} generalize these precision-samplers to applications with missing data. Since future values of variables can simply be considered as missing, we show how draws from the predictive density - unconditional or conditional on future values of a subset of the system's variables - can be obtained. In a simulation study we compare the performance of these algorithms to the Kalman-filter based implementations proposed by \citet{bgl_2015ijf}.

The results of the forecast evaluation show that for the majority of time series under consideration the factor model produces forecasts that are as accurate as an autoregressive benchmarks, with relative root mean squared forecast errors (RMSFE) and continous ranked probability scores (CRPS) ranging from 0.9 to 1.1. For a few variables we find much larger gains compared to the benchmark and these persist as the forecast horizon increases. When we incorporate external information in the form of GDP growth and CPI inflation forecasts from a survey of professional forecasters, we generally find some improvements in forecast accuracy for the variables of interest from categories such as real activity, prices and the labor market. Only for a smaller number of series, however, do we find that conditioning produces large gains and these are typically the series for which the model already produces accurate unconditional forecasts. While the point and density forecast accuracy is broadly comparable in the sense that relative gains compared to an autoregressive benchmark for a given variable are similar, we find that when evaluating the entire predictive distribution the loss from the condtional forecasts is significantly lower than that of the model's unconditional predictions. For point forecasts, there is virtually no such evidence.

The remainder of this paper is structured as follows: \Cref{sec:precsampler} illustrates the precision-based sampling algorithms for conditional forecasting, while in \Cref{sec:evaluation} we discuss the datatset including the forecast survey used to condition the factor model forecasts as well as the evaluation set-up. \Cref{sec:results} discusses the results and several robustness checks while \Cref{sec:conclusion} concludes.

\section{Precision-based sampling algorithms for conditional forecasting}\label{sec:precsampler}

In this section, we lay out the precision-based sampling algorithms to obtain conditional forecasts. To fix notation and ideas, we first review the general precision-based sampler in \Cref{subsec:precsampler} before turning to forecasting applications (\Cref{subsec:forecasting}). Lastly, we also discuss how repeated draws from the predictive density given parameters can be obtained (\Cref{subsec:softcond}). 

\subsection{Precision-based sampling}\label{subsec:precsampler}

We outline the precision-based algorithms through the lens of a dynamic factor model but the analysis can be generalized to other state space models with unobserved components. Factor models feature prominently in the macroeconomic literature. For an overview of the methodological developments of factor models and their applications, see \citet{stockwatson2016_hbmacro}. Precision-based sampling applications include \citet{chanjeliazkov_2009,mccausland_factor2015,kaufmannschumacher_jae2017,kaufmannschumacher_jectrcs2019}.

A \textit{dynamic factor model} is defined as

\begin{subequations}
    \label{eqn:factormodel}
    \begin{align}
        \mathbf{y}_t &= \sum_{k=0}^K \lambda_k \, \mathbf{f}_{t-k} + \mathbf{e}_t \label{eqn:facmod_obs}\\ 
        \mathbf{f}_t &= \sum_{p=1}^P \phi_p \, \mathbf{f}_{t-p} + \boldsymbol{\upsilon}_t \label{eqn:facmod_factors}\\
        \mathbf{e}_t &= \sum_{j=0}^J \psi_j \, \mathbf{e}_{t-j} + \boldsymbol{\epsilon}_t \label{eqn:facmod_idios}
    \end{align}
\end{subequations}

where $\mathbf{f}_t$ denotes an $R \times 1$ vector of unobserved factors which summarizes the co-movement of the observables $\mathbf{y}_t$. The dynamics of the factors are modelled as a vector autoregression of order $P$. The idiosyncratic components, $\mathbf{e}_t$ are modeled as independent autoregressive processes of order $J$, i.e. $\psi_1, \dots, \psi_J$ are diagonal matrices. The $N \times R$ loadings matrices $\mathbf{\lambda}_k$ capture the dynamic relationships between observables and factors. Setting $K=0$, so that the variables only load contemporaneously on the factors, yields a \textit{static factor model}. The innovations $\epsilon_t$, $\upsilon_t$ are Normal, uncorrelated at all leads and lags and their covariance matrices given by $\Omega$ and $\Sigma$, respectively. 

Stacking the observables, factors and idiosyncratic components over $t$ yields 

\begin{equation}
    \mathbf{y} = \mathbf{M} \, \mathbf{f} + \mathbf{e}
\end{equation} 

\noindent where 

$$
\mathbf{M}
=
\begin{bmatrix}
    \boldsymbol{\lambda_0} &   \\
    \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}   \\
    \boldsymbol{\lambda_2} & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0} \\
     & \ddots & \ddots & \ddots \\
     & \boldsymbol{\lambda_K} &\dots & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}
     &  \\
     & & \ddots & \ddots & \ddots & \ddots \\
    & & & \boldsymbol{\lambda_K} &\dots & \boldsymbol{\lambda_1} & \boldsymbol{\lambda_0}
\end{bmatrix}.
$$

The vector of idiosyncratic components can be written as $\mathbf{H}_e\, \mathbf{e} = \boldsymbol{\varepsilon}$ with 

$$
\mathbf{H}_e
=
\begin{bmatrix}
    I_N &  \\
    \shortminus \psi_1 & I_N &  \\
    \shortminus \psi_2 & \shortminus \psi_1 & I_N &  \\
     &  &  &  &  & \\
    \shortminus \psi_J & \dots & \dots & \shortminus \psi_1 & I_N \\
     &  &  &  &  & \\
    & & \shortminus \psi_J & \dots & \dots & \shortminus \psi_1 & I_N
\end{bmatrix}
$$

\noindent and $\boldsymbol{\varepsilon} \sim \mathcal{N}(0_{NT}, I_T \otimes \boldsymbol{\Omega})$. It follows that $\mathbf{e}$ is Normal, with mean $0$ and covariance matrix $\bm{V}_{e} = (\transpose{\mathbf{H}}_{e} (I_T \otimes \boldsymbol{\Omega}^{-1}) \mathbf{H}_e)^{-1}$.

In a similar fashion, the vector of factors is distributed as 

$$
\mathbf{f} \sim \mathcal{N}\left(0, (\transpose{\mathbf{H}}_{\sf{f}} (I_T \otimes \boldsymbol{\Sigma^{-1}}) \mathbf{H}_{\sf{f}})^{-1}\right)
$$

with 

$$
\mathbf{H}_{\sf{f}}
=
\begin{bmatrix}
    I_N &  \\
    \shortminus \phi_1 & I_N &  \\
    \shortminus \phi_2 & \shortminus \phi_1 & I_N &  \\
     &  &  &  &  & \\
    \shortminus \phi_P & \dots & \dots & \shortminus \phi_1 & I_N \\
     &  &  &  &  & \\
    & & \shortminus \phi_P & \dots & \dots & \shortminus \phi_1 & I_N
\end{bmatrix}.
$$

Define 

\begin{equation}
    \mathbf{z} 
    \coloneqq 
    \begin{bmatrix}
        \mathbf{f} \\
        \mathbf{y}
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        I_{TR} & 0 \\
        \mathbf{M} & I_{TN} 
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{f}\\
        \mathbf{e}\\
    \end{bmatrix}.
\end{equation}

It follows that $\mathbf{z}$ is Normal with mean 0 and covariance matrix $\mathbf{V}_{\!z}$. The corresponding precision matrix is the inverse of the covariance matrix: 

\begin{align*}
    \mathbf{Q}_z 
    &= 
    \mathbf{V}^{-1}_{\!z} \\
    &= 
    \text{\sf{Var}}\left(
    \begin{bmatrix}
        \mathbf{f} \\
        \mathbf{y}
    \end{bmatrix}
    \right)^{-1} \\
    &=  
    \left(
    \begin{bmatrix}
        I & 0 \\
        \mathbf{M} & I
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{H}_{\sf{f}}^{-1} \mathbf{V}_{\!\upsilon} \transpose{\mathbf{H}_{\sf{f}}^{-1}} & 0 \\
        0 & \mathbf{V}_{\!e}
    \end{bmatrix} 
    \begin{bmatrix}
        I & \transpose{\mathbf{M}} \\
        0 & I
    \end{bmatrix}
    \right)^{-1}\\
    &=
    \begin{bmatrix}
        I & \shortminus\transpose{\mathbf{M}} \\
        0 & I
    \end{bmatrix}
    \begin{bmatrix}
        \transpose{\mathbf{H}_{\sf{f}}} \mathbf{V}_{\!\upsilon} \mathbf{H}_{\sf{f}} & 0 \\
        0 & \mathbf{V}_{\!e}
    \end{bmatrix} 
    \begin{bmatrix}
        I & 0 \\
        \shortminus\mathbf{M} & I 
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
         \transpose{\mathbf{H}_{\sf{f}}} \mathbf{V}_{\!\upsilon} \mathbf{H}_{\sf{f}} + \transpose{\mathbf{M}}\bm{V}_{\varepsilon}\mathbf{M}  & \shortminus \transpose{\mathbf{M}} \mathbf{V}_{\!e} \\
         \shortminus \mathbf{V}_{\!e} \mathbf{M} & \mathbf{V}_{\!e}
    \end{bmatrix} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\sf{f}} & \mathbf{Q}_{\sf{f} y} \\
        \transpose{\mathbf{Q}}_{\sf{f} y} & \mathbf{Q}_{y}  
    \end{bmatrix}
\end{align*}

Bayesian estimation of \Crefrange{eqn:facmod_obs}{eqn:facmod_idios} requires a draw from the distribution of the factors conditional on the observations and parameters $\Theta$, $p(\mathbf{f}_1, \dots, \mathbf{f}_T | \mathbf{y}_1, \dots, \mathbf{y}_T, \Theta)$. "Simulation smoothing" methods based on the Kalman filter/smoother have been proposed by Carter and Kohn (1994) and Durbin and Koopman (2001) and are widely used in the literature. Precision-based sampling algorithms \citep{chanjeliazkov_2009} provide an alternative to these methods.

Using standard results for multivariate Normal distributions \citep[e.g.][pp. 86-87]{bishop_prml_2006}, we can derive the desired conditional distribution:

\begin{align*}
    p
    \left(
    \mathbf{f} \mid \mathbf{y}, \Theta
     \right) 
    &\sim
    \mathcal{N}
    \left(
    \mathbf{Q}^{-1}_{\alpha} \left( \mathbf{Q}_{\alpha}\boldsymbol{\mu}_{\alpha} - \mathbf{Q}_{\alpha y} \left(\mathbf{y} - \boldsymbol{\mu}_{\text{y}}\right)\right), \mathbf{Q}_{\alpha}^{-1}
    \right) \\
    &\coloneqq \mathcal{N}
        \left(
            \boldsymbol{\mu}_{\alpha\mid y}, \mathbf{Q}_{\alpha\mid y}^{-1}
        \right) \\
\end{align*}

with \citep[see also][eqn. 6-8]{chanjeliazkov_2009}:
\begin{align*}
    \mathbf{Q}_{\sf{f}\mid y} &= \mathbf{Q}_{\sf{f}} \\
    &= 
    \transpose{\mathbf{H}} \mathbf{V}^{\,-1}_{\!\upsilon} \mathbf{H} + {\transpose{\mathbf{M}} \mathbf{V}^{\,-1}_{\!\epsilon}\mathbf{M}}\\
    \boldsymbol\mu_{\sf{f}\mid y} &= \mathbf{Q}_{\sf{f}}^{-1} \mathbf{Q}_{\sf{f}} \tilde{\mathbf{c}}+ \transpose{\mathbf{M}} \mathbf{V}_{\!\epsilon}^{\,-1} \mathbf{y}
\end{align*}

Draws from this distribution can be obtained efficiently since the $T\cdot S \times R$ matrix $\mathbf{Q}_{\sf{f}}$ is banded and its inversion not required. Specifically, calculate the conditional mean following \citet[][Algorithm 2.1]{rueheld_2005}. This requires the computation of the lower Cholesky factor $\mathbf{L}$ of $\mathbf{Q}_{\sf{f}}$. Then draw $\mathbf{v} \sim \mathcal{N}(0,I_{TR})$, solve $\transpose{\mathbf{L}} \mathbf{x} = \mathbf{v}$ for $\mathbf{x}$ and set $\boldsymbol{\mu}_{\sf{f}\mid y} +\mathbf{x}$, yielding a draw of $\mathbf{f}$ conditional on $\mathbf{y}$ \citep[][Algorithm 2.4]{rueheld_2005}. 

\subsection{Forecasting}\label{subsec:forecasting}

In addition to a draw of the factors conditional on the data, macroeconomic applications may require forecasts - both unconditional and conditional on future paths of a subset of the observables - from the model in \Crefrange{eqn:facmod_obs}{eqn:facmod_idios}. Examples include projections of inflation which take assumptions regarding the macroeconomic environment into account \citep{giannone_etal_2014_ijf} or external nowcasts as "jump-off" points for longer-term forecasts in reduced form or structural models \citep{faustwright2009_jbes,wolters2015_jae,delnegro_schorfheide_2013_hb}; \citet{knotekzaman2019_ijof} show how macroeconomic forecasts can be improved by conditioning on nowcasts for financial variables.

Let $\mathbf{y}^{\sf{o}} = \transpose{[\mathbf{y}^{\sf{T}}_{1}, \dots, \mathbf{y}^{\sf{T}}_{T}]}$ denote the available observations through $T$. For the forecast periods $h = 1:H$, $\mathbf{y}_{c,t}\: \forall t = T+1:T+H$ are the observations which are being conditioned on. The entire conditioning set is given by 
$$
\mathbf{y}^{\sf{c}} = \transpose{[\mathbf{y}^{\sf{T}}_{c,T+1}, \dots, \mathbf{y}^{\sf{T}}_{c,T+H}]}
$$ 
with $N^c = \text{dim}(\mathbf{y}^{\sf{c}})$. Conversely, $\mathbf{y}_{-c,t} = \mathbf{y}_t\notin\mathbf{y}_{c,t}$ denotes the observations for which no conditioning information is available at time $t$ and which are being forecast: $\mathbf{y}^{\sf{f}} = \transpose{[\mathbf{y}^{\sf{T}}_{-c,T+1}, \dots, \mathbf{y}^{\sf{T}}_{-c,T+H}]}$.

In what follows, we will not be concerned with the estimation of model parameters and assume that random draws from $p(\Theta|\mathbf{y}^{\sf{o}})$ are readily available. Also, for the sake of exposition, assume that between periods $t=1$ and $t=T$ all observations are available. Situations where this is not the case, e.g. because some series start at a later date in the sample or because outlying observations have been removed by setting them to missing, can naturally be accommodated.

From a Bayesian perspective, forecasting boils down to sampling from the predictive density given by \citep[see e.g.][]{gewekewhiteman_2006} 

\begin{equation}\label{eqn:preddens}    
    p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}},\mathbf{y^{\sf{c}}}, \Theta) p(\Theta|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})\, \text{d}\Theta.
\end{equation}

Note that this density takes into account that the conditioning set $\mathbf{y}^c$ contains information about which $\Theta$ are more likely a posteriori. Draws from this density can be obtained by applying the insights in Hauber and Schumacher (2021) who generalize the precision sampler outlined above to applications with missing data. Since future values of the variables can simply be considered as missing, draws from the joint distribution of states and forecasts conditional on the data (and possibly the future paths of a subset of the variables) can be sampled efficiently.

To evaluate Equation \ref{eqn:preddens}, derive the joint distribution of states and observations from $t=1:T+H$: 

\begin{align*}
    \mathbf{z} &= \transpose{[\transpose{\mathbf{f}}, \transpose{\mathbf{y}}]} \\
     &= \transpose{[\mathbf{f}_1, \dots, \mathbf{f}_{T+H}, \mathbf{y}_1, \dots, \mathbf{y}_{T+H}]} 
\end{align*}

It follows that the distribution of $\mathbf{z}$ is Normal with mean $\boldsymbol{\mu}_{z} = 0$ and precision matrix $\mathbf{Q}_{z}$ as in \Cref{subsec:precsampler}. To sample from the conditional distribution $p\left(\mathbf{y}^{\sf{f}}, \mathbf{f} \mid \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}, \Theta \right)$  requires a permutation of $\mathbf{z}$. To this end, define the permutation matrix $\mathcal{P}$ that reorders $\mathbf{z}$ such that the states and elements of $\mathbf{y^f}$ - ordered by time period $t$ - are placed first. That is to say,

$$
\mathbf{z}_{\mathcal{P}} \coloneqq \mathcal{P} \mathbf{z} =
 \transpose{[
     \mathbf{f}^{\sf{T}}_1, 
     \dots, 
     \mathbf{f}^{\sf{T}}_T, 
     \mathbf{f}^{\sf{T}}_{T+1}, 
     \mathbf{y}^{\sf{T}}_{\shortminus c,T+1}, 
     \dots, 
     \mathbf{f}^{\sf{T}}_{T+H}, 
     \mathbf{y}^{\sf{T}}_{\shortminus c,T+H},
     \mathbf{y}^{\sf{T}}_1,
     \dots,
     \mathbf{y}^{\sf{T}}_{T+H},
     \mathbf{y}^{\sf{T}}_{c, T+1},
     \dots, 
     \mathbf{y}^{\sf{T}}_{c, T+H}]}.
$$

Note that the conditioning arguments $\mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}$ are now ordered last. The permuted vector of states and observations is also Normal with (permuted) moments given by 

\begin{equation*}
    \mathbf{z_{\mathcal{P}}} \sim \mathcal{N}\left(\mathcal{P}\boldsymbol{\mu}_z, \left(\mathcal{P} \mathbf{Q}_z \transpose{\mathcal{P}}\right)^{-1}\right)
\end{equation*}

where we have made use of the fact that the inverse of a permutation matrix is equal to its transpose.  The precision matrix of $\mathbf{z}_{\mathcal{P}}$ can be partitioned as follows: 

\begin{align*}
    \mathbf{Q}_{z_{\mathcal{P}}} &\coloneqq  \mathcal{P} \mathbf{Q}_z \transpose{\mathcal{P}} \\
    &= 
    \begin{bmatrix}
        \mathbf{Q}_{\sf{f}\! y^{\sf{f}}} & \mathbf{Q}_{\sf{f}\! y^{\sf{f}}, y^{\sf{o}}} \\ 
        \mathbf{Q}^{\sf{T}}_{\sf{f}\! y^{\sf{f}}, y^{\sf{o}}} & \mathbf{Q}_{y^{\sf{o}}}
    \end{bmatrix}
\end{align*}

where $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}, y^{\sf{o}}}$ is a 
$T\!\cdot\!R + H\!\cdot\!N \times T\!\cdot\!N$ matrix and $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}}$ and $\mathbf{Q}_{y^{\sf{o}}}$ are square with dimensions $T\!\cdot\!R + H\!\cdot\!N$ and $T\!\cdot\!N $, respectively.


Similarly, the mean of $\mathbf{z}_{\mathcal{P}}$ can be partitioned as 
$$
\boldsymbol{\mu}_{z_{\mathcal{P}}} = 
\begin{bmatrix}
    \boldsymbol{\mu}_{\sf{f} y^{\sf{f}}} \\
    \boldsymbol{\mu}_{y^{\sf{o}}}
\end{bmatrix}
$$

Denote by $\mathbf{z}^*$ a draw from the conditional distribution (surpressing the dependence on the parmeters $\Theta$ for the sake of readability)

\begin{equation*}
    \mathbf{z}_{\sf{f} y^{\sf{f}}} | \mathbf{y}^{o}, \mathbf{y}^{c} \sim 
    \mathcal{N}\left(
        \mathbf{Q}^{-1}_{\sf{f} y^{\sf{f}}}\left( \mathbf{Q}_{\sf{f} y^{\sf{f}}}\boldsymbol{\mu}_{\sf{f} y^{\sf{f}}} - \mathbf{Q}_{\sf{f} y^{\sf{f}}, y^{\sf{o}}}\left(\mathbf{y}^o-\boldsymbol{\mu}_{y^{\sf{o}}}\right)\right)
    ,
    \mathbf{Q}_{\sf{f} y^{\sf{f}}}
    \right).
\end{equation*}

Then by reversing the permutation, i.e 

$$
\begin{bmatrix}
    \mathbf{f} \\
    \mathbf{y}_1 \\
    \vdots \\
    \mathbf{y}_{T+H} 
\end{bmatrix}
= 
\mathcal{P}^{-1} 
\begin{bmatrix}
    \mathbf{z}^*\\
    \mathbf{y^{o}} \\
    \mathbf{y^{c}}
\end{bmatrix}
$$

we can back out the draws of $\mathbf{f}$ and $\mathbf{y}^{\sf{f}} = [\mathbf{y}_{\shortminus c, T+1}, \dots, \mathbf{y}_{\shortminus c, T+H}]$.

Note that by having placed those components of $\mathbf{z}$ which are conditionally dependent close to each other, the bandwidth of $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}}$ is kept small. \Cref{figQDFM} shows the precision matrix of $\mathbf{z}_{\mathcal{P}}$ for a factor model with $T=100$, $N=100$ and $R=10$, a forecast horizon of $H=20$ and conditioning on the first 10 variables\footnote{For the sake of illustration we set $P=1$, $J=0$, $S=0$.}, i.e.

$$
\mathbf{y}^c = [y_{1,T+1}, \dots, y_{10,T+1}, \dots, y_{1,T+20}, \dots, y_{10,T+20}].
$$

The precision matrix $\mathbf{Q}_{\sf{f}\! y^{\sf{f}}}$ is highlighted by the square.

\begin{figure}[htbp] \centering
    \caption{Precision precision matrix in the case of a dynamic factor model \label{figQDFM}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_Qperm_DFM.pdf}} \\
    {
    \footnotesize \textbf{Note:} The figure shows the precision matrix of the permuted vector of observations and states $\mathbf{z}_{\mathcal{P}}$ for a factor model with $T=100$, $N=100$ and $R=10$. Highlighted is the $(T+H)\cdot R + N^c \times (T+H) \cdot R + N^c$ partition corresponding to $\mathbf{z}_{\sf{f} y^{\sf{f}}}$.  
    }
    \end{tabular}
    \newline
    \normalsize
\end{figure}

Similar to \cite[][Algorithm 1]{waggonerzha1999_res} and \citet[][p.745]{bgl_2015ijf}, draws from the predictive density $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})$ can then be obtained by the following algorithm:\\

\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
    \noindent\textbf{Algorithm 1: Draws from the predictive density $p(\mathbf{y}^f | \mathbf{y}^o, \mathbf{y}^c)$}\\

    Initialize $\Theta^{0}$ from $p(\Theta | y^o)$. For $g=1:(G_0+G)$
    
    \begin{enumerate}    
        \item Construct the system matrices $\mathbf{H}, \mathbf{M}, \mathbf{V}_{\!\varepsilon}, \mathbf{V}_{\!\upsilon}, \tilde{\mathbf{c}}, \mathbf{d}$ given the parameters $\Theta^{(g-1)}$
        \item Compute the mean $\boldsymbol{\mu}_{z}$ and precision matrix $\mathbf{Q}_{z}$ of the joint distribution of $\mathbf{z} = [\boldsymbol{\alpha}, \mathbf{y}]$  
        \item Permute $\mathbf{z}$, yielding $\mathbf{z}_{\mathcal{P}'} \sim \mathcal{N}(\mu_{z_\mathcal{P}'}, Q_{z_\mathcal{P}'}^{-1})$ with partitions $\transpose{[\mathbf{z}^{\sf{T}}_{\alpha\! y^{\sf{f}}}, \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}]}$
        \item Sample from the conditional distribution $\mathbf{z}_{\alpha\! y^{\sf{f}}} | \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}, \Theta^{(g-1)}$ using \citep[][Algorithm 2.1 and 2.4]{rueheld_2005} and reverse the permutation to back out the draws of $\mathbf{y}^{\sf{f}}$ and $\boldsymbol{\alpha}$
        \item Draw $\Theta^{(g)}$ from $p(\Theta|\mathbf{y}^o, \mathbf{y}^{\sf{c}}, {\mathbf{y}^{\sf{f}}}^{(g)}, \alpha^{(g)})$ 
     \end{enumerate}
    
    Discard the first $G_0$ draws as burn-in.\\


    \xdef\tpd{\the\prevdepth}
\end{minipage}

If the sample information and prior dominate the information in $\mathbf{y}^{\sf{c}}$, the impact of the conditioning arguments on the posterior distribution of the parameters is likely to be negligible and $p(\Theta | \mathbf{y}^o, \mathbf{y}^c) \approx p(\Theta | \mathbf{y}^o)$ \citep{delnegro_schorfheide_2013_hb,bgl_2015ijf}. In such applications, draws from Equation \ref{eqn:preddens} can then be obtained by skipping step 5 at each iteration in \textbf{Algorithm 1}. Similarly, when $\mathbf{y}^c$ is empty, Step 5 can obviously be skipped as well and the algorithm returns unconditional forecasts, i.e. draws from the density $p(\mathbf{y}^f | \mathbf{y}^o)$.

While the discussion above has focussed on dynamic factor models, it can easily be extended to general state space models of the form:

\begin{subequations}
    \label{eqn:statespacesys}
    \begin{align}
    \mathbf{y}_t &= Z \boldsymbol{\alpha}_t + \boldsymbol{\varepsilon_t}, \: \boldsymbol{\varepsilon_t} \sim \mathcal{N}(0, H) \\
    \boldsymbol{\alpha}_{t+1} &= T \boldsymbol{\alpha}_t + \boldsymbol{\eta}_t, \: \boldsymbol{\eta}_t \sim \mathcal{N}(0, Q) 
    \end{align}
\end{subequations}

by setting $P=1$, $J=S=0$.

Lastly, note that Hauber and Schumacher (2021) also propose a sequential sampler that first draws the missing observations conditional on the states and then the states given a complete data set. This approach performs somewhat better in terms of runtime and could in principle be used in Algorithm 1, which in any case requires a Gibbs sampling step to update the parameters given the conditioning set. However, the gains in performance from faster sampling of states and forecasts may not outweigh the longer burn-in required due to the additional block in the Gibbs Sampler.



\subsection{Soft conditions and repeated samples from $p(\mathbf{y}^f|\mathbf{y}^o, \Theta^{(g)})$}\label{subsec:softcond}

So far we have considered conditional forecasts that fix the future values of some of the endogeneous variables at single points. \citet{waggonerzha1999_res} labels these "hard" conditions to distinguish them from soft conditions where each element of $\mathbf{y}^c$ is merely restricted to lie within a pre-specified range, i.e. $\mathbf{y}^{\sf{f}}_l \leq \mathbf{y}^{\sf{f}} \leq \mathbf{y}^{\sf{f}}_u$. Draws from the unconditional predictive density $p(\mathbf{y}^f|\mathbf{y}^o, \Theta^{(g)})$ can be obtained until the conditions are satisfied.\footnote{Similarly, hard and soft conditions can be combined, e.g. if inflation forecasts are produced conditional on an oil price path and subject to the restriction that the Federal Funds rate is positive. In such a scenario, draws from $p(\mathbf{y}^f|\mathbf{y}^o, \mathbf{y}^c, \Theta^{(g)})$ are obtained until the soft restrictions are satisfied.}

The precision-sampling algorithms outlined above are particularly suited to this task since the bottleneck in the calculations is the Cholesky factorization of the band matrix $\mathbf{Q}_{\sf{f} y^{\sf{f}}}$ which requires $((T+H)R+NH)\cdot b_w^2$ floating point operations where $b_w$ is the lower bandwidth of $\mathbf{Q}_{\sf{f}, y^{\sf{f}}}$ \citep[][4.3.5]{GolubvanLoan2013}. However, it only needs to be computed once, irrespective of the number of desired draws \citep{rue2001_jrss}. Repeated samples thus come at a significantly reduced computational burden, consisting only of $(T+H)S+NH$ independent draws from the standard Normal distribution, band backward substitution - which requires $2 ((T+H)S+NH) \cdot b_w$ floating point operations \citep[][4.3.2]{GolubvanLoan2013}- and a vector addition.

\subsection{Comparison with Kalman filter-based simulation smoothers}

In simulations to determine the computational efficiency of the algorithms outlined above in comparison to Kalman filter-based simulation smoothers, we find that the former is computationally more efficient in terms of runtime for both large and small factor models and forecast horizons. The gains in computational efficiency are particularly pronounced for soft conditioning, where repeated draws from the predictive density are required. See \Cref{app:simulation} for details of the simulation study.  

% \subsubsection{Motivation and related literature}

% In simulations I compare the computational efficiency of the precision-sampling algorithms outlined above to simulation smoothers such as \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} that rely on the Kalman-filter and are commonly used in the literature. The focus is \textbf{not} on finding the overall best-performing algorithms. In particular, I do not consider low-level implementations in C or Fortran. Also I do not explore the extent to which parallelized execution could provide. Instead, the aim is to compare the algorithms .Previous work in this area has analyzed the computational advantages of such simulation smoothers compared to the algorithms for conditional forecasting given in \citet{waggonerzha1999_res} in the context of a vector autoregressions \citep{bgl_2015ijf}; \citet{mmp2011_csda} analyzes the performance of precision-based samplers in the context of a time-varying parameter regression and dynamic factor models and compares the performance to the \citet{durbinkoopman2002_biomtr} simulation smoother.\\

% I focus on both small ($N=20,\,R=2$) and large factor models ($N=100\,,R=10$) as well as a medium- and large-sized vector autoregression with $N=20$ and $N=100$, respectively; the lag length equals $P=4$ in both vector autoregressions. I evaluate the runtime of obtaining $100$ draws from the predictive density $p(y^f|y^o, y^c)$ given a sample size $T=100$ and forecast horizons $H = \{5, 20, 50\}$ conditional on a share $\kappa = 0.1$ of the observables (\textit{hard conditioning}).\footnote{The size of the conditioning set does not have a large impact on the relative runtime and in the interest of space I do not report results for $\kappa = \{0.5, 0.75\}$. They are available upon request.} Furthermore, I also consider the costs of producing repeated samples from the predictive density given the parameters as would be the case when on top of the conditioning set $$\mathbf{y}^{c} = [y_{1, T+1}, \dots, y_{\kappa \cdot N, T+1}, \dots, y_{1,T+H}, \dots, y_{\kappa \cdot N, T+H}]$$ restrictions are imposed on the remainining observables to lie within a prespecified range (\textit{soft conditioning}). 

% \subsubsection{Data-generating process}

% In the case of the dynamic factor model outlined in Section \ref{sec:dfm} the factor dynamics are given by a VAR(1) with parameters $\phi_1 = 0.7 \cdot I_R$ and $\Sigma = I_R$. The elements of the loadings on the contemporaneous factors $\lambda_0$ are drawn from independent Normal distributions with mean $0.5$ and variance $0.1$; all remaining loadings are set to 0, i.e. $K=0$. Setting $J=0$, the idiosyncratic components are i.i.d and $\omega_i$ is chosen such that the common component - $\boldsymbol{\lambda}_{0,i\cdot} \mathbf{f}_t$ - explains \sfrac{2}{3} of the total variation in the i-th variable. \\

% To generate data from a vector autoregression as discussed in Section \ref{sec:var}, I closely follow the set-up outlined in \citet{CHP2020_ijf}. Let $b_{p, ij}$ denote the element of $B_p$ in row i and column j. The diagonal elements are set to

% \begin{subequations}
%     \begin{align}
%         b_{1, ii} = \mathcal{N}(0, \sigma^2_o), \: \forall i = 1, \dots, N \\
% b_{p, ii} = \frac{b_{1,ii}}{p}, \: \forall p = 2, \dots, P
%     \end{align}    
% \end{subequations}

% \noindent while each off-diagonal element is drawn from $\mathcal{N}(0, \sigma^2_c)$ with probability $p_c$, otherwise it is set to $10^7$. The lag length $P$ equals 4 and for the medium-sized VAR with $N=20$ variables $\sigma^2_o = 0.2 $, $\sigma^2_c = 0.05$ and $p_c = 0.2$. To ensure a stationary VAR when the cross-section is large, i.e. $N=100$, the values are adjusted to $\sigma^2_o = 0.05 $, $\sigma^2_c = 0.001$ and $p_c = 0.1$.

% Details on the implementation of the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers are provided in Appendix \ref{app:kalmansimsmoothers}.\\

% \subsubsection{Results}

% I simulate each model $10$ times and calculate the time it takes to draw from the predictive density using the Kalman-filter based simulation smoother and the precision sampling algorithms outlined above.\footnote{The simulations were run using Matlab2020a and a XX and XX. To time the execution of the code, I used the function \texttt{timeit()}.} Figure \ref{figsimulation} shows the relative runtime for hard (left column) and soft (right column) as boxplots. The precision-based algorithms perform as well or better than the Kalman-filter based simulation smoothers for all models and horizons. When repeated draws from the predictive density are required as in the case of soft conditioning, the gains in computational efficiency are generally quite large. 

% \begin{figure}[htbp] \centering
%     \caption{Computational efficiency analysis of different simulation smoothers \label{figsimulation}}
%     \footnotesize
%     \begin{tabular}{p{16cm}}
%     \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_simulations_Ncond_10.pdf}} \\
%     {
%     \footnotesize \textbf{Note:} The figure shows the time it takes to generate a draw from the predictive density $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})$ for different forecast horizons $H$ using the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers relative to the precision sampler outlined above. The conditioning set $\mathbf{y}^{\sf{c}}$ consists of the first $0.1\cdot N$ variables (left column). For the small factor model, the number of variables is $N=20$ and the number of static factors is $R=2$. For the large factor model $N=100,\, R = 10$. For the medium- and large-sized vector autoregressions the number of lags is set to $P=4$ as well as $N=20$ and $N=100$, respectively. The runtime measures the entire procedure of generating a draw from the predictive density, including building the system matrices. For the precision-sampler this includes the costs of setting up the permutation matrices which is incurred only once.In the case of soft conditioning (right column), the runtime of producing 100 draws given the same parameters is reported. For details, see the main text.
%     }
%     \end{tabular}
%     \newline
%     \normalsize
% \end{figure}

\section{Real-time evaluation of unconditional and conditional forecasts}\label{sec:evaluation}

In this section, we present the data used in the forecast evaluation and provide details on the Bayesian estimation. Furthermore, we discuss the set-up of the real-time evaluation. 

\subsection{Data}
\subsubsection{Real-time dataset for the German economy}

To estimate the factor model, we construct a large quarterly real-time dataset covering different aspects of the macroeconomy. Beyond real gross domestic product, the main building block of the dataset are other series from the national accounts such as the entire expenditure side components, e.g. private and public consumption, gross fixed capital formation, export and imports. From the production side of GDP we can only include gross value added in the industrial and construction sector as well as trade, transport and hospitality as real-time vintages for other sectors are only available from 2011 onwards.\footnote{The three mentioned sectors comprise roughly 50 percent of total gross value added.} 

In addition to the chained volume indices we also include the corresponding deflators of the expenditure and production side components. Furthermore, our dataset contains series on real activity such as industrial production, turnover or orders as well as construction, headline CPI and PPI indices as well as the corresponding core indices and a measure of prices in the construction sector as well as labor market indicators such as economy-wide employment and wages and hours worked in the industrial and construction sector.    

In addition, data are supplemented with financial indicators such as interest rates, exchange rates and stock market indices. Lastly, we also include a few survey-based indicators provided by the European Commission (ESI) and covering the industrial, construction and services sectors as well as a measure of firms' employment expectations (EEI). Note that for the latter two sources, there is no real-time problem, as financial indicators are not revised subsequent to their original publication. Similarly, the European Commission's survey indicators are only revised due the seasonal adjustment procedure. That is to say, the survey responses for May 2006 as reported in 2018 might differ (slightly) from those originally published as the seasonal factors have since been updated. While this violates the real-time assumption, such revisions are likely to be small. We therefore abstract from these subsequent changes in what follows  and rely on the seasonally adjusted indices.

Altogether, our dataset contains \NumSeries\ series. All of them are transformed to stationarity prior to estimaton. In most cases, this involves taking the first difference of the logarithm of the original series, though in some cases, e.g. interest rates or survey indicators, we take simple differences.\footnote{The Deutsche Bundesbank's real-time database includes vintages of chained volume indices and nominal values for the national accounts. The implicit deflators are calculated by dividing the nominal series with the chained volume index and taking the first differenc of the logarithm of the resulting series.}Details of the time series, their sources as well as the transformation employed  can be found in \Cref{app:data}.

\subsubsection{Professional forecasts from the Reuters Poll}

As conditioning information, we rely on the Reuters Poll of forecasts for German GDP and CPI inflation. The survey compiles the views of around twenty different professional forecasters from the private sector and research institutes. It is conducted once a quarter during the first month, i.e. January, April, July and October. Until mid-2014, the respondents submit their views during the first few days of the month; from then on the survey date is around the middle of the month.\footnote{A complete list of the dates when the forecast polls were conducted is available upon request.}  

For GDP, the forecasts are given in terms of quarter-on-quarter (q/q) growth rates and we can thus use the median forecast directly to obtain conditional forecasts from the model outlined above. The inflation forecasts, however, are reported as year-over-year (y/y) growth rates of the consumer price index. To bring the forecasts in line with the treatment of CPI inflation in the model, we have to transform the individual forecasts. Specifically, we interpret the y/y forecasts of inflation, $x_t^{y/y}$, as describing the four-quarter change in the quarterly consumper price index $X$, i.e. $ x_t^{y/y} = log(X_t/X_{t-4})$. Thus, given a sequence of y/y forecasts we can back out the implicit forecasts of the (log of the) consumer price index and derive the corresponding q/q forecasts, i.e. $ x_t^{q/q} = log(X_t/X_{t-1})$ for each participant in the survey. We then use the median of these transformed values in the conditional forecasting exercise.

\begin{figure}[htbp] \centering
    \caption{GDP growth and CPI inflation forecasts from the Reuters poll of professional forecasters \label{fig:reuterspoll}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_ReutersPoll.pdf}} \\
	    {
        \footnotesize \textbf{Note:} The figure shows the mean forecast from the Reuters Poll of professional forecasters for quarter-on-quarter change in the comsumer price index (top panel) and quarter-on-quarter GDP growth. The forecast horizon $h$ is in quarters and relative to the reference period. For details, see the main text. \textit{Source}: Thomson Reuters, author's calculations.  
        }
    \end{tabular}
    \newline
    \normalsize
\end{figure}

\Cref{fig:reuterspoll} plots the median forecast of q/q CPI inflation (top row) and GDP growth (bottom row). The lines correspond to different forecast horizons relative to the reference period and are given in quarters. That is to say, the $h=1$ forecast for the first quarter in 2010 was made at the start of October 2009. 
 

\subsection{Estimation}

The parameters of the factor model outlined above are estimated with Bayesian methods. Draws from the joint posterior distribution of factors and parameters are obtained via a Gibbs Sampler which alternately draws from the conditional posterior of the factors given the parameters and \textit{vice versa}. For the estimation, we set $J=1$ and $S=0$ and $R=2$ and run the Gibbs Sampler for a total of 20000 iterations. We discard the first 10000 as burn-in and of the remaining, store every second draw, yielding 5000 from the posterior distribution of the parameters, $p(\Theta|y)$. As the priors used in the estimation as well as the resulting conditional posterior distributions are standard in the literature, we relegate a discussion to \Cref{app:gibbssampler}. 

\subsection{Evaluation set-up}

Given a vintage reflecting the data available to forecasters at the time the Reuters Poll was conducted and corresponding draws from the posterior distribution $p(\Theta|y)$, we generate draws from the predictive density as described in \Cref{subsec:precsampler} for all national accounts, price, labor market and activity indicators. These forecasts are unconditional as well as conditional on the values from the Reuters Poll for GDP growth and CPI inflation for the current and following two quarters, i.e. $h=\{0, 1, 2\}$. The evaluation sample starts in 2006 from which point on real-time vintages are available on a broad basis. It ends in the fourth quarter of 2017 as afterwards the number of missing entries in the Reuters Poll increases considerably. 

Within the sample, there are also a few data irregularities. For example, there are no CPI inflation forecasts in 2013Q1 from the Reuters Poll. We therefore exclude 2012Q3, 2012Q4 and 2013Q1 from the evaluation. Note also that for two labor market series there are some vintages which are released outside of the regular publication calendar. For example, the employment vintage scheduled for release at the end of December was not published until the first week of January, most likely due to the original publication date falling on a bank holiday. As such, when the professional forecasters were asked by Reuters to submit their forecasts by January 1\textsuperscript{st}, the latest available information for this series was published at the end of November. For hours worked in the manufacturing sector, all vintages between March 3\textsuperscript{rd} 2009 and March 4\textsuperscript{th} 2010 are missing. As these irregularities alter the information set and effective forecast horizon compared to the other quarters, we do not consider the affected vintages for these two series. For the remaining series, however, we have a total of 41 quarters to be evaluated for all horizons.

To benchmark the performance of the different priors, we consider a simple univariate Bayesian autoregressive model of order 2 for all the series.  i.e. 

\begin{equation}\label{eqn:benchmark}
y_{i,t} = \beta_0 + \beta_1 y_{i,t-1} + \beta_2 y_{i,t-2} + \epsilon_t; \, \epsilon_t \sim \mathcal{N}(0, \sigma^2)
\end{equation}

The parameters $\beta = [\beta_0, \beta_1, \beta_2]$  and $\sigma^2$ are estimated via a Gibbs Sampler with diffuse priors given by $\beta \sim \mathcal{N}(0,10 \cdot I_3)$  and $\sigma^2 \sim \mathcal{G}^{-1}(3,0.01)$. Draws from the h-step ahead predicitve density are obtained by plugging in draws from the posterior distribution of the coefficients and iterating Equation \ref{eqn:benchmark} forward.

Both the point and density forecast performance is evaluated. For the former, we compute the root mean squared forecast error (RMSFE) defined as
\begin{equation*}
\text{RMSFE} = \left( \frac{1}{S} \sum_{s=1}^S (\hat{y}_{i, T+s}|\Omega_{v} - y'_{i, T+s})^2 \right) ^{\frac{1}{2}}
\end{equation*}
for a sequence of S nowcasts, where $\hat{y}_{i, T+s}|\Omega_{v}$ is the mean of the predictive density of variable i at time $T+s$, conditional on the information set $\Omega_{v_s}$ available in real-time at date $v_s$ when the forecast for period $T+s$ was made. The corresponding realization, given by the first release, is denoted by $y'_{i,T+s}$. Density forecasts are evaluated by the continuous ranked probability score (CRPS) which is given by \citep[see][]{Gneitingetal2016}:
\begin{equation*}
\text{CRPS} = \frac{1}{S} \sum_{s=1}^{S} \left( \frac{1}{G} \sum_{k=1}^G \left\lvert {y}^{(k)}_{i,T+s}|\Omega_{v} - y'_{i, T+s} \right\rvert - \sum_{k=1}^G \sum_{j=1}^G \left\lvert {y}^{(k)}_{i,T+s}|\Omega_{v} - {y}^{(j)}_{i,T+s}|\Omega_{v} \right\rvert \right).
\end{equation*}

To calculate the CRPS given draws from the predictive density as well a realization, we use the R package \texttt{scoringRules} \citep{jkl_2019_jss}.

Finally, to compare the accuracy of the conditional and unconditional forecasts more formally, we follow \citet{ganicsodendahl_2021_ijf} and run Diebold-Mariano tests \citep{dm_1995_jbes}. Let $L_t(y^c_{i, t+h})$ denote the loss associated with the conditional forecasts made at time for variable i and horizon h. In the case of point forecasts, this is simply given by the squared forecast error; for density forecasts it is given by the corresponding CRPS. Similarly, define $L_t(y^u_{i, t+h})$ as the loss for the unconditional forecast and $d_t = L_t(y^c_{i, t+h}) - L_t(y^u_{i, t+h})$ the loss differential. The test statistic can then be written as 

$$
    DM_{i, h} = \frac{\frac{1}{T} \sum_{t=1}^{T} d_t}{\sqrt{\hat{\sigma}^2_d}}
$$

where $\hat{\sigma}^2_d$ is an estimate of the variance of $d_t$. We test the null hypothesis of equal predicitve ability against the one-sided alternative that the conditional forecasts yield a lower loss. 

\section{Results}\label{sec:results}

In this section we present the results of the forecast evaluation. We begin with point forecast performance before looking at how the evaluation changes when we focus on the entire predictive distribution. Lastly, we consider several robustness checks like the evaluation period as well as the model size (i.e. the number of factors). 

\subsection{Point forecast accuracy}

Turning to the peformance of the unconditional forecasts first, we find that for all horizons and the majority of variables, the factor model produces forecasts that perform similar to the autoregressive benchmark with relative RMSFE between 0.9 and 1.1 (\Cref{fig:rmsfe}). Outliers in this context are the headline and "core", i.e. excluding energy, producer price indices at $h=0$ where the performanc is considerably worse than the benchmark. Conversely, for a few series there are noteworthy gains over the simple univariate AR(2) model and these persist or even increase for larger horizons. For example, for $h=0$ the relative RMSFE for the construction price index and gross value added in the industrial sector is around 0.6. Smaller but still sizeable improvements can also be found for two series from the expenditure side of the national accounts: equipment investment as well as the deflator for residential investment. For larger $h$, these relative gains even increase: the relative RMSFE for the construction subcomponent of the PPI reaches 0.35; for gross value added in the industrial sector is 0.4. In addition to these variables, at $h=2$ there also noteworthy improvements for series that are likely highly correlated to gross industrial value added like production in the industrial and construction sector as well as industrial turnover. Note, however, that these improvements are partly due to the fact that the performance of the benchmark generally worsens as the forecast horizon increases.

For comparison, the model's unconditional forecasts for GDP growth lead to substantial relative gains of up to 20 percent relative to the autoregressive benchmark. For $h=0$, the professional forecasters are considerably more accurate (relative RMSFE of 0.65) but for larger horizons the performance is similar to the unconditional forecasts. The picture for CPI inflation is similar: the professional forecasters are more accurate than the model's unconditional forecast at $h=0$ but by a much smaller margin; for $h=\{1, 2\}$, the forecasts from the Reuters Poll are considerably (slightly) worse than the autoregressive benchmark (the model's unconditonal) forecasts.

Focusing on how the forecast performance increases when we condition on professional forecasters' views on GDP growth and CPI inflation, we find that there are only large differences in the relative RMSFE between the unconditional and conditional forecasts for $h=0$. Perhaps not surprisingly gains to conditioning arise for those series for which the models already produce decent forecasts (relative to autoregressive benchmarks) and are mainly concentrated to series from the national accounts group and - to some extent - indicators capturing real activity (but not part of the national accounts). For the labor market indicators and CPI and PPI inflation, we find much smaller gains from conditioning. Filled entries in the above plot correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 5 percent level in favor of the altnerative that the loss of the conditional forecasts is lower. This is almost never the case for any of the 37 variables we consider irrespective of the forecast horizon (\Cref{table:dm_rsmfe}). At $h=0$, i.e. forecasts that are made in the reference quarter, we reject the null for the gross value added in the trade, transport and hospitality sector; at $h=2$ for gross value added in the trade, transport and hospitality sector and employment. 

\begin{figure}[htp!] \centering
    \caption{Point forecast evaluation \label{fig:rmsfe}}
    \footnotesize
    \begin{tabular}{p{16cm}}
        \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_eval_rmsfe.pdf}} \\
        {
        \footnotesize \textbf{Note:} The figure shows the root mean squared forecast errors (RMSFE)  corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters' view on GDP growth and CPI inflation (y-axis) for different time series. For each series, the RMSFE is relative to an autoregressive benchmark. Entries above (below) the 45-degree line indicate that conditional forecasts perform worse (better) than the unconditional ones. Filled points correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 5 percent level. For details, see the main text. 
        }
        \end{tabular}
    \newline
    \normalsize
\end{figure}


\begin{table}[htb!]
    \centering
    \begin{threeparttable}
        \caption{Point forecast evaluation}
        \label{table:dm_rsmfe}
        \begin{tabular}{lrrr}
        \toprule
        category/horizon & h=0 & h=1 & h=2 \\ 
        \midrule
        activity &  1/20 & 0/20 & 1/20  \\ 
        labor market & 0/4 & 0/4 & 1/4  \\ 
        prices &0/13 & 0/13 & 0/13 \\ 
        \textbf{total} & \textbf{1/37} & \textbf{0/37} & \textbf{2/37} \\
        \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \scriptsize
            \item The table shows the fraction of times the null hypothesis of the Diebold-Mariano test is rejected at the 5 percent significance level against the one-sided alternative that the conditional forecasts yield a lower loss. For details, see the main text.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\subsection{Density forecast accuracy}

By and large, the results obtained in terms of point forecast accuracy carry over when we evaluate the entire predictive density. In particular, the performance relative to the benchmark as well as the gains from conditioning as measured by the average CRPS are very similar (\Cref{fig:crps}). Contrary to the findings for point forecasts, however, we do find significant differences between the conditional and unconditional forecast performance, at least for $h=\{0, 1\}$, again highlighted in the figure by the filled dots. For example, at $h=0$ the null of equal predictive accuracy is rejected for a third of all the variables that we consider, mainly concentrated in the activity group. For one-quarter ahead forecasts the number decreases slightly but for many activity series we still find significant gains (\Cref{table:dm_crps}). 

\begin{figure}[htp!] \centering
    \caption{Density forecast evaluation \label{fig:crps}}
    \footnotesize
    \begin{tabular}{p{16cm}}
        \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_eval_crps.pdf}} \\
        {
        \footnotesize \textbf{Note:} The figure shows the average continuous ranked probability score (CRPS)  corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters' view on GDP growth and CPI inflation (y-axis) for different time series. For each series, the CRPS is relative to an autoregressive benchmark. Entries above (below) the 45-degree line indicate that conditional forecasts perform worse (better) than the unconditional ones. Filled points correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 5 percent level. For details, see the main text. 
        }
        \end{tabular}
    \newline
    \normalsize
\end{figure}

\begin{table}[htb!]
    \centering
    \caption{Density forecast evaluation \label{table:dm_crps}}
    \begin{threeparttable}
        \begin{tabular}{lrrr}
        \toprule
        category/horizon & h=0 & h=1 & h=2 \\ 
        \midrule
        activity & 10/20 & 8/20 & 1/20 \\ 
        labor market &  1/4 & 1/4 & 2/4  \\ 
        prices & 2/13 & 0/13 & 0/13 \\ 
        \textbf{total} & \textbf{13/37} & \textbf{9/37} & \textbf{3/37} \\
        \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \scriptsize
            \item The table shows the fraction of times the null hypothesis of the Diebold-Mariano test is rejected at the 5 percent significance level against the one-sided alternative that the conditional forecasts yield a lower loss. For details, see the main text.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\subsection{Robustness checks}

As a first robustness check, we drop the global financial crisis of 2008-09 from the evaluation sample, to see if the gains in predictive accuracy for point and density forecasts are also apparent in more tranquil times. \Cref{fig:postcrisis} presents the results for the evaluation subsample ranging from 2011Q1 to 2017Q4.

\begin{figure}[htbp] \centering
    \caption{Robustness check: post-crisis evaluation sample \label{fig:postcrisis}}
    \footnotesize
    \begin{tabular}{p{16cm}}
        \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_eval_postcrisis.pdf}} \\
        {
        \footnotesize \textbf{Note:} The figure shows the root mean squared forecast errors (RMSFE) over the evaluation sample 2011Q1-2017Q4 corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters' view on GDP growth and CPI inflation (y-axis) for different time series. For each series, the RMSFE is relative to an autoregressive benchmark. Entries above (below) the 45-degree line indicate that conditional forecasts perform worse (better) than the unconditional ones. Filled points correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 10 percent level. For details, see the main text. 
        }
        \end{tabular}
    \newline
    \normalsize
\end{figure}

Overall, the forecast performance declines markedly relative to the autoregressive benchmarks but for some series, gains from conditioning still arise. However, for $h=\{1,2\}$ we also find that for many series - particularly from the activity group - conditional forecasts are on average less accuratewith the RMSFE of the unconditional forecasts, though not significantly so. Rejection of the null of the Diebold-Mariano test still occurs but with a fex exceptions, this occurs in cases where the autoregressive benchmark actually performs better than the factor model. As such, it is fair to say that developments during the financial crisis exert some influence on the results presented above for the entire evaluation sample.

Besides the evaluation period, the results presented above may also be sensitive to the model specification. In particular, they were obtained under a given number of factors, $R=2$. However, differently specified models yield similar forecast performance. \Cref{fig:differentR} shows the relative RMSFE of the model with $R=2$ on which the results above are based as well as both smaller ($R=1$) and larger ($R = {5,\,8}$) models. There is little indication that model specification systematically alters the forecast performance. Conditional forecasts at $h=0$ for the change in the producer index improve substantially when $R=8$ with the relative RMSFE decreasing by 0.25. To a lesser extent this also holds for the unconditional forecasts with the relative RMSFE dropping by around 0.1. However, this is the exception and for other horizons and variables we find much smaller differences in either way. Consequently, the points corresponding to the relative RMSFE for $R={1, 5, 8}$ in \Cref{fig:differentR} all hug the 45-degree line, which indicates identical forecast performance, quite closely. 

\begin{figure}[htbp] \centering
    \caption{Robustness check: different $R$ \label{fig:differentR}}
    \footnotesize
    \begin{tabular}{p{16cm}}
        \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_eval_robustness_Nr.pdf}} \\
        {
        \footnotesize \textbf{Note:} The figure shows the root mean squared forecast error (RMSFE) relative to the autoregressive benchmarks for different number of factors. On the x-axis are the values of the RMSFE for the model with 2 factors on which the main results are based. On the y-axis are the RMSFE for alternative model specifications, differentiated by colors. Entries above (below) the 45-degree line indicate that the model with 2 factors performs better (worse) than the alternative models.  
        }
        \end{tabular}
    \newline
    \normalsize
\end{figure}

\section{Conclusion}\label{sec:conclusion}

In this we assess the forecast performance of factor models  in real-time for Germany covering the period 2006-2017. We contribute to the literature by broadening the horizon of the forecast evaluation to include a large number of the variables that are used to estimate the model. In addition, we also investigate to what extent the forecast performance of the model improves when we incorporate external information in the form of professional forecasters' views on GDP growth and CPI inflation. Conditioning on forecasts rather than actual realized values gives an accurate assessement of the uncertainty around conditional forecasts that is relevant to policy-makers by capturing both how accurate the model can summarize the comovement of the time series as well as the accuracy of the forecasts on which we condition.

Our results show that the factor model produces forecasts for most of the series and horizons that are as accurate as those from autoregressive benchmarks. In some cases they outperform the benchmark by a large margin. When conditioning on the forecasts of professional forecasters for GDP growth and CPI inflation, we generally find some improvements in forecast accuracy for the variables of interest from categories such as real activity, prices and the labor market. Only for a smaller number of series, however, do we find that conditioning produces large gains. Moreover, for point forecasts the differences are statistically insignificant for virtually all series. When evaluating the predictive densities with the CRPS, we do find significant gains at $h=0$ ($h=1$) for around a third (a quarter) of the variables under consideration. To some extent the results appear to be driven by developments during the recession of 2008-09 following the Global Financial Crisis. as the forecast performance deteriorates relative to the benchmark when we shorten the evaluation sample to 2011-2017. On the other hand, the results are robust to model specification in the sense that increasing or decreasing the number of factors from $R=2$ as in the baseline specification does not systematically alter the forecast performance.

Lastly, this paper proposes precision-based sampling algorithms for conditional and unconditional forecasts in factor models and more broadly state space models with unobserved components. Treating forecasts as missing observations, the insights in \citet{HauberSchumacher2021} can be obtained directly to obtain draws from the relevant predictive densities. Simulations document that these algorithms can be more efficient from a computational point of view than simulation smoothers based on the Kalman filter. While the exact quantitative results of the excercise may vary depending on platform and implementation, the qualitative results indicate that precison-based sampling algorithms are a viable alternative in such applications. 
\newpage
\pagebreak


\bibliographystyle{aernobold}
\bibliography{papers}
\pagebreak
\appendix

% \section{Kalman-filter based simulation smoothers}\label{app:kalmansimsmoothers}

% This appendix provides details on the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers that are used in the simulations in Section \ref{sec:models}

% In this section we highlight how we implement the Kalman-filter based simulation smoothers and show how to obtain draws from the predictive densities. Although both the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers rely on the Kalman filter and smoother to produce a draw from the conditional distribution of the states, there are important conceptual differences between the two. Let $a_{t|s} = E[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ and $P_{t|s} = Var[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ denote the conditional mean and variance of the state at time $t$ conditional on information up to time $s$ which can be obtainted from the Kalman filter or smoother. In addition, let $a_t$ denote a draw from the conditional distribution $p(\alpha_t | \mathbf{y}_1, \dots, \mathbf{y}_{T+H})$. 

% \subsubsection{\citet{carterkohn1994_biomtr} simulation smoother}

% Given the parameters of the state space model $\Theta$ and  $a_{t|t}, P_{t|t} \: \forall t = 1, \dots, T+H$, the algorithm in \citet{carterkohn1994_biomtr} generates $a_{T+H}$ from 

% $$
% p(\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})
% $$

% \noindent with

% \begin{align*}
%     E[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = a_{T+H|T+H} \\
%     Var[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = P_{T+H|T+H}.
% \end{align*}

%  For $t=T+H-1, \dots, 1$ $a_t$ is generated from $p(\alpha_{t}|\mathbf{y}_t, a_{t+1})$. For details on how to derive the moments of the conditional distributions given the output of the Kalman filter, see the original paper as well as the textbook treatment in \citet{KimNelson1999mit}.

%  \subsubsection{\citet{durbinkoopman2002_biomtr} simulation smoother}

% The \citet{durbinkoopman2002_biomtr} simulation smoother produces a draw from the conditional distribution of the states by first simulating the state space model in Equation \ref{eqn:statespacesys}, producing a draw from the \textit{joint} distribution of states and observables

% $$p(\alpha_{1}, \dots, \alpha_{T+H}, \mathbf{y}_1, \dots, \mathbf{y}_{T+H}).$$

% Denote this joint draw by $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Running the Kalman smoother recursions yields $a_{t|T+H}, \: a^+_{t|T+H}$ where $a^+_{t|T+H} = E[\alpha^+_t|y^+_1, \dots, y^+_{T+H}]$.  By setting $a_t = a_{t|T+H} - a^+_{t|T+H} + a^+$ adjusts the mean of $a^+_t$ to yield a draw from the desired conditional distribution. In practice, it is more efficient to first construct $y^* = y-y^+$ and run the Kalman smoother only once, yielding $a^*_{t|T+H} = E[a^*_t|y^*]$. A draw from $p(\alpha_{1}, \dots, \alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})$ is then obtainted by setting $a_t = a^*_{t|T+H} + a^+_t$. Note that since only an estimate of the smoothed mean of the states is required, more efficient smoothing recursions can be employed which do not require the storage of $a_{t|t}$ and $P_{t|t}$ \citep[see][ch. 4.4.2]{durbinkoopman2002_biomtr}.

% \subsubsection{Drawing from the (conditional) predictive density $p(y^f|I, \Theta)$}
% Under the common assumption of uncorrelated measurement errors, draws from the predictive density $p(y^{\sf{f}}_t\,|\,a_t, y^o, y^c, \Theta)$ can  be obtained by sampling the measurement errors independently from $\mathcal{N}(0, \omega_i) \: \forall i s.t. y_{i,t} \notin y_{c,t}$ and adding them to the draw of the state vector \citep[see][for the general case of a full covariance matrix $\Omega$]{bgl_2015ijf}. For the \citet{durbinkoopman2002_biomtr} simulation smoother the generated artifical observations $y^+$ can be re-used, since $F a^*_{t|T+H} + y^+_t$ is a draw of the observations conditional on the sampled states. In the case of a VAR, there is no 
% measurement error so that $y^{\sf{f}}_t$ can be set equal to the corresponding elements of $a_t$. 

% \subsubsection{Multiple draws from $p(y^f|I, \Theta)$}
% Additional draws from the (conditional) predictive density given the same set of parameters $\Theta$ can be produced recursively from the \citet{carterkohn1994_biomtr} simulation smoother by independently sampling $a_t$ as many times as required and conditioning on the draw to generate $y^{\sf{f}}_t$. In the case of the \citet{durbinkoopman2002_biomtr} simulation smoother, each additional draw requires new simulated values of the states and observations, $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Given these, the entire filtering and smoothing recursions need to be re-run. However, computational savings still arise as the state covariances $P_{t|t}$ and functions thereof that are calculated during the recursions do not depend on the observations. As such they only need be calculated once and can be passed on to subsequent runs of the Kalman filter and smoother \citep[p. 606]{durbinkoopman2002_biomtr}.

% \subsubsection{Initialisation}
% In the case of a dynamic factor model, the Kalman filter is initialized by setting $a_{1|0}$ and $P_{1|0}$ equal to the unconditional mean and variance, respectively, of the factor VAR. Vector autoregression applications only require the simulation smoother to run over the sample with missing observations, i.e. $t\geq T+1$. The recursions are therefore initialized with $a_{T+1|T} = \mathbf{T} \, \transpose{[\transpose{\mathbf{y}}_T, \dots, \transpose{\mathbf{y}}_{T-P+1}]}$ and $P_{T+1|T} = \Sigma$, unless the more efficient one-step algorithm in \citet{durbinkoopman2002_biomtr} is employed, in which case $a_{T+1|T} = 0$ is required \citep[see][]{jarocinski2015csda}.

% \pagebreak

\section{Data}
\label{app:data}
This Appenix provides detail on the real-time dataset used in the empirical application. The \NumSeries\ time series can be grouped in to the following categories: \textbf{activity} which includes the gross domestic product as well as expenditure- and production-side components of output. In addition, series like industrial production, turnover and orders also form part of this category. The \textbf{prices} category contains the (implicit) deflators corresponding to the chained volume indices of the national accounts mentioned above as well as consumer and producer price series. The \textbf{labor market} is covered by series on total employment, wages as well as hours worked in the industrial and construction sector. These are the series which we focus on in the forecast evaluation and their vintages are all sourced from the Deutsche Bundesbank's Real-Time Database (RTD). Moreover, to capture the \textbf{financial} side of the economy, we include series such as interest rates, commodity prices and the CDAX stock market index. These series are also sourced from the Deutsche Bundesbank's website. Lastly, \textbf{survey} indicators are taken from the European Commission's Business and Consumer Surveys. We focus on the components of the Economic Sentiment Indicator (ESI) which capture confidence in the industrial, consumer, services, retail trade and construction sectors. In addition, we also include the employment expectations index (EEI).\footnote{The series were downloaded on April 27th, 2021 from \url{https://ec.europa.eu/economy_finance/db_indicators/surveys/documents/series/nace2_ecfin_2106/main_indicators_sa_nace2.zip}} \Cref{tab:data} lists the data series used in the empirical application, the group the indicators belong to, their source as well as any transformation applied to the series prior to estimation, e.g. whether series are in logs or differenced (diff). 

\begingroup\fontsize{6}{8}\selectfont
\begin{ThreePartTable}
    \begin{TableNotes}
    \item[s] seasonally adjusted
    \item[c] calendar adjusted
    \end{TableNotes}
\begin{landscape}
\begin{longtable}{| l | c | l | l | l | l |} 
    \caption{Description of the dataset} \\% needs to go inside longtable environment
    \toprule
    \rowcolor{gray!50}
    \textbf{Series} & \textbf{Mnem.} &  \textbf{Group} & \textbf{Transf.} & \textbf{Source} & \textbf{Notes} \\
    \midrule
    gross domestic product, chain index\tnote{s,c} & gdp &  activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.AG1.CA010.A.I  \\ \hline 
    gross domestic product, deflator\tnote{s,c} & p\_gdp &  prices &  log, 1\textsuperscript{st} diff  & Bundesbank RTD & implicit deflator, i.e. nominal series divided by chain index  \\ \hline 
    private consumption expenditure, chain index\tnote{s,c} & c\_priv &  activity &  log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.CA1.BA100.A.I  \\ \hline 
    private consumption expenditure, deflator\tnote{s,c} & p\_c\_priv &  prices &  log, diff  & Bundesbank RTD & implicit deflator, i.e. nominal series divided by chain index  \\ \hline 
    government consumption expenditure, chain index\tnote{s,c} & c\_gov &  activity &  log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.CA1.BA100.A.I  \\ \hline 
    government consumption expenditure, implicit deflator\tnote{s,c} & p\_c\_gov &  prices &  log, diff  & Bundesbank RTD & nominal series divided by chain index  \\ \hline
    equipment investment, chain index\tnote{s,c} & gfcf\_equip & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.CE1.CA010.A.I\\ \hline
    equipment investment, implicit deflator\tnote{s,c} & p\_gfcf\_equip & prices & log, diff  & Bundesbank RTD & implicit deflator, i.e. nominal series divided by chain index\\ \hline
    construction investment, chain index\tnote{s,c} & gfcf\_constr & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.CF1.CA010.A.I\\ \hline
    construction investment, implicit deflator\tnote{s,c} & p\_gfcf\_constr & prices & log, diff  & Bundesbank RTD & implicit deflator, i.e. nominal series divided by chain index\\ \hline
    other investment, chain index\tnote{s,c} & gfcf\_other & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.CI1.CA010.A.I\\ \hline
    other investment, implicit deflator\tnote{s,c} & p\_gfcf\_other & prices & log, diff  & Bundesbank RTD & nominal series (BBKRT.Q.DE.Y.A.CI1.CA010.V.A) divided by chain index\\ \hline
    exports (of goods and services), chain index\tnote{s,c} & x & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.CX1.CA010.A.I\\ \hline
    exports (of goods and services), implicit deflator\tnote{s,c} & p\_x & prices & log, diff  & Bundesbank RTD & nominal series (BBKRT.Q.DE.Y.A.CX1.CA010.V.A) divided by chain index\\ \hline
    imports (of goods and services), chain index\tnote{s,c} & m & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.CM1.CA010.A.I\\ \hline
    imports (of goods and services), implicit deflator\tnote{s,c} & p\_m & prices & log, diff  & Bundesbank RTD & nominal series (BBKRT.Q.DE.Y.A.CM1.CA010.V.A) divided by chain index\\ \hline
    gross value added, industry, chain index\tnote{s,c} & gva\_ind & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.AU1.AA020.A.I\\ \hline
    gross value added, industry, implicit deflator\tnote{s,c} & p\_gva\_ind & prices & log, diff  & Bundesbank RTD & nominal series (BBKRT.Q.DE.Y.A.CM1.CA010.V.A) divided by chain index\\ \hline
    gross value added, construction, chain index\tnote{s,c} & gva\_constr & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.AU1.AA030.A.I\\ \hline
    gross value added, construction, implicit deflator\tnote{s,c} & p\_gva\_constr & prices & log, diff  & Bundesbank RTD & nominal series (BBKRT.Q.DE.Y.A.AU1.AA030.V.A) divided by chain index\\ \hline    
    gross value added, trade, transport \& hospitality, chain index\tnote{s,c} & gva\_tth & activity & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.Y.A.AU1.AA040.A.I\\ \hline
    gross value added, trade, transport \& hospitality, implicit deflator\tnote{s,c} & p\_gva\_tth & prices & log, diff  & Bundesbank RTD & nominal series (BBKRT.Q.DE.Y.A.AU1.AA040.V.A) divided by chain index\\ \hline   
    consumer prices, index\tnote{s,c} & cpi & prices & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.Y.P.PC1.PC100.R.I \\ \hline
    consumer prices excl. food \& energy, index\tnote{s,c} & cpi\_core & prices & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.S.P.PC1.PC110.R.I \\ \hline
    producer prices, index\tnote{s} & ppi & prices & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.S.P.PP1.PP100.R.I \\ \hline
    producer prices excl. energy, index\tnote{s} & ppi\_core & prices & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.S.P.PP1.PP200.R.I \\ \hline    
    producer prices, construction, index & ppi\_constr & prices & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.N.P.PP1.PP300.R.I \\ \hline
    producer prices, agriculture, index\tnote{s} & ppi\_agri & prices & log, diff  & Bundesbank RTD & Code: BBKRT.Q.DE.N.P.PP1.PP400.R.I \\ \hline 
    production, industry, index\tnote{s,c} & prod & activity & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.Y.I.IP1.ACM01.C.I \\ \hline
    orders, industry, index\tnote{s,c} & ord & activity & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.Y.I.IO1.ACM01.C.I \\ \hline
    turnover, industry, index\tnote{s,c} & to & activity & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.Y.I.IT1.ACM01.V.I \\ \hline
    production, construction, index\tnote{s,c} & prod & activity & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.Y.I.IP1.AA020.C.I \\ \hline
    orders, construction, index\tnote{s,c} & ord & activity & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.Y.I.IO1.AA031.C.I \\ \hline
    turnover, construction, index\tnote{s,c} & to & activity & log, diff  & Bundesbank RTD & Code: BBKRT.M.DE.Y.I.IT1.AA031.V.A \\ \hline
    turnover, retail, index\tnote{s,c} & to & activity & log, diff  & Bundesbank RTD & price adjusted, Code: BBKRT.M.DE.Y.I.IT1.AGA01.C.I \\ \hline
    employment \tnote{s, c} & emp & labor market & log, diff & Bundesbank RTD & Code: BBKRT.Q.DE.S.A.BF1.CA010.P.A \\ \hline
    hours worked, industry\tnote{s,c} & h\_ind & labor market & log, diff & Bundesbank RTD & Code: BBKRT.M.DE.Y.L.BE2.AA022.H.I \\ \hline
    hours worked, construction\tnote{s,c} & h\_constr & labor market & log, diff & Bundesbank RTD & Code: BBKRT.M.DE.Y.L.BE2.AA031.H.A \\ \hline
    wages and salaries \tnote{s,c} & w & labor market & log, diff & Bundesbank RTD & Code: BBKRT.Q.DE.S.A.DE2.CA010.V.A \\ \hline
    CDAX, index & cdax & financial & log, diff & Bundesbank & Code: BBK01.WU001A \\ \hline
    Nominal effective exchange rate, narrow & neer19 & financial & log, diff & Bundesbank & Vis-a-vis 19 trading partners, code: BBEE1.M.I8.AAA.XZE012.A.AABAN.M00 \\ \hline
    Nominal effective exchange rate, broad & neer38 & financial & log, diff & Bundesbank & Vis-a-vis 38 trading partners, code: BBEE1.M.I8.AAA.XZE021.A.AABAN.M00 \\ \hline
    Government bond yields (1-year) & i1y & financial & diff & Bundesbank & Code: BBK01.WZ9808 \\ \hline
    Government bond yields (5-year) & i1y & financial & diff & Bundesbank & Code: BBK01.WZ9816 \\ \hline
    Government bond yields (10-year) & i1y & financial & diff & Bundesbank & Code: BBK01.WZ9826 \\ \hline
    Commodity prices, energy, index & comm\_en & financial & log, diff & HWWI & in €, sourced via Deutsche Bundesbank (BBDG1.M.HWWI.N.EURO.ENERGY00.I15.EUR.A) \\ \hline
    Commodity prices, excl. energy, index & comm\_exen & financial & log, diff & HWWI & in €, sourced via Deutsche Bundesbank (BBDG1.M.HWWI.N.EURO.TOTNXNGY.I15.EUR.A) \\ \hline
    industrial confidence indicator\tnote{s} & survey\_ind &  surveys & diff  & Eurp. Commission &  \textemdash \\ \hline 
    services confidence indicator\tnote{s} & survey\_serv &  surveys & diff  & Eurp. Commission & \textemdash  \\ \hline 
    consumer confidence indicator\tnote{s} & survey\_cond &  surveys & diff  & Eurp. Commission & \textemdash  \\ \hline 
    retail trade confidence indicator\tnote{s} & survey\_retail &  surveys & diff  & Eurp. Commission & \textemdash  \\ \hline 
    construction confidence indicator\tnote{s} & survey\_constr &  surveys & diff  & Eurp. Commission & \textemdash  \\ \hline 
    employment expectation index\tnote{s} & survey\_eei &  surveys & diff  & Eurp. Commission & \textemdash  \\ \hline 
    \bottomrule
\insertTableNotes
\label{tab:data}
\end{longtable}
\end{landscape}
\end{ThreePartTable}
\endgroup

\pagebreak
\newpage

\section{Gibbs Sampler}
\label{app:gibbssampler}
\normalsize
This Appendix details how to sample from the posterior distribution of the parameters of the factor model, $p(\Theta|\mathbf{y})$. For the reader's convenience, I repeat the model description before discussing the blocks of the Gibbs Sampler. Setting $J=1$, $P=2$ and $S=0$, the following equations constitute the model:

\begin{subequations}
    \label{eqn:app_model}
    \begin{align}
        \mathbf{y}_t &=  \lambda \, \mathbf{f}_{t} + \mathbf{e}_t \label{eqn:app_facmod_obs}\\ 
        \mathbf{f}_t &= \phi_1 \, \mathbf{f}_{t-1} + \phi_2 \mathbf{f}_{t-2} + \boldsymbol{\upsilon}_t \label{eqn:app_facmod_factors}\\
        \mathbf{e}_t &= \psi \, \mathbf{e}_{t-1} + \boldsymbol{\varepsilon}_t \label{eqn:app_facmod_idios}
    \end{align}
\end{subequations}

The Gibbs Sampler then cycles through the following blocks to draw from the conditional posterior distributions\footnote{Initial values for the coefficients  $\Theta$ are obtained from principal component estimates of the factors. 
}:
\newline
\linebreak
\textbf{Step 1:}$p(\mathbf{f}|\mathbf{y}, \Theta)$
\newline
Conditional on the parameters, the factors are drawn from their posterior distribution following the precision sampler outlined in \Cref{subsec:precsampler} of the main text.
\newline
\linebreak
\textbf{Step 2:} $p(\lambda| \mathbf{y}, \mathbf{f}, \psi, \Sigma)$
\newline
Given the factors, the observation equations of the model are independent and the loadings can be sampled variable by variable. Let the prior on the loadings be given by $\mathcal{N}(0, P_{\lambda}^{-1})$ and let $P_{e_i} = (H^{\sf{T}}_{\psi} \sigma^{-2}_i H_{\psi})^{-1}$ denote the $T \times T$ precision matrix of $e_i$, where 
$$
\begin{bmatrix}
    I_N &  \\
    \shortminus \psi_i & I_N &  \\
     & \shortminus \psi_i & I_N &  \\
     &  &  &  &  & \\
    & \dots & \dots & \shortminus \psi_i & I_N \\
     &  &  &  &  & \\
    & &  & \dots & \dots & \shortminus \psi_i & I_N.
\end{bmatrix}
$$
Using standard linear regression results \citep[see][]{chankroese_2013}, the conditional posterior for the i-th equation is given by
\begin{eqnarray}\label{eq:posterior_lambda}
    \lambda_{i} | \cdot &\sim& \mathcal{N}\left(\mathbf{m}^{\lambda}_i,\mathbf{M}^{\lambda}_i \right) \\
    \mathbf{M}^{\lambda}_i &=& (\mathbf{P}_{\lambda} + \mathbf{f}^{\sf{T}} \mathbf{P}_{e_i} \mathbf{f})^{-1} \\ 
    \mathbf{m}^{\lambda}_i &=& \mathbf{M}^{\lambda}_i (\mathbf{f}^{\sf{T}} \mathbf{P}_{e_i} \mathbf{y}_i).
\end{eqnarray} 
We set $\mathbf{P}_{\psi} = \mathbf{I}_R$ for all variables. 
\newline
\linebreak
\textbf{Step 3:} $p(\psi| \mathbf{y}, \mathbf{f}, \lambda, \Sigma)$
\newline       
 Conditional on a draw of the loadings, for each series we can back out the idiosyncratic components $\mathbf{e}$ and update $\psi$. Given a Normal prior for $\psi_i$ with mean 0 and precision matrix $P_{\psi}$, the resulting posterior is given by 
    \begin{eqnarray}\label{eq:posterior_psi}
        \psi_{i \boldsymbol{\cdot} } | \cdot &\sim& \mathcal{N}\left( m^{\psi}_i,M^{\psi}_i \right) \\
        M^{\psi}_i &=& \left( P_{\psi} +  \frac{1}{\sigma^2_i} \sum\nolimits_{t=1}^{T-1} e_{i, t}^2 \right)^{-1}\\
        m^{\psi}_i &=& M^{\psi}_i \left( \frac{1}{\sigma^2_i}\sum\nolimits_{t=2}^T e_{i, t-1}^{\prime} e_{i,t} \right).
    \end{eqnarray}
For all variables, we set $P_{\psi} = 1$.
\newline
\linebreak
\textbf{Step 4:} $p(\Sigma| \mathbf{y}, \mathbf{f}, \lambda, \psi)$
\newline       
Each diagonal elements of $\Sigma$ can be sampled by drawing from 
\begin{equation*}
\sigma^2_i \sim \mathcal{G}^{-1} \left( \frac{u + T-1}{2},\frac{U + \sum_{t=2}^T \varepsilon_{i,t}^2}{2} \right)
\end{equation*}  where $u$ and $U$ are the prior shape and rate. We set $u=3, \, U = 0.5$ so that the prior mean and standard deviation equal 0.25. 
\newline
\linebreak
\textbf{Step 5:} $p(\phi_1, \phi_2| \mathbf{f}, \Omega)$
\newline  
Let $\mathbf{X}_t = I_R \otimes [\mathbf{f}^{\sf{T}}_{t-1}, \mathbf{f}^{\sf{T}}_{t-2}]$ denote the lags of the factors and $\mathbf{X} = [\mathbf{X}_1, \dots, \mathbf{X}_t]$. Then given a Normal prior $p(\phi) \sim \mathcal{N}(0, \mathbf{P}_{\phi}^{-1})$, the coefficients of the factor VAR can be sampled by drawing from $\phi | \cdot \sim \mathcal{N}(\mathbf{m}^{\phi}, \mathbf{M}^{\phi})$ where
$$
\mathbf{M}^{\phi} = (\mathbf{P}_{\phi} + X^{\sf{T}}(\mathbf{I}_T \otimes \Omega^{-1})\mathbf{X})^{-1}, \: \mathbf{m}^{\phi} = \mathbf{M}^{\phi} (\mathbf{X}^{\sf{T}} (\mathbf{I}_T \otimes \Omega^{-1})\mathbf{f})
$$
In line with the literature on Bayesian vectorautoregressions, we impose a Minnesota-type prior on the VAR coefficients. The prior mean is set equal to 0 and the prior precision increases in the lag length. Specifically, the prior variance of the elements of $\phi$ for $p = 1:P$ is given by:
\[
    Var(\phi_{p,ij}) = 
	\begin{cases}
    	\frac{\pi_0}{p^2},& \text{if } i=j\\
    	\frac{\pi_1}{p^2},& \text{otherwise.}
	\end{cases}
  \]
Common values in the literature are $\pi_0 = 0.2$ and $\pi_1=0.1$, thus shrinking coefficients on the lags of other factors stronger towards 0.
\newline   
\pagebreak
\newpage

\section{Computational efficiency analysis}\label{app:simulation}

This Appendix compares the computational efficiency of the precision-sampling algorithms outlined in the main text to simulation smoothers such as \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} that rely on the Kalman-filter and are commonly used in the literature. Previous work in this area has analyzed the computational advantages of such simulation smoothers compared to the algorithms for conditional forecasting given in \citet{waggonerzha1999_res} in the context of a vector autoregressions \citep{bgl_2015ijf}; \citet{mmp2011_csda} analyzes the performance of precision-based samplers in the context of a time-varying parameter regression and dynamic factor models and compares the performance to the \citet{durbinkoopman2002_biomtr} simulation smoother.

The focus lies on both small factor models with $N=20$ variables and $R=2$ factors as well as two variants of larger factor models with $N=100\,,R=2$ (\textit{large N factor model}) and $N=100\,,R=2$ (\textit{large factor model}).  We evaluate the runtime of obtaining $100$ draws from the predictive density $p(y^f|y^o, y^c)$ given a sample size $T=100$ and forecast horizons $H = \{5, 20, 50\}$ conditional on a share $\kappa = 0.1$ of the observables (\textit{hard conditioning}).\footnote{The size of the conditioning set does not have a large impact on the relative runtime and in the interest of space I do not report results for $\kappa = \{0.5, 0.75\}$. They are available upon request.} Furthermore, we also consider the costs of producing repeated samples from the predictive density given the parameters as would be the case when on top of the conditioning set $$\mathbf{y}^{c} = [y_{1, T+1}, \dots, y_{\kappa \cdot N, T+1}, \dots, y_{1,T+H}, \dots, y_{\kappa \cdot N, T+H}]$$ restrictions are imposed on the remainining observables to lie within a prespecified range (\textit{soft conditioning}).

The dynamics of the factor model outlined in \Cref{sec:precsampler} are given by a VAR(1) with parameters $\phi_1 = 0.7 \cdot I_R$ and $\Sigma = I_R$. The elements of the loadings on the contemporaneous factors $\lambda_0$ are drawn from independent Normal distributions with mean $0.5$ and variance $0.1$; all remaining loadings are set to 0, i.e. $K=0$. Setting $J=0$, the idiosyncratic components are i.i.d and $\omega_i$ is chosen such that the common component - $\boldsymbol{\lambda}_{0,i\cdot} \mathbf{f}_t$ - explains \sfrac{2}{3} of the total variation in the i-th variable. 

Details on the implementation of the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers are provided in \Cref{app:kalmansimsmoothers}.

Each model is simulated $10$ times.\footnote{The simulations were run using Matlab2020a on an Intel Core i5 2.3 Ghz with 8 GB of RAM. To time the execution of the code, we used the function \texttt{timeit()}.} \Cref{figsimulation} shows the relative runtime for hard (left column) and soft (right column) conditioning as boxplots. The precision-based algorithms performs compared to the Kalman-filter based simulation smoothers for all horizons, with gains in computational efficiency up to a factor of 10. When repeated draws from the predictive density are required as in the case of soft conditioning, the gains in computational efficiency are much larger.

\begin{figure}[htbp] \centering
    \caption{Computational efficiency analysis of different simulation smoothers \label{figsimulation}}
    \footnotesize
    \begin{tabular}{p{16cm}}
    \multicolumn{1}{c}{\includegraphics*[scale = 0.5]{../figures/fig_simulations_Ncond_10.pdf}} \\
    {
    \footnotesize \textbf{Note:} The figure shows the time it takes to generate draws from the predictive density $p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})$ for different forecast horizons $H$ using the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers relative to the precision sampler outlined in the main text. The conditioning set $\mathbf{y}^{\sf{c}}$ consists of the first $0.1\cdot N$ variables (left column). For the small factor model, the number of variables is $N=20$ and the number of static factors is $R=2$; for the large N factor model, the number of variables is $N=100$, while the large factor model also has $R=10$. The runtime measures the entire procedure of generating a draw from the predictive density, including building the system matrices. For the precision-sampler this includes the costs of setting up the permutation matrices which is incurred only once. In the case of soft conditioning (right column), the runtime of producing 100 draws given the same parameters is reported. For details, see the main text.
    }
    \end{tabular}
    \newline
    \normalsize
\end{figure}

\pagebreak
\newpage

\section{Kalman-filter based simulation smoothers}\label{app:kalmansimsmoothers}

This appendix provides details on the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers that are used in the simulations in \Cref{app:simulation}. Although both the \citet{carterkohn1994_biomtr} and \citet{durbinkoopman2002_biomtr} simulation smoothers rely on the Kalman filter and smoother to produce a draw from the conditional distribution of the states, there are important conceptual differences between the two. 

Let $a_{t|s} = E[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ and $P_{t|s} = Var[\alpha_t|\mathbf{y}_1, \dots, \mathbf{y}_s]$ denote the conditional mean and variance of the state at time $t$ conditional on information up to time $s$ which can be obtainted from the Kalman filter or smoother. In addition, let $a_t$ denote a draw from the conditional distribution $p(\alpha_t | \mathbf{y}_1, \dots, \mathbf{y}_{T+H})$. 

\subsubsection*{\citet{carterkohn1994_biomtr} simulation smoother}

Given the parameters of the state space model $\Theta$ and  $a_{t|t}, P_{t|t} \: \forall t = 1, \dots, T+H$, the algorithm in \citet{carterkohn1994_biomtr} generates $a_{T+H}$ from 

$$
p(\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})
$$

\noindent with

\begin{align*}
    E[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = a_{T+H|T+H} \\
    Var[\alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H}] = P_{T+H|T+H}.
\end{align*}

 For $t=T+H-1, \dots, 1$ $a_t$ is generated from $p(\alpha_{t}|\mathbf{y}_t, a_{t+1})$. For details on how to derive the moments of the conditional distributions given the output of the Kalman filter, see the original paper as well as the textbook treatment in \citet{KimNelson1999mit}.

 \subsubsection*{\citet{durbinkoopman2002_biomtr} simulation smoother}

The \citet{durbinkoopman2002_biomtr} simulation smoother produces a draw from the conditional distribution of the states by first simulating the state space model in Equation \ref{eqn:statespacesys}, producing a draw from the \textit{joint} distribution of states and observables

$$p(\alpha_{1}, \dots, \alpha_{T+H}, \mathbf{y}_1, \dots, \mathbf{y}_{T+H}).$$

Denote this joint draw by $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Running the Kalman smoother recursions yields $a_{t|T+H}, \: a^+_{t|T+H}$ where $a^+_{t|T+H} = E[\alpha^+_t|y^+_1, \dots, y^+_{T+H}]$.  By setting $a_t = a_{t|T+H} - a^+_{t|T+H} + a^+$ adjusts the mean of $a^+_t$ to yield a draw from the desired conditional distribution. In practice, it is more efficient to first construct $y^* = y-y^+$ and run the Kalman smoother only once, yielding $a^*_{t|T+H} = E[a^*_t|y^*]$. A draw from $p(\alpha_{1}, \dots, \alpha_{T+H}|\mathbf{y}_1, \dots, \mathbf{y}_{T+H})$ is then obtainted by setting $a_t = a^*_{t|T+H} + a^+_t$. Note that since only an estimate of the smoothed mean of the states is required, more efficient smoothing recursions can be employed which do not require the storage of $a_{t|t}$ and $P_{t|t}$ \citep[see][ch. 4.4.2]{durbinkoopman2002_biomtr}.

\subsubsection*{Drawing from the (conditional) predictive density $p(y^f|I, \Theta)$}
Under the common assumption of uncorrelated measurement errors, draws from the predictive density $p(y^{\sf{f}}_t\,|\,a_t, y^o, y^c, \Theta)$ can  be obtained by sampling the measurement errors independently from $\mathcal{N}(0, \omega_i) \: \forall i s.t. y_{i,t} \notin y_{c,t}$ and adding them to the draw of the state vector \citep[see][for the general case of a full covariance matrix $\Omega$]{bgl_2015ijf}. For the \citet{durbinkoopman2002_biomtr} simulation smoother the generated artifical observations $y^+$ can be re-used, since $F a^*_{t|T+H} + y^+_t$ is a draw of the observations conditional on the sampled states.

\subsubsection*{Multiple draws from $p(y^f|I, \Theta)$}
Additional draws from the (conditional) predictive density given the same set of parameters $\Theta$ can be produced recursively from the \citet{carterkohn1994_biomtr} simulation smoother by independently sampling $a_t$ as many times as required and conditioning on the draw to generate $y^{\sf{f}}_t$. In the case of the \citet{durbinkoopman2002_biomtr} simulation smoother, each additional draw requires new simulated values of the states and observations, $a^+_1, \dots, a^+_{T+H}, y^+_1, \dots, y^+_{T+H}$. Given these, the entire filtering and smoothing recursions need to be re-run. However, computational savings still arise as the state covariances $P_{t|t}$ and functions thereof that are calculated during the recursions do not depend on the observations. As such they only need be calculated once and can be passed on to subsequent runs of the Kalman filter and smoother \citep[p. 606]{durbinkoopman2002_biomtr}.

\subsubsection*{Initialisation}

In the case of a dynamic factor model, the Kalman filter is initialized by setting $a_{1|0}$ and $P_{1|0}$ equal to the unconditional mean and variance, respectively, of the factor VAR.

\pagebreak
% \section{Extension to vector autoregressions}

% Vector autoregressions (VAR) are another class of popular macroeconometric models and can be described by the following equation: 

% \begin{equation}\label{eqn:var}
%     \mathbf{y}_t = \mathbf{b} + \sum_{p=1}^P \mathbf{B}_p \mathbf{y}_{t-p} + \boldsymbol{\upsilon}_t
% \end{equation}

% where $\mathbf{b}$ is an $N \times 1$ vector of intercepts, $\mathbf{B}_1, \dots, \mathbf{B}_P$ are the $N \times N$ coefficient matrices and $\boldsymbol{\upsilon}_t \sim \mathcal{N}(0, \Sigma)$. With slight modifications that account for the fact that in the case of a VAR the states are observed without meausrement error, the algorithm outlined above can also be employed to generate conditional forecasts.\footnote{Unconditional forecasts given the parameters can simply be obtained by drawing $\boldsymbol{\upsilon}_{T+1}, \dots, \boldsymbol{\upsilon}_{T+1}$ and using Equation \ref{eqn:var} to recursively construct $\mathbf{y}_{T+1}, \dots, \mathbf{y}_{T+H}$.}

% For conditional forecasts, the predictive density is given by 

% \begin{equation}\label{eqn:preddens}    
%     p(\mathbf{y^{\sf{f}}}|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}}) \propto \int_{\Theta} p(\mathbf{y^{\sf{f}}}|\mathbf{y}_T, \dots, \mathbf{y}_{T-P+1}, \mathbf{y^{\sf{c}}}, \Theta) p(\Theta|\mathbf{y^{\sf{o}}}, \mathbf{y^{\sf{c}}})\, \text{d}\Theta.
% \end{equation}

% where in the absence of unobserved states the forecasts given the parameters only depend on the last $P$ observations of the observables. 

% That is to say, given the parameters the model can rewritten in stacked form as


% \begin{equation*}
%     \begin{bmatrix}
%         I_N \\
%         \shortminus\mathbf{B}_1 & I_N \\
%         & \ddots & \ddots & \\
%         \shortminus\mathbf{B}_{P} & \dots & \shortminus\mathbf{B}_1 & I_N & & \\
%         & \ddots &  & \ddots & \ddots & \\
%         & & \shortminus\mathbf{B}_{P} & \dots & \shortminus\mathbf{B}_1 & I_N       
%     \end{bmatrix}
%     \begin{bmatrix}
%         \mathbf{y}_{T-P+1} \\
%         \vdots \\
%         \mathbf{y}_{T} \\
%         \mathbf{y}_{T+1} \\
%         \vdots \\
%         \mathbf{y}_{T+H} 
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%         \mathbf{b} \\
%         \vdots \\
%         \mathbf{b} \\
%         \mathbf{b} \\
%         \vdots \\
%         \mathbf{b} 
%     \end{bmatrix}
%     +
%     \begin{bmatrix}
%         \boldsymbol{\upsilon}_{T-P+1} \\
%         \vdots \\
%         \boldsymbol{\upsilon}_{T} \\
%         \boldsymbol{\upsilon}_{T+1} \\
%         \vdots \\
%         \boldsymbol{\upsilon}_{T+H} 
%     \end{bmatrix}
% \end{equation*}

% or more compactly:
% \begin{equation*}
%     \mathbf{H} \mathbf{y} = \mathbf{c} + \boldsymbol{\upsilon}.
% \end{equation*}

% Define $\mathbf{z}={y} = \transpose{[\transpose{\mathbf{y}_{T-P+1}}, \dots, \transpose{\mathbf{y}_T}, \dots, \transpose{\mathbf{y}_{T+H}}]}$. Then the joint distribution of available observations and forecasts is Normal:

% $$
% \mathbf{z} \sim \mathcal{N}(\mathbf{H}^{-1} \mathbf{c}, (\transpose{\mathbf{H}} \mathbf{V}^{-1}_{\!\upsilon}\mathbf{H})^{-1}).
% $$

% Draws from the predictive density $p(y^f| y^o, y^c)$ can be obtained by permuting $\mathcal{P} \mathbf{z} \coloneqq \mathbf{z}_{\mathcal{P}} = \transpose{[\mathbf{y}^{\sf{f}}, \mathbf{y}^{\sf{o}}, \mathbf{y}^{\sf{c}}]}$, where $y^o = \transpose{[\transpose{\mathbf{y}_{T-P+1}}, \dots, \transpose{\mathbf{y}_T}]}$.\\

% The corresponding mean and precision matrix can be partitioned as follows:

% \begin{align*}
%     \boldsymbol{\mu}_{z_{\mathcal{P}}} &= 
%     \coloneqq
%     \begin{bmatrix}
%         \boldsymbol{\mu}_{y^{\sf{f}}} \\
%         \boldsymbol{\mu}_{y^{\sf{o}}}
%     \end{bmatrix}
%     ,\\
%     \mathbf{Q}_{z_{\mathcal{P}}} &= 
%     \coloneqq
%     \begin{bmatrix}
%         \mathbf{Q}_{y^f} & \mathbf{Q}_{y^f\!y^o, y^c} \\
%         \mathbf{Q}^{\sf{T}}_{y^f\!y^o, y^c} & \mathbf{Q}_{y^o, y^c}
%     \end{bmatrix}
%     .
% \end{align*}

% The distribution of forecasts given observed data and conditioning set $y^c$ is then given by 

% \begin{align*}
%     \boldsymbol{\mu}_{y^f|y^o, y^c} &= 
%     \boldsymbol{\mu}_{y^f} - \mathbf{Q}_{y^f}^{-1} \mathbf{Q}_{y^{\sf{f}}\!y^o, y^c} (\mathbf{y}_o - \boldsymbol{\mu}_{y^o, y^c})\\
%     \mathbf{Q}_{y^f|y^o, y^c} &= \mathbf{Q}_{y^f}.
% \end{align*}

% Draws from $p(y^f|y^o, y^c)$ can be obtained efficiently even when $H$ is large as the precision matrix $\mathbf{Q}_{y^f}$ is banded. As above, we illustrate this point by way of an example: Figure \ref{figQVAR} shows the precision matrix $Q_{z_{\mathcal{P}}}$ for a VAR with $N=100$, $P=4$, a forecast horizon of $H=20$ and forecasts conditional on the first 10 variables, i.e. 

% $$y^c = [y_{1,T+1}, \dots, y_{10,T+1}, \dots, y_{1,T+20}, \dots, y_{10,T+20}].$$

% The highlighted area corresponds to $\mathbf{Q}_{y^f}$.

% \begin{figure}[htbp] \centering
%     \caption{Precision matrix $Q_{\mathcal{z}_{\mathcal{P}}}$ in the case of a VAR \label{figQVAR}}
%     \footnotesize
%     \begin{tabular}{p{16cm}}
%     \multicolumn{1}{c}{\includegraphics*[scale = 0.6]{../figures/fig_Qperm_VAR.pdf}} \\
%     {
%     \footnotesize \textbf{Note:} The figure shows the precision matrix of the permuted vector $\mathbf{z}_{\mathcal{P}}$ for a vector autoregression with  $N=100$, $P=4$, a forecast horizon of $H=20$ and forecasts conditional on the first 10 variables. Highlighted by the square is the partition corresponding to the forecasts $\mathbf{y}^{\sf{f}} = [y_{-c, T+1}, \dots, y_{-c, T+H}]$.
%     }
%     \end{tabular}
%     \newline
%     \normalsize
% \end{figure}

\pagebreak







\end{document}
