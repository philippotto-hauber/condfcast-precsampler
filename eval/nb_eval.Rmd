---
title: "Forecast evaluation - Analysis and plots"
output: html_notebook
---

This notebook analyses the performance of unconditional and conditional forecasts for a large number of macroeconomic time series. 

## Set-up

```{r}
setwd("C:/Users/Philipp/Documents/GitHub/condfcast-precsampler/eval")
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate)
library(ggsci)
```

## Load model and benchmark forecasts

```{r}
# models
load("df_eval_models.Rda")
df_eval_models$quarter <- as_date(df_eval_models$quarter)
df_eval_models$vintage <- as_date(df_eval_models$vintage)

# benchmarks
load("benchmarks/df_eval_benchmark.Rda")
```

## Manually remove some vintages

For two labor market series, there are some missing vintages. These lead to an information set that is inconsistent with the rest of the evaluation period. For example, because of the missing vintages and data, a calendar-based forecast horizon of $-1$, say, does not reflect the same horizon in terms of observations. 

The series are

- `h_ind`: For hours worked in the manufacturing sector, several vintages in the year 2009 are missing. Specifically, no vintages are published between March 16 2009 and March 4 2010. However, it seems that the usual release pattern is only followed from March 15 2010 onwards. On March 4, only the values up to December 2009 are published. Therefore, all vintages between March 16 2009 and March 15 2010 are excluded from the analysis

```{r}
df_eval_models <- filter(df_eval_models, 
                        !(mnemonic == "h_ind" &
                              vintage < "2010-03-15" &
                                vintage > "2009-03-16")
            )

df_eval_benchmark <- filter(df_eval_benchmark, 
                           !(series == "h_ind" & 
                               vintage < "2010-03-15" & 
                                  vintage > "2009-03-16")
                          )
```


- `emp`: For total employment, the "December 2013" vintage was released in early January 2014 (quite likely due to the original or usual publication date falling on a bank holiday). The previous vintage was published on November 28. Any vintages in between those two dates are removed! 

```{r}
df_eval_models <- filter(df_eval_models, 
                        !(mnemonic == "emp" &
                              vintage < "2014-01-07" &
                                vintage > "2013-11-28")
                  )

df_eval_benchmark <- filter(df_eval_benchmark, 
                           !(series == "emp" &
                                vintage < "2014-01-07" &
                                  vintage > "2013-11-28")
                          )
```

## Function to calculate relative RMSFE, average log score, CRPS

Function that calculates the forecast accuracy of a selected **model** in a selected **subsample** relative to the benchmark. Output is dataframe `df_eval`
```{r}
calc_relative_forecast_accuracy <- function(df_eval_models, df_eval_benchmark, str_model, subsample)
{
  # calculate RMSFE, log score and CRPS over subsample for chosen model and benchmark
  df_eval_models %>% 
  filter(model == str_model,
         quarter >= subsample[1],
         quarter <= subsample[2]) %>%
  group_by(mnemonic, horizon, type, model) %>%
  summarise(rmsfe = sqrt(mean(sfe)), 
            mean_logs = mean(logs),
            mean_crps = mean(crps)) %>%
  ungroup() -> df_eval_models_avg

  df_eval_benchmark %>% 
  filter(quarter >= subsample[1],
         quarter <= subsample[2]) %>%
  group_by(series, horizon) %>%
  summarise(rmsfe = sqrt(mean(sfe)), 
            mean_logs = mean(logs),
            mean_crps = mean(crps)) %>%
  ungroup() -> df_eval_benchmark_avg
  
  # relative RMSFE, log score and CRPS
  df_eval_models_avg %>% 
    merge(select(df_eval_benchmark_avg, 
                  horizon,
                  mnemonic = series, 
                  rmsfe_ar = rmsfe,
                  logs_ar = mean_logs, 
                  crps_ar = mean_crps
                ), 
          by = c("horizon", "mnemonic")) %>%
    mutate(rel_rmsfe = rmsfe / rmsfe_ar,
           rel_logs = mean_logs / logs_ar,
           rel_crps = mean_crps / crps_ar) -> df_eval
  
  # merge with categories
  source("../data/realtime_data.R")
  tmp <- realtime_data()
  tmp <- select(tmp, mnemonic, category)
  df_eval <- merge(df_eval, tmp, by = "mnemonic")
  rm(tmp)
  
  # select columns
  df_eval <- select(df_eval, mnemonic, horizon, type, category, rel_rmsfe, rel_logs, rel_crps)
  
  return(df_eval)
}
```


## Analysis

```{r}
horizon_as_factor <- function(x)
{
  return(factor(paste0("h=", x)))
}
```



```{r}
str_model <- "Nr2_Nj1_Np2_Ns0"
subsample <- c(min(df_eval_models$quarter), max(df_eval_models$quarter))

df_eval <- calc_relative_forecast_accuracy(df_eval_models, 
                                           df_eval_benchmark, 
                                           str_model, 
                                           subsample)

```
### Point forecast accuracy

```{r}
# df_plot
str_title <- "rel. RMSFE"
df_eval %>%
  filter(horizon >= 0) %>%
  pivot_longer(c(rel_rmsfe, rel_crps, rel_logs), names_to = "score", values_to = "value") %>%
  filter(value != Inf, score == "rel_rmsfe") %>%
  pivot_wider(names_from = type, values_from = value) -> df_plot 

# data-dependent lims!
ylims <- c(min(min(df_plot$unconditional), min(df_plot$conditional_hard)),
           max(max(df_plot$unconditional), max(df_plot$conditional_hard)))
xlims <- ylims

# manually set lims, excluding "outliers"
# ylims <- c(0.8, 1.2)
# xlims <- ylims

ggplot(df_plot, aes(x = unconditional, 
                    y = conditional_hard, 
                    color = category)
       )+
  geom_point(size = 2, alpha = 0.7)+
  geom_abline()+
  geom_vline(xintercept = 0,  size = 0.2)+
  geom_hline(yintercept = 0, size = 0.2)+
  facet_wrap(~horizon_as_factor(horizon), nrow = 1)+
  xlim(xlims)+
  ylim(ylims)+
  scale_color_jco()+
  #theme_minimal()+
  theme(legend.position="bottom", 
        legend.text = element_text(size = 8),
        legend.title = element_blank())+
  labs(title = str_title, x = "unconditional forecasts", y = "conditional forecasts")
```


### Density forecast accuracy

```{r}
# df_plot
str_title <- "rel. CRPS"
df_eval %>%
  filter(horizon >= 0) %>%
  pivot_longer(c(rel_rmsfe, rel_crps, rel_logs), names_to = "score", values_to = "value") %>%
  filter(value != Inf, score == "rel_crps") %>%
  pivot_wider(names_from = type, values_from = value) -> df_plot 

# data-dependent lims!
ylims <- c(min(min(df_plot$unconditional), min(df_plot$conditional_hard)),
           max(max(df_plot$unconditional), max(df_plot$conditional_hard)))
xlims <- ylims

# manually set lims, excluding "outliers"
# ylims <- c(0.8, 1.2)
# xlims <- ylims

ggplot(df_plot, aes(x = unconditional, 
                    y = conditional_hard, 
                    color = category)
       )+
  geom_point(size = 2)+
  geom_abline()+
  geom_vline(xintercept = 0,  size = 0.2)+
  geom_hline(yintercept = 0, size = 0.2)+
  facet_wrap(~horizon_as_factor(horizon), nrow = 1)+
  xlim(xlims)+
  ylim(ylims)+
  scale_color_jco()+
  #theme_minimal()+
  theme(legend.position="bottom", 
        legend.text = element_text(size = 8),
        legend.title = element_blank())+
  labs(title = str_title, x = "unconditional forecasts", y = "conditional forecasts")
```

## Robustness

### Different subsamples

```{r}
str_model <- "Nr2_Nj1_Np2_Ns0"
subsample <- c(as.Date("2010-01-01"), max(df_eval_models$quarter))

df_eval <- calc_relative_forecast_accuracy(df_eval_models, 
                                           df_eval_benchmark, 
                                           str_model, 
                                           subsample)

```

```{r}
# df_plot
str_title <- "post-crisis sample: RMSFE and CRPS"
df_eval %>%
  filter(horizon >= 0) %>%
  pivot_longer(c(rel_rmsfe, rel_crps, rel_logs), names_to = "score", values_to = "value") %>%
  filter(value != Inf, score %in% c("rel_rmsfe", "rel_crps")) %>%
  pivot_wider(names_from = type, values_from = value) -> df_plot 

# data-dependent lims!
ylims <- c(min(min(df_plot$unconditional), min(df_plot$conditional_hard)),
           max(max(df_plot$unconditional), max(df_plot$conditional_hard)))
xlims <- ylims

# manually set lims, excluding "outliers"
# ylims <- c(0.8, 1.2)
# xlims <- ylims

ggplot(df_plot, aes(x = unconditional, 
                    y = conditional_hard, 
                    color = category,
                    shape = score)
       )+
  geom_point(size = 2, alpha = 0.7)+
  geom_abline()+
  geom_vline(xintercept = 0,  size = 0.2)+
  geom_hline(yintercept = 0, size = 0.2)+
  facet_wrap(~horizon_as_factor(horizon), nrow = 1)+
  xlim(xlims)+
  ylim(ylims)+
  scale_color_jco()+
  #theme_minimal()+
  theme(legend.position="bottom", 
        legend.text = element_text(size = 8),
        legend.title = element_blank())+
  labs(title = str_title, x = "unconditional forecasts", y = "conditional forecasts")
```



### Different models



