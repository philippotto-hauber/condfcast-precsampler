---
title: "Forecast evaluation - Analysis and plots"
output: html_notebook
---

This notebook analyses the performance of unconditional and conditional forecasts for a large number of macroeconomic time series. 

## Set-up

```{r}
rm(list = ls())
setwd("C:/Users/Philipp/Documents/GitHub/condfcast-precsampler/eval")
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate)
library(ggsci)
library(latex2exp)
library(xtable)

source("../functions/functions.R")

# turn of scientific notation
options(scipen = 999) 
```

## Load model and benchmark forecasts

```{r}
# models
load("df_eval_models.Rda")
df_eval_models$quarter <- as_date(df_eval_models$quarter)
df_eval_models$vintage <- as_date(df_eval_models$vintage)

# benchmarks
load("benchmarks/df_eval_benchmark.Rda")
```

## Manually remove some series and vintages

For **gross value added in finance, rent and professional servies** there are only vintages up until 2012, after which disaggregate series for the different sector are available. I therefore do not consider the series

```{r}
df_eval_models <- filter(df_eval_models, !(mnemonic == "gva_freprof"), !(mnemonic == "p_gva_freprof"))
```


For two labor market series, there are some missing vintages. These lead to an information set that is inconsistent with the rest of the evaluation period. For example, because of the missing vintages and data, a calendar-based forecast horizon of $-1$, say, does not reflect the same horizon in terms of observations. 

The series are

- `h_ind`: For hours worked in the manufacturing sector, several vintages in the year 2009 are missing. Specifically, no vintages are published between March 16 2009 and March 4 2010. However, it seems that the usual release pattern is only followed from March 15 2010 onwards. On March 4, only the values up to December 2009 are published. Therefore, all vintages between March 16 2009 and March 15 2010 are excluded from the analysis

```{r}
df_eval_models <- filter(df_eval_models, 
                        !(mnemonic == "h_ind" &
                              vintage < "2010-03-15" &
                                vintage > "2009-03-16")
            )

df_eval_benchmark <- filter(df_eval_benchmark, 
                           !(series == "h_ind" & 
                               vintage < "2010-03-15" & 
                                  vintage > "2009-03-16")
                          )
```


- `emp`: For total employment, the "December 2013" vintage was released in early January 2014 (quite likely due to the original or usual publication date falling on a bank holiday). The previous vintage was published on November 28. Any vintages in between those two dates are removed! 

```{r}
df_eval_models <- filter(df_eval_models, 
                        !(mnemonic == "emp" &
                              vintage < "2014-01-07" &
                                vintage > "2013-11-28")
                  )

df_eval_benchmark <- filter(df_eval_benchmark, 
                           !(series == "emp" &
                                vintage < "2014-01-07" &
                                  vintage > "2013-11-28")
                          )
```

## Function to calculate relative RMSFE, average log score, CRPS

Function that calculates the forecast accuracy of a selected **model** in a selected **subsample** relative to the benchmark. Output is dataframe `df_eval`
```{r}
calc_relative_forecast_accuracy <- function(df_eval_models, df_eval_benchmark, str_model, subsample)
{
  # calculate RMSFE, log score and CRPS over subsample for chosen model and benchmark
  df_eval_models %>% 
  filter(model == str_model,
         quarter >= subsample[1],
         quarter <= subsample[2]) %>%
  group_by(mnemonic, horizon, type, model) %>%
  summarise(rmsfe = sqrt(mean(sfe)), 
            mean_logs = mean(logs),
            mean_crps = mean(crps)) %>%
  ungroup() -> df_eval_models_avg

  df_eval_benchmark %>% 
  filter(quarter >= subsample[1],
         quarter <= subsample[2]) %>%
  group_by(series, horizon) %>%
  summarise(rmsfe = sqrt(mean(sfe)), 
            mean_logs = mean(logs),
            mean_crps = mean(crps)) %>%
  ungroup() -> df_eval_benchmark_avg
  
  # relative RMSFE, log score and CRPS
  df_eval_models_avg %>% 
    merge(select(df_eval_benchmark_avg, 
                  horizon,
                  mnemonic = series, 
                  rmsfe_ar = rmsfe,
                  logs_ar = mean_logs, 
                  crps_ar = mean_crps
                ), 
          by = c("horizon", "mnemonic")) %>%
    mutate(rel_rmsfe = rmsfe / rmsfe_ar,
           rel_logs = mean_logs / logs_ar,
           rel_crps = mean_crps / crps_ar) -> df_eval
  
  # merge with categories
  source("../data/realtime_data.R")
  tmp <- realtime_data()
  tmp <- select(tmp, mnemonic, category = group)
  df_eval <- merge(df_eval, tmp, by = "mnemonic")
  rm(tmp)
  
  # select columns
  df_eval <- select(df_eval, mnemonic, horizon, type, category, rel_rmsfe, rel_logs, rel_crps)
  
  return(df_eval)
}
```

## Function for DM test 

```{r}
calc_dm_stats <- function(df_eval_models, pval_threshold, subsample, str_model, score)
{
  if (score == "sfe")
  {
    df_eval_models %>% 
    select(-logs, -crps, -vintage) %>% 
    filter(horizon >= 0, model == str_model) %>% 
    pivot_wider(names_from = "type", values_from = "sfe") -> tmp1
  } else if (score == "crps") 
  {
    df_eval_models %>% 
    select(-logs, -sfe, -vintage) %>% 
    filter(horizon >= 0, model == str_model) %>% 
    pivot_wider(names_from = "type", values_from = "crps") -> tmp1
  } else 
    stop("Expected score to be either sfe or crps. Abort")


  tmp1 %>% 
    filter(quarter>=subsample[1], quarter <= subsample[2]) %>% 
    group_by(mnemonic, model, horizon) %>% 
    summarise(dm_pval = dm_test(conditional_hard, unconditional)) %>%
    left_join(unique(select(df_eval, 
                            mnemonic, 
                            category)), by = "mnemonic") %>% 
    group_by(category, horizon, model) %>% 
    mutate(is_significant = ifelse(dm_pval <= pval_threshold, 
                                   1, 0)) -> dm_stats
  return(dm_stats)
}
```

```{r}
table_dm_test <- function(dm_stats)
{
  dm_stats %>% 
  summarise(n_significant = sum(is_significant),
            n_tot = n()) -> tmp1

dm_stats %>% 
  ungroup() %>% 
  group_by(horizon, model) %>% 
  summarise(n_significant = sum(is_significant),
            n_tot = n()) %>% 
  mutate(category = "total") -> tmp2

tmp3 <- rbind(tmp1, tmp2)

tmp3 %>% 
  group_by(category, model, horizon) %>% 
  mutate(result = paste0(n_significant, "/", n_tot),
         horizon = paste0("h=", horizon)) %>% 
  select(-n_significant, -n_tot, -model) %>% 
  pivot_wider(names_from = "horizon", values_from = "result") %>% 
  xtable() %>% 
  print.xtable() 
  return(NULL)
}
```

```{r}
add_is_significant_col<- function(df_plot, dm_stats)
{
  dm_stats %>% 
  ungroup() %>% 
  select(mnemonic, is_significant, horizon) %>% 
  right_join(df_plot, by = c("mnemonic", "horizon")) -> df_plot
  return(df_plot)
}
```



## Analysis

```{r}
horizon_as_factor <- function(x)
{
  return(factor(paste0("h=", x)))
}
```


```{r}
str_model <- "Nr2_Nj1_Np2_Ns0"
subsample <- c(min(df_eval_models$quarter), max(df_eval_models$quarter))

df_eval <- calc_relative_forecast_accuracy(df_eval_models, 
                                           df_eval_benchmark, 
                                           str_model, 
                                           subsample)
```
### Point forecast accuracy

#### DM tests

```{r}
pval_threshold <- 0.05
dm_stats <- calc_dm_stats(df_eval_models, pval_threshold, subsample, str_model, "sfe")
```

```{r}
table_dm_test(dm_stats)
```



```{r}
# df_plot
str_title <- "rel. RMSFE"
df_eval %>%
  filter(horizon >= 0) %>%
  pivot_longer(c(rel_rmsfe, rel_crps, rel_logs), names_to = "score", values_to = "value") %>%
  filter(value != Inf, score == "rel_rmsfe") %>%
  pivot_wider(names_from = type, values_from = value) -> df_plot

```

add variable indicating if DM test is significant

```{r}
df_plot <- add_is_significant_col(df_plot, dm_stats)
```

```{r}
# data-dependent lims!
ylims <- c(min(min(df_plot$unconditional), min(df_plot$conditional_hard)),
           max(max(df_plot$unconditional), max(df_plot$conditional_hard)))
xlims <- ylims

# manually set lims, excluding "outliers"
# ylims <- c(0.8, 1.2)
# xlims <- ylims
```



```{r}
ggplot(mapping = aes(x = unconditional, 
                     y = conditional_hard, 
                     color = category)
       )+
  geom_point(data = filter(df_plot, is_significant == 1), size = 3, shape = 16, show.legend = FALSE)+
  geom_point(data = filter(df_plot, is_significant == 0), size = 3, shape = 1)+
  geom_abline()+
  geom_vline(xintercept = 0,  size = 0.2)+
  geom_hline(yintercept = 0, size = 0.2)+
  facet_wrap(~horizon_as_factor(horizon), nrow = 1)+
  xlim(xlims)+
  ylim(ylims)+
  scale_color_jco()+
  #theme_minimal()+
  theme(legend.position="bottom", 
        legend.text = element_text(size = 8),
        legend.title = element_blank())+
  labs(title = "", x = "RMSFE, unconditional forecasts", y = "RMSFE, conditional forecasts")
```

#### Export plot to pdf

```{r}
ggsave(filename = "../figures/fig_eval_rmsfe.pdf", 
       width = 10, height = 6, units = "in")
```

#### Notes for figure

> The figure shows the root mean squared forecast errors (RMSFE)  corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters' view on GDP growth and CPI inflation (y-axis) for different time series. For each series, the RMSFE is relative to an autoregressive benchmark. Filled points correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 10 percent level. For details, see the main text. Entries above (below) the 45-degree line indicate that conditional forecasts perform worse (better) than the unconditional ones.  



#### Analysis

```{r}
  df_eval_benchmark %>% 
  filter(quarter >= subsample[1],
         quarter <= subsample[2]) %>%
  group_by(series, horizon) %>%
  summarise(rmsfe = sqrt(mean(sfe))) %>% 
  select(series, horizon, rmsfe) %>% 
  pivot_wider(names_from = "horizon", values_from = "rmsfe")
```


- aboslute RMSFE across horizons

```{r}
  df_eval_models %>% 
  filter(model == str_model,
         quarter >= subsample[1],
         quarter <= subsample[2],
         type == "unconditional",
         horizon >= 0) %>%
  group_by(mnemonic, horizon, model) %>%
  summarise(rmsfe = sqrt(mean(sfe))) %>% 
  select(mnemonic, horizon, rmsfe) %>% 
  pivot_wider(names_from = "horizon", values_from = "rmsfe")
```


- `h=0`

top 10

```{r}
df_plot %>% 
  filter(horizon == 0) %>% 
  select(mnemonic, unconditional, horizon) %>% 
  pivot_wider(names_from = "horizon", values_from = "unconditional") %>% 
  arrange(`0`) %>% 
  head(10)
```


bottom 10

```{r}
df_plot %>% 
  filter(horizon == 0) %>% 
  select(mnemonic, unconditional, horizon) %>% 
  pivot_wider(names_from = "horizon", values_from = "unconditional") %>% 
  arrange(`0`) %>% 
  tail(10)
```


```{r}


df_plot %>% 
  filter(horizon == 1) %>% 
  select(mnemonic, unconditional, horizon) %>% 
  pivot_wider(names_from = "horizon", values_from = "unconditional") %>% 
  arrange(`1`) %>% 
  head(10)

df_plot %>% 
  filter(horizon == 2) %>% 
  select(mnemonic, unconditional, horizon) %>% 
  pivot_wider(names_from = "horizon", values_from = "unconditional") %>% 
  arrange(`2`) %>% 
  head(10)
```

```{r}
df_plot %>% 
  mutate(diff_fore = conditional_hard - unconditional) %>% 
  group_by(category, horizon) %>% 
  summarise(mean_diff = mean(diff_fore)) %>% 
  pivot_wider(values_from = mean_diff, names_from = horizon)
```
### Density forecast accuracy

#### DM tests

```{r}
pval_threshold <- 0.05
dm_stats <- calc_dm_stats(df_eval_models, 
                          pval_threshold, 
                          subsample,
                          str_model, 
                          "crps")
```

```{r}
table_dm_test(dm_stats)
```

#### Plot

```{r}
# df_plot
str_title <- "rel. CRPS"
df_eval %>%
  filter(horizon >= 0) %>%
  pivot_longer(c(rel_rmsfe, rel_crps, rel_logs), names_to = "score", values_to = "value") %>%
  filter(value != Inf, score == "rel_crps") %>%
  pivot_wider(names_from = type, values_from = value) -> df_plot 
```



add variable indicating if DM test is significant
```{r}
df_plot <- add_is_significant_col(df_plot, dm_stats)
```

```{r}
# data-dependent lims!
ylims <- c(min(min(df_plot$unconditional), min(df_plot$conditional_hard)),
           max(max(df_plot$unconditional), max(df_plot$conditional_hard)))
xlims <- ylims

# manually set lims, excluding "outliers"
# ylims <- c(0.8, 1.2)
# xlims <- ylims
```


```{r}
ggplot(df_plot, aes(x = unconditional, 
                    y = conditional_hard, 
                    color = category)
       )+
    geom_point(data = filter(df_plot, is_significant == 1), size = 3, shape = 16, show.legend = FALSE)+
  geom_point(data = filter(df_plot, is_significant == 0), size = 3, shape = 1)+
  geom_abline()+
  geom_vline(xintercept = 0,  size = 0.2)+
  geom_hline(yintercept = 0, size = 0.2)+
  facet_wrap(~horizon_as_factor(horizon), nrow = 1)+
  xlim(xlims)+
  ylim(ylims)+
  scale_color_jco()+
  #theme_minimal()+
  theme(legend.position="bottom", 
        legend.text = element_text(size = 8),
        legend.title = element_blank())+
  labs(title = "", x = "CRPS, unconditional forecasts", y = "CRPS, conditional forecasts")
```

#### Export plot to pdf

```{r}
ggsave(filename = "../figures/fig_eval_crps.pdf", 
       width = 10, height = 6, units = "in")
```

#### Notes for figure

> The figure shows the continous rank probability score (CRPS) corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters' view on GDP growth and CPI inflation (y-axis) for different time series. For each series, the CRPS is relative to an autoregressive benchmark. For details, see the main text. Entries above (below) the 45-degree line indicate that conditional forecasts perform worse (better) than the unconditional ones.  

#### Analyis

## Robustness

### Different subsamples

```{r}
str_model <- "Nr2_Nj1_Np2_Ns0"
subsample <- c(as.Date("2011-01-01"), max(df_eval_models$quarter))

df_eval <- calc_relative_forecast_accuracy(df_eval_models, 
                                           df_eval_benchmark, 
                                           str_model, 
                                           subsample)

```

#### DM tests

```{r}
pval_threshold <- 0.05
dm_stats <- calc_dm_stats(df_eval_models, 
                          pval_threshold, 
                          subsample, 
                          str_model, 
                          "sfe")
```

```{r}
table_dm_test(dm_stats)
```

```{r}
# df_plot
str_title <- "post-crisis sample: RMSFE and CRPS"
df_eval %>%
  filter(horizon >= 0) %>%
  pivot_longer(c(rel_rmsfe, rel_crps, rel_logs), names_to = "score", values_to = "value") %>%
  filter(value != Inf, score %in% c("rel_rmsfe", "rel_crps")) %>%
  pivot_wider(names_from = type, values_from = value) -> df_plot 
```

```{r}
df_plot <- add_is_significant_col(df_plot, dm_stats)
```

```{r}
# data-dependent lims!
ylims <- c(min(min(df_plot$unconditional), min(df_plot$conditional_hard)),
           max(max(df_plot$unconditional), max(df_plot$conditional_hard)))
xlims <- ylims

# manually set lims, excluding "outliers"
# ylims <- c(0.8, 1.2)
# xlims <- ylims
```


```{r}
ggplot(df_plot, aes(x = unconditional, 
                    y = conditional_hard, 
                    color = category)
       )+
    geom_point(data = filter(df_plot, is_significant == 1), size = 3, shape = 16, show.legend = FALSE)+
  geom_point(data = filter(df_plot, is_significant == 0), size = 3, shape = 1)+
  geom_abline()+
  geom_vline(xintercept = 0,  size = 0.2)+
  geom_hline(yintercept = 0, size = 0.2)+
  facet_wrap(~horizon_as_factor(horizon), nrow = 1)+
  xlim(xlims)+
  ylim(ylims)+
  scale_color_jco()+
  #theme_minimal()+
  theme(legend.position="bottom", 
        legend.text = element_text(size = 8),
        legend.title = element_blank())+
  labs(title = "", x = "RMSFE, unconditional forecasts", y = "RMSFE, conditional forecasts")
```

#### Export plot to pdf

```{r}
ggsave(filename = "../figures/fig_eval_postcrisis.pdf", 
       width = 10, height = 6, units = "in")
```

#### Notes for figure

> The figure shows the root mean squared forecast errors (RMSFE) over the evaluation sample 2011Q1-2017Q4 corresponding to unconditional forecasts (x-axis) and forecasts conditional on professional forecasters' view on GDP growth and CPI inflation (y-axis) for different time series. For each series, the RMSFE is relative to an autoregressive benchmark. Entries above (below) the 45-degree line indicate that conditional forecasts perform worse (better) than the unconditional ones. Filled points correspond to those variables for which the null hypothesis of the Diebold-Mariano test can be rejected at the 10 percent level. For details, see the main text.  



### Different models

Compare the models' performance when $R\neq 2$
```{r}
subsample <- c(min(df_eval_models$quarter), max(df_eval_models$quarter))
df_compare_R <- data.frame()
dm_stats_R <- data.frame()

for (nr in c(2, 1, 5, 8))
{
  str_model <- paste0("Nr", nr, "_Nj1_Np2_Ns0")

  calc_relative_forecast_accuracy(df_eval_models, 
                               df_eval_benchmark, 
                               str_model, 
                               subsample) %>% 
  mutate(Nr = paste0(nr)) %>% 
  rbind(df_compare_R) -> df_compare_R
  
  calc_dm_stats(df_eval_models, 
                        pval_threshold, 
                        subsample, 
                        str_model, 
                        "sfe") %>% 
  mutate(Nr = paste0(nr)) %>% 
  rbind(dm_stats_R) -> dm_stats_R
}
```


```{r}
df_compare_R %>% 
  filter(horizon >= 0) %>%
  pivot_longer(c(rel_rmsfe, rel_crps, rel_logs), names_to = "score", values_to = "value") %>%
  filter(value != Inf, score %in% c("rel_rmsfe")) %>%
  select(-category, -score) %>%
  pivot_wider(id_cols = c(mnemonic, type, horizon), names_from = "Nr", values_from = "value")%>%
  rename(base_model = `2`) %>%
  pivot_longer(cols = c(`1`, `5`, `8`), 
               names_to = "Nr", 
               values_to = "other_models") -> df_plot
# data-dependent lims!
ylims <- c(min(min(df_plot$base_model), min(df_plot$other_models)),
           max(max(df_plot$base_model), max(df_plot$other_models)))
xlims <- ylims

# change label
df_plot$type[df_plot$type == "conditional_hard"] <- "conditional"


ggplot(df_plot, aes(x = base_model, y = other_models, color = Nr, shape = type))+
      geom_point(size = 3)+
  geom_abline()+
  geom_vline(xintercept = 0,  size = 0.2)+
  geom_hline(yintercept = 0, size = 0.2)+
  facet_wrap(~horizon_as_factor(horizon), nrow = 1)+
  scale_x_continuous(limits = xlims, 
                     breaks = seq(0.5, 1.5, by = 0.5))+
  scale_y_continuous(limits = ylims, 
                     breaks = seq(0.5, 1.5, by = 0.5))+
  scale_shape_discrete(name = element_blank())+
  scale_color_jco(name = "R")+
  theme(legend.position="bottom", 
        legend.text = element_text(size = 8))+
  guides(shape = guide_legend(order = 2),col = guide_legend(order = 1))+
  labs(title = "", x = "rel. RMSFE, R=2", 
       y = paste0("rel. RMSFE, Râ‰ 2"))
```

#### Analysis

For which series do the forecasts improve at $h=0$? 

```{r}
df_plot %>% 
  filter(Nr == 8) %>% 
  mutate(diff_models = base_model - other_models) %>% 
  arrange(desc(diff_models)) %>% 
  select(mnemonic, type, diff_models, horizon) %>% 
  head(20)
```


#### Results

> Besides the evaluation period, the results presented above may also be sensitive to the model specification. In particular, they were obtained under a given number of factors, $R=2$. However, differently specified models yield similar forecast performance. Figure XX shows the relative RMSFE of the model with $R=2$ on which the results above are based as well as both smaller ($R=1$) and larger ($R = {5,8}$) models. There is little indication that model specification systematically alters the forecast performance. Conditional forecasts at $h=0$ for the change in the producer index improve substantially when $R=8$ with the relative RMSFE decreasing by 0.25. To a lesser extent this also holds for the unconditional forecasts with the relative RMSFE dropping by around 0.1. However, this is the exception and for other horizons and variables we find much smaller differences in either way. Consequently, the points corresponding to the relative RMSFE for $R={1, 5, 8}$ in Figure XX all hug the 45-degree line, which indicates identical forecast performance, quite closely. 

#### Notes for figure

> Note: The figure shows the root mean squared forecast error (RMSFE) relative to the autoregressive benchmarks for different number of factors. On the x-axis are the values of the RMSFE for the model with 2 factors on which the main results are based. On the y-axis are the RMSFE for alternative model specifications, differentiated by colors. Entries above (below) the 45-degree line indicate that the model with 2 factors performs better (worse) than the alternative models.  

#### Export plot to pdf

```{r}
ggsave(filename = "../figures/fig_eval_robustness_Nr.pdf", 
       width = 10, height = 6, units = "in", device=cairo_pdf)
```



